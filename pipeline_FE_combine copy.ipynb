{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T10:29:40.613231Z",
     "iopub.status.busy": "2024-12-20T10:29:40.612998Z",
     "iopub.status.idle": "2024-12-20T10:29:57.450536Z",
     "shell.execute_reply": "2024-12-20T10:29:57.449871Z",
     "shell.execute_reply.started": "2024-12-20T10:29:40.613198Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import copy\n",
    "import pickle\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from scipy.optimize import minimize\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "import polars as pl\n",
    "import polars.selectors as cs\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator, FormatStrFormatter, PercentFormatter\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.optimizers import Adam\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from colorama import Fore, Style\n",
    "from IPython.display import clear_output\n",
    "import warnings\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import VotingRegressor, RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import plotly.express as px\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-20T10:29:57.452522Z",
     "iopub.status.busy": "2024-12-20T10:29:57.451913Z",
     "iopub.status.idle": "2024-12-20T10:29:57.456875Z",
     "shell.execute_reply": "2024-12-20T10:29:57.456023Z",
     "shell.execute_reply.started": "2024-12-20T10:29:57.452499Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "SEED = 42\n",
    "n_splits = 5\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineer for tabular data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "def feature_engineering_v2_tabular(df, selector=None, imputer=None, fit=True):\n",
    "    df = df.loc[:, ~df.columns.duplicated()]\n",
    "    if fit: \n",
    "        y = df['sii']\n",
    "\n",
    "    numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "    season_cols = [col for col in df.columns if 'Season' in col]\n",
    "    pciat_cols = [col for col in df.columns if 'PCIAT' in col and 'Season' not in col]\n",
    "    remaining_numeric_cols = [col for col in numeric_cols if col not in pciat_cols and col not in ['sii']]\n",
    "    X = df[remaining_numeric_cols]\n",
    "    if np.any(np.isinf(X)):\n",
    "        X = X.replace([np.inf, -np.inf], np.nan)\n",
    "    if fit: \n",
    "        imputer = SimpleImputer()\n",
    "        imputed_data = imputer.fit_transform(X)\n",
    "        train_imputed = pd.DataFrame(imputed_data, columns=remaining_numeric_cols)\n",
    "        X = train_imputed\n",
    "    else:\n",
    "        X = imputer.transform(X)\n",
    "\n",
    "    if fit:\n",
    "        selector = SelectKBest(score_func=f_regression, k=30)\n",
    "        X_new = selector.fit_transform(X, y)\n",
    "        selected_features = X.columns[selector.get_support()]\n",
    "    else: \n",
    "        X_new = selector.transform(X)\n",
    "        selected_features = [col for col, selected in zip(remaining_numeric_cols, selector.get_support()) if selected]\n",
    "    df_selected = pd.DataFrame(X_new, columns=selected_features)\n",
    "    return df_selected, selector, imputer\n",
    "\n",
    "def feature_engineering_tabular(df):\n",
    "    season_cols = [col for col in df.columns if 'Season' in col]\n",
    "    df = df.drop(season_cols, axis=1) \n",
    "    df['BMI_Age'] = df['Physical-BMI'] * df['Basic_Demos-Age']\n",
    "    df['Internet_Hours_Age'] = df['PreInt_EduHx-computerinternet_hoursday'] * df['Basic_Demos-Age']\n",
    "    df['BMI_Internet_Hours'] = df['Physical-BMI'] * df['PreInt_EduHx-computerinternet_hoursday']\n",
    "    df['BFP_BMI'] = df['BIA-BIA_Fat'] / df['BIA-BIA_BMI']\n",
    "    df['FFMI_BFP'] = df['BIA-BIA_FFMI'] / df['BIA-BIA_Fat']\n",
    "    df['FMI_BFP'] = df['BIA-BIA_FMI'] / df['BIA-BIA_Fat']\n",
    "    df['LST_TBW'] = df['BIA-BIA_LST'] / df['BIA-BIA_TBW']\n",
    "    df['BFP_BMR'] = df['BIA-BIA_Fat'] * df['BIA-BIA_BMR']\n",
    "    df['BFP_DEE'] = df['BIA-BIA_Fat'] * df['BIA-BIA_DEE']\n",
    "    df['BMR_Weight'] = df['BIA-BIA_BMR'] / df['Physical-Weight']\n",
    "    df['DEE_Weight'] = df['BIA-BIA_DEE'] / df['Physical-Weight']\n",
    "    df['SMM_Height'] = df['BIA-BIA_SMM'] / df['Physical-Height']\n",
    "    df['Muscle_to_Fat'] = df['BIA-BIA_SMM'] / df['BIA-BIA_FMI']\n",
    "    df['Hydration_Status'] = df['BIA-BIA_TBW'] / df['Physical-Weight']\n",
    "    df['ICW_TBW'] = df['BIA-BIA_ICW'] / df['BIA-BIA_TBW']\n",
    "    df['BMI_PHR'] = df['Physical-BMI'] * df['Physical-HeartRate']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineer for time series data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: Autoencoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T10:29:57.526195Z",
     "iopub.status.busy": "2024-12-20T10:29:57.526013Z",
     "iopub.status.idle": "2024-12-20T10:29:57.534049Z",
     "shell.execute_reply": "2024-12-20T10:29:57.533355Z",
     "shell.execute_reply.started": "2024-12-20T10:29:57.526179Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, encoding_dim):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, encoding_dim*3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(encoding_dim*3, encoding_dim*2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(encoding_dim*2, encoding_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_dim, encoding_dim*2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(encoding_dim*2, encoding_dim*3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(encoding_dim*3, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "\n",
    "def perform_autoencoder(df, encoding_dim=50, epochs=50, batch_size=32):\n",
    "    scaler = StandardScaler()\n",
    "    df_scaled = scaler.fit_transform(df)\n",
    "    \n",
    "    data_tensor = torch.FloatTensor(df_scaled)\n",
    "    \n",
    "    input_dim = data_tensor.shape[1]\n",
    "    autoencoder = AutoEncoder(input_dim, encoding_dim)\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(autoencoder.parameters())\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for i in range(0, len(data_tensor), batch_size):\n",
    "            batch = data_tensor[i : i + batch_size]\n",
    "            optimizer.zero_grad()\n",
    "            reconstructed = autoencoder(batch)\n",
    "            loss = criterion(reconstructed, batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}]')    \n",
    "    return autoencoder, scaler\n",
    "\n",
    "def encode_data(autoencoder, scaler, df):\n",
    "    df_scaled = scaler.transform(df)\n",
    "    data_tensor = torch.FloatTensor(df_scaled)\n",
    "    with torch.no_grad():\n",
    "        encoded_data = autoencoder.encoder(data_tensor).numpy()\n",
    "\n",
    "    df_encoded = pd.DataFrame(encoded_data, columns=[f'Enc_{i + 1}' for i in range(encoded_data.shape[1])])\n",
    "    return df_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: External knowledge\n",
    "\n",
    "First, prepare the features used in my sleep detection model. Please refer to the implementation by [@tatamikenn](https://www.kaggle.com/tatamikenn) [here](https://www.kaggle.com/code/tatamikenn/sleep-hdcza-a-pure-heuristic-approach-lb-0-447).\n",
    "\n",
    "This pipeline processes accelerometer data for sleep detection, utilizing time-series datasets. It generates features to identify sleep episodes, static periods, and motion patterns, inspired by @tatamikenn's implementation.\n",
    "\n",
    "-  `transform`: this function processes input data to generate features for analysis. It breaks down the timestamp into components like year, month, day, hour, and weekday. It also groups data by night, adjusting the timestamp if necessary, and creates a unique `night_group` identifier for each night. Additionally, a cumulative step count (norm_step) is computed for each group to facilitate sequential analysis.\n",
    "\n",
    "- `transform_series`: this function enhances the transform function by adding a new feature: detecting clipped ENMO values. It flags instances where the enmo (motion metric) is zero, marking potential data quality issues.\n",
    "\n",
    "- `transform_events`: this function processes event data by adding a night column and pivoting the data. The events are rearranged by `series_id`, `group_id`, and night to simplify time-series analysis.\n",
    "\n",
    "- `add_feature`: this function generates advanced features for sleep detection, including:\n",
    "    - Difference Features: Computes the differences in anglez (angular motion) and enmo (motion magnitude).\n",
    "    - Rolling Median: Calculates rolling medians of anglez_diff and enmo_diff over a 5-minute window.\n",
    "    - Critical Threshold: Determines static periods by evaluating anglez_diff variability over a day.\n",
    "    - Static and Sleep Blocks: Flags periods with minimal motion (is_static) and identifies sleep blocks over 30-minute windows.\n",
    "    - Sleep Episodes: Detects continuous sleep episodes, identifies the longest one, and flags interruptions in sleep.\n",
    "\n",
    "-  `create_heuristic`: the main function processes raw data files by converting timestamps and applying transformations. It calls the transform_series function to prepare the data and the add_feature function to generate sleep-related features. Finally, it saves the processed data into .parquet files for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_FILE = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(df, night_offset=20):\n",
    "    return (\n",
    "        df.with_columns(\n",
    "            [\n",
    "                (pl.col(\"timestamp\").dt.year() - 2000).cast(pl.Int8).alias(\"year\"),\n",
    "                pl.col(\"timestamp\").dt.month().cast(pl.Int8).alias(\"month\"),\n",
    "                pl.col(\"timestamp\").dt.day().cast(pl.Int8).alias(\"day\"),\n",
    "                pl.col(\"timestamp\").dt.hour().cast(pl.Int8).alias(\"hour\"),\n",
    "                pl.col(\"timestamp\").dt.minute().cast(pl.Int8).alias(\"minute\"),\n",
    "                pl.col(\"timestamp\").dt.second().cast(pl.Int8).alias(\"second\"),\n",
    "                pl.col(\"timestamp\").dt.weekday().cast(pl.Int8).alias(\"weekday\"),\n",
    "            ]\n",
    "        )\n",
    "        .with_columns( \n",
    "            pl.when(pl.col(\"hour\") < night_offset)\n",
    "            .then(pl.col(\"timestamp\"))\n",
    "            .otherwise(pl.col(\"timestamp\") + pl.duration(days=1))\n",
    "            .dt.date()\n",
    "            .alias(\"night_group\"),\n",
    "        )\n",
    "        .with_columns(\n",
    "            [\n",
    "                (\n",
    "                    pl.col(\"series_id\") + pl.lit(\"_\") + pl.col(\"night_group\").cast(pl.Datetime).dt.strftime(\"%Y%m%d\")\n",
    "                ).alias(\"group_id\"),\n",
    "            ]\n",
    "        )\n",
    "        .with_columns(\n",
    "            [\n",
    "                pl.col(\"timestamp\").cum_count().over(\"group_id\").alias(\"norm_step\"),\n",
    "            ]\n",
    "        )\n",
    "        .drop([\"night_group\"])\n",
    "    )\n",
    "\n",
    "\n",
    "def transform_series(df):\n",
    "    return transform(df).with_columns(\n",
    "        [\n",
    "            (pl.col(\"enmo\") == 0).alias(\"is_enmo_clipped\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def transform_events(df):\n",
    "    return (\n",
    "        transform(df)\n",
    "        .with_columns(\n",
    "            [\n",
    "                pl.col(\"night\").cast(pl.UInt32).alias(\"night\"),\n",
    "            ]\n",
    "        )\n",
    "        .pivot([\"step\", \"timestamp\", \"tz_offset\"], [\"series_id\", \"group_id\", \"night\"], \"event\")\n",
    "    )\n",
    "\n",
    "\n",
    "def add_feature(\n",
    "    df,\n",
    "    day_group_col=\"group_id\",\n",
    "    term1=(5 * 60) // 5,\n",
    "    term2=(30 * 60) // 5,\n",
    "    term3=(60 * 60) // 5,\n",
    "    min_threshold=0.005,\n",
    "    max_threshold=0.04,\n",
    "    center=True,\n",
    "):\n",
    "    return (\n",
    "        df.with_columns(\n",
    "            [\n",
    "                pl.col(\"anglez\").diff(1).abs().alias(\"anglez_diff\"),\n",
    "                pl.col(\"enmo\").diff(1).abs().alias(\"enmo_diff\"),\n",
    "            ]\n",
    "        )\n",
    "        .with_columns(\n",
    "            [\n",
    "                pl.col(\"anglez_diff\")\n",
    "                .rolling_median(term1, center=center)  # 5 min window\n",
    "                .alias(\"anglez_diff_median_5min\"),\n",
    "                pl.col(\"enmo_diff\")\n",
    "                .rolling_median(term1, center=center)  # 5 min window\n",
    "                .alias(\"enmo_diff_median_5min\"),\n",
    "            ]\n",
    "        )\n",
    "        .with_columns(\n",
    "            [\n",
    "                pl.col(\"anglez_diff_median_5min\")\n",
    "                .quantile(0.1)\n",
    "                .clip(min_threshold, max_threshold)\n",
    "                .over(day_group_col)\n",
    "                .alias(\"critical_threshold\")\n",
    "            ]\n",
    "        )\n",
    "        .with_columns([(pl.col(\"anglez_diff_median_5min\") < pl.col(\"critical_threshold\") * 15).alias(\"is_static\")])\n",
    "        .with_columns(\n",
    "            [\n",
    "                pl.col(\"is_static\").cast(pl.Int32).rolling_sum(term2, center=center).alias(\"is_static_sum_30min\"),\n",
    "            ]\n",
    "        )\n",
    "        .with_columns([(pl.col(\"is_static_sum_30min\") == ((30 * 60) // 5)).alias(\"tmp\")])\n",
    "        .with_columns(\n",
    "            [\n",
    "                pl.col(\"tmp\").shift(term2 // 2).alias(\"tmp_left\"),\n",
    "                pl.col(\"tmp\").shift(-(term2 // 2)).alias(\"tmp_right\"),\n",
    "            ]\n",
    "        )\n",
    "        .with_columns(\n",
    "            [\n",
    "                (pl.col(\"tmp_left\") | pl.col(\"tmp_right\")).alias(\"is_sleep_block\"),\n",
    "            ]\n",
    "        )\n",
    "        .drop([\"tmp\", \"tmp_left\", \"tmp_right\"])\n",
    "        .with_columns([pl.col(\"is_sleep_block\").not_().alias(\"is_gap\")])\n",
    "        .with_columns([pl.col(\"is_gap\").cast(pl.Int32).rolling_sum(term3, center=center).alias(\"gap_length\")])\n",
    "        .with_columns([(pl.col(\"gap_length\") == term3).alias(\"tmp\")])\n",
    "        .with_columns(\n",
    "            [\n",
    "                pl.col(\"tmp\").shift(term3 // 2).alias(\"tmp_left\"),\n",
    "                pl.col(\"tmp\").shift(-(term3 // 2)).alias(\"tmp_right\"),\n",
    "            ]\n",
    "        )\n",
    "        .with_columns(\n",
    "            [\n",
    "                (pl.col(\"tmp_left\") | pl.col(\"tmp_right\")).alias(\"is_large_gap\"),\n",
    "            ]\n",
    "        )\n",
    "        .drop([\"tmp\", \"tmp_left\", \"tmp_right\"])\n",
    "        .with_columns([pl.col(\"is_large_gap\").not_().alias(\"is_sleep_episode\")])\n",
    "        #\n",
    "        # extract longest sleep episode\n",
    "        #\n",
    "        .with_columns(\n",
    "            [\n",
    "                # extract false->true transition\n",
    "                (\n",
    "                    (\n",
    "                        pl.col(\"is_sleep_episode\")\n",
    "                        & pl.col(\"is_sleep_episode\").shift(1, fill_value=pl.lit(False)).not_()\n",
    "                    )\n",
    "                    .cum_sum()\n",
    "                    .over(\"group_id\")\n",
    "                ).alias(\"sleep_episode_id\")\n",
    "            ]\n",
    "        )\n",
    "        .with_columns(\n",
    "            [pl.col(\"is_sleep_episode\").sum().over([\"group_id\", \"sleep_episode_id\"]).alias(\"sleep_episode_length\")]\n",
    "        )\n",
    "        .with_columns([pl.col(\"sleep_episode_length\").max().over([\"group_id\"]).alias(\"max_sleep_episode_length\")])\n",
    "        .with_columns(\n",
    "            [\n",
    "                (\n",
    "                    pl.col(\"is_sleep_episode\") & (pl.col(\"sleep_episode_length\") == pl.col(\"max_sleep_episode_length\"))\n",
    "                ).alias(\"is_longest_sleep_episode\")\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "use_columns = [\n",
    "    \"series_id\",\n",
    "    \"step\",\n",
    "    \"is_longest_sleep_episode\",\n",
    "    \"is_sleep_block\",\n",
    "    \"is_gap\",\n",
    "    \"is_large_gap\",\n",
    "    \"is_sleep_episode\",\n",
    "    \"is_static\",\n",
    "]\n",
    "\n",
    "def create_heuristic(paths, train_or_test):\n",
    "    i = 0\n",
    "    for path in tqdm(paths):\n",
    "        i += 1\n",
    "        if (i == MAX_FILE):\n",
    "            break\n",
    "        sdf = pl.read_parquet(path)\n",
    "    \n",
    "        # dummy timestamp\n",
    "        sdf = sdf.with_columns((pl.col(\"time_of_day\") == 0).cast(pl.Int32).cum_sum().alias(\"day_offset\"))\n",
    "        sdf = sdf.with_columns(\n",
    "            (\n",
    "                datetime.datetime(2020, 1, 1)\n",
    "                + (pl.col(\"day_offset\") * 86400_000_000 + pl.col(\"time_of_day\") / 1000).cast(pl.Duration(\"us\"))\n",
    "            ).alias(\"timestamp\")\n",
    "        )\n",
    "    \n",
    "        sdf = sdf.with_columns(pl.lit(path.split(\"/\")[-2]).alias(\"series_id\"))\n",
    "        sdf = sdf.sort(\"step\")\n",
    "        sdf = transform_series(sdf)\n",
    "        sdf = add_feature(sdf)\n",
    "        sdf = sdf[use_columns].fill_null(False)\n",
    "    \n",
    "        sidf = path.split(\"/\")[-2]\n",
    "        save_path = f\"/kaggle/working/heuristic_features/{train_or_test}/{sidf}.parquet\"\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        sdf.write_parquet(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    sys.path.append(\"/kaggle/input/cmi-2023-src\")\n",
    "    from consts import ANGLEZ_MEAN, ANGLEZ_STD, ENMO_MEAN, ENMO_STD\n",
    "    from torch_models.dataset import ZzzPatchDataset\n",
    "    from torch_models.models import ZzzConv1dGRUModel, ZzzTransformerGRUModel, ZzzWaveGRUModel\n",
    "\n",
    "    from utils.feature_contena import Features\n",
    "    from utils.lightning_utils import MyLightningDataModule, MyLightningModule\n",
    "    from utils.set_seed import seed_base_torch\n",
    "    from utils.torch_template import EnsembleModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detection(paths=f\"/kaggle/input/child-mind-institute-problematic-internet-use/series_train.parquet/id=*/part-0.parquet\", train_or_test=\"train\"):\n",
    "    MODEL_NAME = \"patch_transformer_gru\"\n",
    "    \n",
    "    PACKAGE_DIR = Path(\"/kaggle/input/cmi-2023-src\")\n",
    "    CFG = yaml.safe_load(open(PACKAGE_DIR / \"config.yaml\", \"r\"))\n",
    "    BLOCK_SIZE = CFG[MODEL_NAME][\"execution\"][\"block_size\"]\n",
    "    \n",
    "    CFG[\"output_dir\"] = f\"/kaggle/input/cmi-2023-output/{CFG[MODEL_NAME]['execution']['best_exp_id']}\"\n",
    "    \n",
    "    seed_base_torch(CFG[\"env\"][\"seed\"])\n",
    "    \n",
    "    DEVICE = \"cuda\"\n",
    "    \n",
    "    files = glob(\n",
    "        paths\n",
    "    )\n",
    "    \n",
    "    features = Features()\n",
    "    features.add_num_features([\"anglez\", \"enmo\"])\n",
    "    features.add_num_features([\"anglez_diff\", \"enmo_diff\"])\n",
    "    features.add_num_features([\"same_count\"])\n",
    "    features.add_num_features([\"large_diff_count\"])\n",
    "    features.add_num_features([\"same_count_shift_plus\", \"same_count_shift_minus\"])\n",
    "    features.add_num_features([\"is_longest_sleep_episode\", \"is_sleep_block\"])\n",
    "    \n",
    "    # transformer + gru\n",
    "    model = ZzzTransformerGRUModel(\n",
    "        max_len=BLOCK_SIZE // CFG[MODEL_NAME][\"execution\"][\"patch_size\"],\n",
    "        input_numerical_size=len(features.all_features()) * CFG[MODEL_NAME][\"execution\"][\"patch_size\"],\n",
    "        **CFG[MODEL_NAME][\"params\"],\n",
    "    )\n",
    "    trn_models = [\n",
    "        MyLightningModule.load_from_checkpoint(\n",
    "            os.path.join(\"/kaggle/input/cmi-2023-output/exp_160\", f\"logs/best_model_fold{fold}.ckpt\"),\n",
    "            model=model,\n",
    "            map_location=torch.device(DEVICE),\n",
    "        ).to(DEVICE)\n",
    "        for fold in range(5 if len(files) > 100 else 1)\n",
    "    ]\n",
    "    \n",
    "    models = trn_models\n",
    "    model = EnsembleModel(models).to(DEVICE)\n",
    "    model.eval()\n",
    "    \n",
    "    all_oof_dfs = []\n",
    "    i = 0\n",
    "    for file in tqdm(files):\n",
    "        # load file\n",
    "        i += 1\n",
    "        if (i == MAX_FILE):\n",
    "            break\n",
    "        df = pd.read_parquet(file)\n",
    "        if len(df) < BLOCK_SIZE:\n",
    "            continue\n",
    "        time_of_days = df[\"time_of_day\"].values\n",
    "    \n",
    "        # same_count\n",
    "        DAY_STEPS = 12 * 60 * 24\n",
    "        n_days = int(len(df) // DAY_STEPS) + 1\n",
    "        df[\"same_count\"] = 0\n",
    "        for day in range(-n_days, n_days + 1):\n",
    "            if day == 0:\n",
    "                continue\n",
    "            df[\"_anglez_diff\"] = df[\"anglez\"].diff(DAY_STEPS * day)\n",
    "            df[\"_anglez_diff\"] = df[\"_anglez_diff\"].fillna(1)\n",
    "            df[\"same_count\"] += (df[\"_anglez_diff\"] == 0).astype(int)\n",
    "        df[\"same_count\"] = (df[\"same_count\"].clip(0, 5) - 2.5) / 2.5\n",
    "    \n",
    "        SHIFT_STEPS = 12 * 60 * 6  # 6h\n",
    "        df[\"same_count_shift_plus\"] = df[\"same_count\"].shift(SHIFT_STEPS).fillna(1.0).astype(np.float16)\n",
    "        df[\"same_count_shift_minus\"] = df[\"same_count\"].shift(-SHIFT_STEPS).fillna(1.0).astype(np.float16)\n",
    "    \n",
    "        # features\n",
    "        df[\"anglez_diffabs\"] = df[\"anglez\"].diff().abs().fillna(0)\n",
    "        df[\"large_diff\"] = (df[\"anglez_diffabs\"] > 5).astype(int)\n",
    "        df[\"large_diff_count\"] = df[\"large_diff\"].rolling(10, center=True).mean().fillna(0)\n",
    "        df[\"large_diff_count\"] = (df[\"large_diff_count\"] - 0.5) * 2\n",
    "    \n",
    "        # normalize\n",
    "        df[\"anglez\"] = (df[\"anglez\"] - ANGLEZ_MEAN) / ANGLEZ_STD\n",
    "        df[\"enmo\"] = (df[\"enmo\"] - ENMO_MEAN) / ENMO_STD\n",
    "        df[\"anglez_diff\"] = df[\"anglez\"].diff().fillna(0)\n",
    "        df[\"enmo_diff\"] = df[\"enmo\"].diff().fillna(0)\n",
    "    \n",
    "        # heuristic_features by @bilzard\n",
    "        sid = file.split(\"/\")[-2]\n",
    "        df[\"series_id\"] = sid\n",
    "        path = f\"/kaggle/working/heuristic_features/{train_or_test}/{sid}.parquet\"\n",
    "        hdf = pd.read_parquet(path)\n",
    "        df = pd.concat([df, hdf.drop(columns=[\"series_id\", \"step\"])], axis=1)\n",
    "        df[[\"is_longest_sleep_episode\", \"is_sleep_block\"]] = df[[\"is_longest_sleep_episode\", \"is_sleep_block\"]] * 2 - 1\n",
    "    \n",
    "        # split\n",
    "        dfs = []\n",
    "        df = df.sort_values(\"step\").reset_index(drop=True)\n",
    "        for start in range(0, len(df), BLOCK_SIZE // 8):\n",
    "            end = start + BLOCK_SIZE\n",
    "            if end > len(df):\n",
    "                end = len(df) - len(df) % CFG[MODEL_NAME][\"execution\"][\"patch_size\"]\n",
    "                start = end - BLOCK_SIZE\n",
    "                assert start >= 0\n",
    "            assert df.iloc[start][\"step\"] % CFG[MODEL_NAME][\"execution\"][\"patch_size\"] == 0\n",
    "            dfs.append(df.iloc[start:end])\n",
    "        gc.collect()\n",
    "    \n",
    "        # inference\n",
    "        train_dataset = ZzzPatchDataset(\n",
    "            dfs, mode=\"test\", features=features, patch_size=CFG[MODEL_NAME][\"execution\"][\"patch_size\"]\n",
    "        )\n",
    "        valid_dataset = ZzzPatchDataset(\n",
    "            dfs, mode=\"test\", features=features, patch_size=CFG[MODEL_NAME][\"execution\"][\"patch_size\"]\n",
    "        )\n",
    "        data_module = MyLightningDataModule(train_dataset, valid_dataset, batch_size=64)\n",
    "        preds = []\n",
    "        with torch.no_grad():\n",
    "            for X in data_module.val_dataloader():\n",
    "                pred = torch.sigmoid(model(X.to(\"cuda\"))).detach().cpu().numpy() * 10\n",
    "                preds.append(pred)\n",
    "    \n",
    "        oof_dfs = []\n",
    "        for pred, df in zip(np.vstack(preds), dfs):\n",
    "            df = df.iloc[\n",
    "                CFG[MODEL_NAME][\"execution\"][\"patch_size\"] // 2 : len(df) : CFG[MODEL_NAME][\"execution\"][\"patch_size\"]\n",
    "            ].reset_index(drop=True)\n",
    "            df[[\"wakeup_oof\", \"onset_oof\"]] = pred\n",
    "            oof_dfs.append(df[[\"series_id\", \"step\", \"wakeup_oof\", \"onset_oof\"]])\n",
    "    \n",
    "        oof_df = pd.concat(oof_dfs)\n",
    "        oof_df = oof_df.groupby([\"series_id\", \"step\"]).mean().reset_index().sort_values([\"series_id\", \"step\"])\n",
    "        oof_df = oof_df[[\"series_id\", \"step\", \"wakeup_oof\", \"onset_oof\"]]\n",
    "        oof_df[\"step\"] = oof_df[\"step\"].astype(int)\n",
    "    \n",
    "        del preds, oof_dfs\n",
    "        gc.collect()\n",
    "    \n",
    "        train = oof_df.reset_index(drop=True)\n",
    "        train[\"time_of_day\"] = time_of_days[\n",
    "            CFG[MODEL_NAME][\"execution\"][\"patch_size\"] // 2 :: CFG[MODEL_NAME][\"execution\"][\"patch_size\"]\n",
    "        ][: len(train)]\n",
    "        all_oof_dfs.append(train[[\"series_id\", \"step\", \"wakeup_oof\", \"onset_oof\", \"time_of_day\"]])\n",
    "        # del dfs, df\n",
    "        gc.collect()\n",
    "\n",
    "    # save\n",
    "    for df in tqdm(all_oof_dfs):\n",
    "        save_path = f\"/kaggle/working/features/sleep_detection/{train_or_test}/{df['series_id'].iloc[0]}.parquet\"\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        df.to_parquet(save_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_of_day_max = 86400000000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering_sleep_detect(paths=\"/kaggle/working/features/sleep_detection/train/*.parquet\", data_paths=\"/kaggle/input/child-mind-institute-problematic-internet-use/series_train.parquet\", train_or_test=\"train\"):\n",
    "    features = []\n",
    "    debug_count = 0\n",
    "    all_files = sorted(glob(paths))\n",
    "    i = 0\n",
    "    for file in tqdm(all_files):\n",
    "        i += 1\n",
    "        if (i == MAX_FILE):\n",
    "            break\n",
    "        df = pl.read_parquet(file)\n",
    "        df = df.with_columns(pl.col(\"step\").cast(pl.UInt32)).drop(\"time_of_day\")\n",
    "        sid = df[\"series_id\"][0]\n",
    "    \n",
    "        sensor_df = pl.read_parquet(\n",
    "            f\"{data_paths}/{sid}/part-0.parquet\"\n",
    "        ).with_columns((pl.col(\"time_of_day\") == 0).cum_sum().alias(\"day\"))\n",
    "    \n",
    "        feature = {\n",
    "            \"id\": sid,\n",
    "            \"length\": df.shape[0],\n",
    "            \"day\": sensor_df[\"relative_date_PCIAT\"].max() - sensor_df[\"relative_date_PCIAT\"].min(),\n",
    "        }\n",
    "    \n",
    "        # skip if time step is not 5sec\n",
    "        diffs = sensor_df[\"time_of_day\"].diff().drop_nulls().unique()\n",
    "        if set(diffs) != set([-86395000000000, 5000000000]):\n",
    "            features.append(feature)\n",
    "            continue\n",
    "    \n",
    "        sensor_df = (\n",
    "            sensor_df.join(df, on=\"step\", how=\"left\")\n",
    "            .sort(\"step\")\n",
    "            .with_columns(\n",
    "                pl.col(\"onset_oof\").interpolate(),\n",
    "                pl.col(\"wakeup_oof\").interpolate(),\n",
    "            )\n",
    "        )\n",
    "    \n",
    "        # onset = 15:00~3:00, wakeup = 3:00~15:00\n",
    "        onset_start = time_of_day_max / 24 * 15  # 15:00\n",
    "        onset_end = time_of_day_max / 24 * 3  # 3:00\n",
    "        sensor_df = sensor_df.with_columns(\n",
    "            ((pl.col(\"time_of_day\") > onset_start) | (pl.col(\"time_of_day\") < onset_end)).alias(\"onset_duration\"),\n",
    "        ).with_columns(\n",
    "            pl.col(\"onset_duration\").cast(pl.Int32).diff().fill_null(0).abs().cum_sum().alias(\"onset_wakeup_duration\")\n",
    "        )\n",
    "    \n",
    "        # get sleep period\n",
    "        sleep_info = []\n",
    "        for _, df in sensor_df.group_by(\"onset_wakeup_duration\", maintain_order=True):\n",
    "            is_onset = df[\"onset_duration\"][0]\n",
    "            if is_onset:\n",
    "                max_idx = df[\"onset_oof\"].arg_max()\n",
    "                if max_idx is None:\n",
    "                    continue\n",
    "                max_score = df[\"onset_oof\"][max_idx]\n",
    "                step = df[\"step\"][max_idx]\n",
    "    \n",
    "                # date\n",
    "                start_time = df[\"time_of_day\"][0] / time_of_day_max * 24\n",
    "                if start_time >= 15:\n",
    "                    day = df[\"day\"][0]\n",
    "                    week_day = df[\"weekday\"][0]\n",
    "                else:\n",
    "                    day = df[\"day\"][0] - 1\n",
    "                    week_day = df[\"weekday\"][0] - 1\n",
    "                    if week_day == 0:\n",
    "                        week_day = 7\n",
    "            else:\n",
    "                max_idx = df[\"wakeup_oof\"].arg_max()\n",
    "                if max_idx is None:\n",
    "                    continue\n",
    "                max_score = df[\"wakeup_oof\"][max_idx]\n",
    "                step = df[\"step\"][max_idx]\n",
    "    \n",
    "                # date\n",
    "                start_time = df[\"time_of_day\"][0] / time_of_day_max * 24\n",
    "                day = df[\"day\"][0] - 1\n",
    "                week_day = df[\"weekday\"][0] - 1\n",
    "    \n",
    "            info = {\n",
    "                \"day\": day,\n",
    "                \"weekday\": week_day,\n",
    "                \"type\": \"onset\" if is_onset else \"wakeup\",\n",
    "                \"step\": step,\n",
    "                \"max_score\": max_score,\n",
    "                \"time\": df[\"time_of_day\"][max_idx] / time_of_day_max * 24,\n",
    "            }\n",
    "            sleep_info.append(info)\n",
    "        sleep_df = pl.DataFrame(sleep_info)\n",
    "    \n",
    "        # merge\n",
    "        sleep_df = (\n",
    "            sleep_df.filter(pl.col(\"type\") == \"onset\")\n",
    "            .drop(\"type\")\n",
    "            .rename(\n",
    "                {\n",
    "                    \"max_score\": \"onset_score\",\n",
    "                    \"step\": \"onset_step\",\n",
    "                    \"time\": \"onset_time\",\n",
    "                }\n",
    "            )\n",
    "            .join(\n",
    "                sleep_df.filter(pl.col(\"type\") == \"wakeup\")\n",
    "                .drop([\"type\", \"weekday\"])\n",
    "                .rename(\n",
    "                    {\n",
    "                        \"max_score\": \"wakeup_score\",\n",
    "                        \"step\": \"wakeup_step\",\n",
    "                        \"time\": \"wakeup_time\",\n",
    "                    }\n",
    "                ),\n",
    "                on=\"day\",\n",
    "            )\n",
    "        ).select(\n",
    "            [\"day\", \"weekday\", \"onset_time\", \"wakeup_time\", \"onset_step\", \"wakeup_step\", \"onset_score\", \"wakeup_score\"]\n",
    "        )\n",
    "    \n",
    "        # feature engineering\n",
    "        sleep_lengths = []  # wakeup - onset\n",
    "        sleep_enmo_mean = []  \n",
    "        sleep_enmo_std = []  \n",
    "        sleep_light_mean = []\n",
    "        sleep_light_std = [] \n",
    "        for i in range(len(sleep_df)):\n",
    "            # sleep period\n",
    "            start = sleep_df[\"onset_step\"][i]\n",
    "            end = sleep_df[\"wakeup_step\"][i]\n",
    "            if sleep_df[\"onset_score\"][i] < 1 or sleep_df[\"wakeup_score\"][i] < 1:\n",
    "                sleep_lengths.append(np.nan)\n",
    "                sleep_enmo_mean.append(np.nan)\n",
    "                sleep_enmo_std.append(np.nan)\n",
    "                sleep_light_mean.append(np.nan)\n",
    "                sleep_light_std.append(np.nan)\n",
    "                continue\n",
    "    \n",
    "            # sleep length\n",
    "            length = end - start\n",
    "            sleep_lengths.append(length * 5 / 60 / 60)  # hour\n",
    "    \n",
    "            # enmo\n",
    "            enmo_mean = sensor_df[\"enmo\"][start:end].mean()\n",
    "            enmo_std = sensor_df[\"enmo\"][start:end].std()\n",
    "            sleep_enmo_mean.append(enmo_mean)\n",
    "            sleep_enmo_std.append(enmo_std)\n",
    "    \n",
    "            # light\n",
    "            light_mean = sensor_df[\"light\"][start:end].mean()\n",
    "            light_std = sensor_df[\"light\"][start:end].std()\n",
    "            sleep_light_mean.append(light_mean)\n",
    "            sleep_light_std.append(light_std)\n",
    "            \n",
    "        sleep_df = sleep_df.with_columns(\n",
    "            pl.DataFrame(\n",
    "                {\n",
    "                    \"sleep_length\": sleep_lengths,\n",
    "                    \"sleep_enmo_mean\": sleep_enmo_mean,\n",
    "                    \"sleep_enmo_std\": sleep_enmo_std,\n",
    "                    \"sleep_light_mean\": sleep_light_mean,\n",
    "                    \"sleep_light_std\": sleep_light_std,\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # leave only high confidence periods\n",
    "        sleep_df = sleep_df.filter((pl.col(\"wakeup_score\") > 1) & (pl.col(\"onset_score\") > 1))\n",
    "        if debug_count < 3:\n",
    "            display(sleep_df.head())\n",
    "        debug_count += 1\n",
    "            \n",
    "    \n",
    "        # agg\n",
    "        feature.update(\n",
    "            {\n",
    "                \"sleep_measurement_count\": sleep_df.shape[0],\n",
    "                \"sleep_length_mean\": sleep_df[\"sleep_length\"].mean(),\n",
    "                \"sleep_length_std\": sleep_df[\"sleep_length\"].std(),\n",
    "                \"sleep_start_mean\": sleep_df[\"onset_time\"].mean(),\n",
    "                \"sleep_start_std\": sleep_df[\"onset_time\"].std(),\n",
    "                \"sleep_end_mean\": sleep_df[\"wakeup_time\"].mean(),\n",
    "                \"sleep_end_std\": sleep_df[\"wakeup_time\"].std(),\n",
    "                \"sleep_enmo_mean_mean\": sleep_df[\"sleep_enmo_mean\"].mean(),\n",
    "                \"sleep_enmo_mean_std\": sleep_df[\"sleep_enmo_mean\"].std(),\n",
    "                \"sleep_enmo_std_mean\": sleep_df[\"sleep_enmo_std\"].mean(),\n",
    "                \"sleep_enmo_std_std\": sleep_df[\"sleep_enmo_std\"].std(),\n",
    "                \"sleep_light_mean_mean\": sleep_df[\"sleep_light_mean\"].mean(),\n",
    "                \"sleep_light_mean_std\": sleep_df[\"sleep_light_mean\"].std(),\n",
    "                \"sleep_light_std_mean\": sleep_df[\"sleep_light_std\"].mean(),\n",
    "                \"sleep_light_std_std\": sleep_df[\"sleep_light_std\"].std(),\n",
    "            }\n",
    "        )\n",
    "        features.append(feature)\n",
    "    output_dir = f\"/kaggle/working/features/{train_or_test}\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    feature_df = pl.DataFrame(features).with_columns(pl.col(\"id\").str.slice(3, 8))\n",
    "    feature_df.write_csv(f\"/kaggle/working/features/{train_or_test}/sleep_features.csv\")\n",
    "    print(feature_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_heuristic(paths=glob(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_test.parquet/id=*/part-0.parquet\"), train_or_test=\"test\")\n",
    "detection(paths=\"/kaggle/input/child-mind-institute-problematic-internet-use/series_test.parquet/id=*/part-0.parquet\", train_or_test=\"test\")\n",
    "feature_engineering_sleep_detect(paths=\"/kaggle/working/features/sleep_detection/test/*.parquet\", data_paths=\"/kaggle/input/child-mind-institute-problematic-internet-use/series_test.parquet\", train_or_test=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T10:29:57.536393Z",
     "iopub.status.busy": "2024-12-20T10:29:57.536207Z",
     "iopub.status.idle": "2024-12-20T10:29:57.553790Z",
     "shell.execute_reply": "2024-12-20T10:29:57.553105Z",
     "shell.execute_reply.started": "2024-12-20T10:29:57.536376Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Handle non-numeric columns\n",
    "def update(df):\n",
    "    global cat_c\n",
    "    for c in cat_c: \n",
    "        df[c] = df[c].fillna('Missing')\n",
    "        df[c] = df[c].astype('category')\n",
    "    return df\n",
    "\n",
    "def create_mapping(column, dataset):\n",
    "    unique_values = dataset[column].unique()\n",
    "    return {value: idx for idx, value in enumerate(unique_values)}\n",
    "\n",
    "# Download timeseries data\n",
    "def process_file(filename, dirname):\n",
    "    df = pd.read_parquet(os.path.join(dirname, filename, 'part-0.parquet'))\n",
    "    df.drop('step', axis=1, inplace=True)\n",
    "    return df.describe().values.reshape(-1), filename.split('=')[1]\n",
    "\n",
    "def load_time_series(dirname) -> pd.DataFrame:\n",
    "    ids = os.listdir(dirname)\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        results = list(tqdm(executor.map(lambda fname: process_file(fname, dirname), ids), total=len(ids)))\n",
    "    stats, indexes = zip(*results)\n",
    "    df = pd.DataFrame(stats, columns=[f\"stat_{i}\" for i in range(len(stats[0]))])\n",
    "    df['id'] = indexes\n",
    "    return df\n",
    "\n",
    "# Function to evaluate the predictions and optimize the thresholds\n",
    "def quadratic_weighted_kappa(y_true, y_pred):\n",
    "    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n",
    "\n",
    "def threshold_Rounder(oof_non_rounded, thresholds):\n",
    "    return np.where(oof_non_rounded < thresholds[0], 0,\n",
    "                    np.where(oof_non_rounded < thresholds[1], 1,\n",
    "                             np.where(oof_non_rounded < thresholds[2], 2, 3)))\n",
    "\n",
    "def evaluate_predictions(thresholds, y_true, oof_non_rounded):\n",
    "    rounded_p = threshold_Rounder(oof_non_rounded, thresholds)\n",
    "    return -quadratic_weighted_kappa(y_true, rounded_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to train the model with processed time series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T10:29:57.555134Z",
     "iopub.status.busy": "2024-12-20T10:29:57.554926Z",
     "iopub.status.idle": "2024-12-20T10:29:57.573411Z",
     "shell.execute_reply": "2024-12-20T10:29:57.572708Z",
     "shell.execute_reply.started": "2024-12-20T10:29:57.555117Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def TrainML(model_class, X, y, test_data):\n",
    "    SKF = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
    "    \n",
    "    train_S = []\n",
    "    test_S = []\n",
    "    \n",
    "    oof_non_rounded = np.zeros(len(y), dtype=float) \n",
    "    oof_rounded = np.zeros(len(y), dtype=int) \n",
    "    test_preds = np.zeros((len(test_data), n_splits))\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(tqdm(SKF.split(X, y), desc=\"Training Folds\", total=n_splits)):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[test_idx]\n",
    "        model = clone(model_class)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_val_pred = model.predict(X_val)\n",
    "\n",
    "        oof_non_rounded[test_idx] = y_val_pred\n",
    "        y_val_pred_rounded = y_val_pred.round(0).astype(int)\n",
    "        oof_rounded[test_idx] = y_val_pred_rounded\n",
    "\n",
    "        train_kappa = quadratic_weighted_kappa(y_train, y_train_pred.round(0).astype(int))\n",
    "        val_kappa = quadratic_weighted_kappa(y_val, y_val_pred_rounded)\n",
    "\n",
    "        train_S.append(train_kappa)\n",
    "        test_S.append(val_kappa)\n",
    "        \n",
    "        test_preds[:, fold] = model.predict(test_data)\n",
    "        \n",
    "        print(f\"Fold {fold+1} - Train QWK: {train_kappa:.4f}, Validation QWK: {val_kappa:.4f}\")\n",
    "        clear_output(wait=True)\n",
    "    print(f\"Mean Train QWK --> {np.mean(train_S):.4f}\")\n",
    "    print(f\"Mean Validation QWK ---> {np.mean(test_S):.4f}\")\n",
    "\n",
    "    KappaOPtimizer = minimize(evaluate_predictions,\n",
    "                              x0=[0.5, 1.5, 2.5], args=(y, oof_non_rounded), \n",
    "                              method='Nelder-Mead')\n",
    "    assert KappaOPtimizer.success, \"Optimization did not converge.\"\n",
    "    print('OPTIMIZED THRESHOLDS', KappaOPtimizer.x)\n",
    "    oof_tuned = threshold_Rounder(oof_non_rounded, KappaOPtimizer.x)\n",
    "    tKappa = quadratic_weighted_kappa(y, oof_tuned)\n",
    "\n",
    "    print(f\"----> || Optimized QWK SCORE :: {Fore.CYAN}{Style.BRIGHT} {tKappa:.3f}{Style.RESET_ALL}\")\n",
    "\n",
    "    tpm = test_preds.mean(axis=1)\n",
    "    tpTuned = threshold_Rounder(tpm, KappaOPtimizer.x)\n",
    "    \n",
    "    submission = pd.DataFrame({\n",
    "        'id': sample['id'],\n",
    "        'sii': tpTuned\n",
    "    })\n",
    "    optimized_thresholds = KappaOPtimizer.x\n",
    "    return submission, oof_tuned, oof_non_rounded, y, optimized_thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T10:29:57.574339Z",
     "iopub.status.busy": "2024-12-20T10:29:57.574140Z",
     "iopub.status.idle": "2024-12-20T10:29:57.660998Z",
     "shell.execute_reply": "2024-12-20T10:29:57.660211Z",
     "shell.execute_reply.started": "2024-12-20T10:29:57.574321Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/train.csv')\n",
    "test = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/test.csv')\n",
    "sample = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/sample_submission.csv')\n",
    "\n",
    "total_features = list(test.columns)\n",
    "total_features.remove('id')\n",
    "\n",
    "cat_c = ['Basic_Demos-Enroll_Season', 'CGAS-Season', 'Physical-Season', \n",
    "          'Fitness_Endurance-Season', 'FGC-Season', 'BIA-Season', \n",
    "          'PAQ_A-Season', 'PAQ_C-Season', 'SDS-Season', 'PreInt_EduHx-Season']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T10:29:57.662067Z",
     "iopub.status.busy": "2024-12-20T10:29:57.661831Z",
     "iopub.status.idle": "2024-12-20T10:29:57.667101Z",
     "shell.execute_reply": "2024-12-20T10:29:57.666345Z",
     "shell.execute_reply.started": "2024-12-20T10:29:57.662044Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n"
     ]
    }
   ],
   "source": [
    "noseason_features = ['Basic_Demos-Age', 'Basic_Demos-Sex',\n",
    "                'CGAS-CGAS_Score', 'Physical-BMI',\n",
    "                'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference',\n",
    "                'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP',\n",
    "                'Fitness_Endurance-Max_Stage',\n",
    "                'Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec',\n",
    "                'FGC-FGC_CU', 'FGC-FGC_CU_Zone', 'FGC-FGC_GSND',\n",
    "                'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone', 'FGC-FGC_PU',\n",
    "                'FGC-FGC_PU_Zone', 'FGC-FGC_SRL', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR',\n",
    "                'FGC-FGC_SRR_Zone', 'FGC-FGC_TL', 'FGC-FGC_TL_Zone',\n",
    "                'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC', 'BIA-BIA_BMI',\n",
    "                'BIA-BIA_BMR', 'BIA-BIA_DEE', 'BIA-BIA_ECW', 'BIA-BIA_FFM',\n",
    "                'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_Fat', 'BIA-BIA_Frame_num',\n",
    "                'BIA-BIA_ICW', 'BIA-BIA_LDM', 'BIA-BIA_LST', 'BIA-BIA_SMM',\n",
    "                'BIA-BIA_TBW', 'PAQ_A-PAQ_A_Total',\n",
    "                'PAQ_C-PAQ_C_Total', 'SDS-SDS_Total_Raw',\n",
    "                'SDS-SDS_Total_T',\n",
    "                'PreInt_EduHx-computerinternet_hoursday', 'BMI_Age','Internet_Hours_Age','BMI_Internet_Hours',\n",
    "                'BFP_BMI', 'FFMI_BFP', 'FMI_BFP', 'LST_TBW', 'BFP_BMR', 'BFP_DEE', 'BMR_Weight', 'DEE_Weight',\n",
    "                'SMM_Height', 'Muscle_to_Fat', 'Hydration_Status', 'ICW_TBW','BMI_PHR']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T10:29:57.668063Z",
     "iopub.status.busy": "2024-12-20T10:29:57.667832Z",
     "iopub.status.idle": "2024-12-20T10:31:05.743645Z",
     "shell.execute_reply": "2024-12-20T10:31:05.742877Z",
     "shell.execute_reply.started": "2024-12-20T10:29:57.668045Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 996/996 [01:07<00:00, 14.68it/s]\n",
      "100%|| 2/2 [00:00<00:00, 12.14it/s]\n"
     ]
    }
   ],
   "source": [
    "train_ts = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_train.parquet\")\n",
    "test_ts = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T10:31:05.744825Z",
     "iopub.status.busy": "2024-12-20T10:31:05.744500Z",
     "iopub.status.idle": "2024-12-20T10:31:15.842604Z",
     "shell.execute_reply": "2024-12-20T10:31:15.841665Z",
     "shell.execute_reply.started": "2024-12-20T10:31:05.744793Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/100], Loss: 1.4613]\n",
      "Epoch [100/100], Loss: 1.4550]\n"
     ]
    }
   ],
   "source": [
    "df_train = train_ts.drop('id', axis=1)\n",
    "df_test = test_ts.drop('id', axis=1)\n",
    "autoencoder, scaler = perform_autoencoder(df_train, encoding_dim=60, epochs=100, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T10:31:15.844417Z",
     "iopub.status.busy": "2024-12-20T10:31:15.843551Z",
     "iopub.status.idle": "2024-12-20T10:31:15.855430Z",
     "shell.execute_reply": "2024-12-20T10:31:15.854731Z",
     "shell.execute_reply.started": "2024-12-20T10:31:15.844391Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_ts_encoded = encode_data(autoencoder, scaler, df_train)\n",
    "test_ts_encoded = encode_data(autoencoder, scaler, df_test)\n",
    "test_ts_encoded.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T10:31:15.856734Z",
     "iopub.status.busy": "2024-12-20T10:31:15.856342Z",
     "iopub.status.idle": "2024-12-20T10:31:15.871239Z",
     "shell.execute_reply": "2024-12-20T10:31:15.870490Z",
     "shell.execute_reply.started": "2024-12-20T10:31:15.856701Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_ts_encoded[\"id\"]=train_ts[\"id\"]\n",
    "test_ts_encoded['id']=test_ts[\"id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additiontal timeseries features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T10:31:15.872143Z",
     "iopub.status.busy": "2024-12-20T10:31:15.871930Z",
     "iopub.status.idle": "2024-12-20T10:31:15.889298Z",
     "shell.execute_reply": "2024-12-20T10:31:15.888669Z",
     "shell.execute_reply.started": "2024-12-20T10:31:15.872112Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "time_series_cols = train_ts.columns.tolist()\n",
    "time_series_cols.remove(\"id\")\n",
    "time_encoded_cols = train_ts_encoded.columns.tolist()\n",
    "time_encoded_cols.remove(\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sleep = pd.read_csv(\"/kaggle/input/sleep-detection/sleep_features.csv\")\n",
    "test_sleep = pd.read_csv(\"/kaggle/working/features/test/sleep_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep_cols = train_sleep.columns.tolist()\n",
    "sleep_cols.remove(\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm -rf /kaggle/working/features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm -rf /kaggle/working/heuristic_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T10:31:15.890335Z",
     "iopub.status.busy": "2024-12-20T10:31:15.890048Z",
     "iopub.status.idle": "2024-12-20T10:31:15.922524Z",
     "shell.execute_reply": "2024-12-20T10:31:15.921963Z",
     "shell.execute_reply.started": "2024-12-20T10:31:15.890300Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_sub1 = pd.merge(train, train_ts_encoded, how=\"left\", on='id')\n",
    "test_sub1 = pd.merge(test, test_ts_encoded, how=\"left\", on='id')\n",
    "train_sub1 = train_sub1.dropna(subset='sii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T10:31:15.923315Z",
     "iopub.status.busy": "2024-12-20T10:31:15.923137Z",
     "iopub.status.idle": "2024-12-20T10:31:15.926728Z",
     "shell.execute_reply": "2024-12-20T10:31:15.925839Z",
     "shell.execute_reply.started": "2024-12-20T10:31:15.923298Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X_sub1 = train_sub1\n",
    "y_sub1 = train_sub1['sii']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T10:40:22.921823Z",
     "iopub.status.busy": "2024-12-20T10:40:22.921494Z",
     "iopub.status.idle": "2024-12-20T10:40:22.951349Z",
     "shell.execute_reply": "2024-12-20T10:40:22.950693Z",
     "shell.execute_reply.started": "2024-12-20T10:40:22.921795Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_sub2 = pd.merge(train, train_sleep, how=\"left\", on='id')\n",
    "test_sub2 = pd.merge(test, test_sleep, how=\"left\", on='id')\n",
    "\n",
    "\n",
    "# pipeline \n",
    "train_sub2 = feature_engineering_tabular(train_sub2)\n",
    "train_sub2 = train_sub2.dropna(subset='sii', ignore_index=True)\n",
    "test_sub2 = feature_engineering_tabular(test_sub2)\n",
    "\n",
    "train_sub2 = train_sub2.drop('id', axis=1)\n",
    "test_sub2  = test_sub2.drop('id', axis=1)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T10:40:23.410663Z",
     "iopub.status.busy": "2024-12-20T10:40:23.410427Z",
     "iopub.status.idle": "2024-12-20T10:40:23.422932Z",
     "shell.execute_reply": "2024-12-20T10:40:23.422149Z",
     "shell.execute_reply.started": "2024-12-20T10:40:23.410641Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "features_sub2 = noseason_features + sleep_cols\n",
    "\n",
    "train_sub2 = pd.merge(train, train_ts, how=\"left\", on='id')\n",
    "test_sub2 = pd.merge(test, test_ts, how=\"left\", on='id')\n",
    "\n",
    "train_sub2 = train_sub2.dropna(subset='sii')\n",
    "if np.any(np.isinf(train_sub2)):\n",
    "    train_sub2 = train_sub2.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "X_sub2 = train_sub2[features_sub2]\n",
    "y_sub2 = train_sub2['sii']\n",
    "test_sub2 = test_sub2[features_sub2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_sub(trial):\n",
    "    CatBoost_Params = {\n",
    "        'learning_rate': trial.suggest_float('catboost_learning_rate', 1e-3, 0.3, log=True),\n",
    "        'depth': trial.suggest_int('catboost_depth', 4, 10),\n",
    "        'random_seed': SEED,\n",
    "        'verbose': 0,\n",
    "        'l2_leaf_reg': trial.suggest_float('catboost_l2_leaf_reg', 0.01, 10.0, log=True),\n",
    "        'iterations': trial.suggest_int('catboost_iterations', 100, 400, 10),\n",
    "        'task_type': 'GPU',  \n",
    "        'devices': '0'      \n",
    "    }\n",
    "    XGB_Params = {\n",
    "        'n_estimators': trial.suggest_int('xgb_n_estimators', 500, 1500, 100),\n",
    "        'max_depth': trial.suggest_int('xgb_max_depth', 1, 10),\n",
    "        'learning_rate': trial.suggest_float('xgb_learning_rate', 0.01, 0.1, log=True),\n",
    "        'subsample': trial.suggest_float('xgb_subsample', 0.1, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('xgb_colsample_bytree', 0.05, 1.0),\n",
    "        'gamma': trial.suggest_float('xgb_gamma', 1e-2, 1.0),\n",
    "        'min_child_weight': trial.suggest_int('xgb_min_child_weight', 1, 100),\n",
    "        'eval_metric': 'rmse',\n",
    "        'objective': 'reg:squarederror',\n",
    "        'tree_method': 'gpu_hist',\n",
    "        'predictor': 'gpu_predictor',\n",
    "        'gpu_id': 0\n",
    "    }\n",
    "    LightGBM_Params = {\n",
    "        'max_depth': trial.suggest_int('lightgbm_max_depth', 3, 12),  # Avoid overly shallow or deep trees\n",
    "        'min_data_in_leaf': trial.suggest_int('lightgbm_min_data_in_leaf', 5, 50),  # Balance between overfitting and splits\n",
    "        'num_leaves': trial.suggest_int('lightgbm_num_leaves', 16, 256),  # Limit complexity to avoid splits failing\n",
    "        'learning_rate': trial.suggest_float('lightgbm_learning_rate', 0.01, 0.1),\n",
    "        'feature_fraction': trial.suggest_float('lightgbm_feature_fraction', 0.7, 1.0),  # Allow feature subsampling tuning\n",
    "        'bagging_fraction': trial.suggest_float('lightgbm_bagging_fraction', 0.7, 1.0),  # Improve generalization\n",
    "        'bagging_freq': trial.suggest_int('lightgbm_bagging_freq', 1, 5),\n",
    "        'lambda_l1': trial.suggest_float('lightgbm_lambda_l1', 0.0, 10.0),  # Regularization tuning\n",
    "        'lambda_l2': trial.suggest_float('lightgbm_lambda_l2', 0.0, 10.0),\n",
    "        'min_gain_to_split': trial.suggest_float('lightgbm_min_gain_to_split', 0.0, 0.1),  # Ensure splits happen\n",
    "        'device_type': 'gpu',\n",
    "        'gpu_device_id': 0,\n",
    "        'verbosity': -1  \n",
    "    }\n",
    "    LightGBM_Model = LGBMRegressor(**LightGBM_Params)\n",
    "    XGB_Model = XGBRegressor(**XGB_Params)\n",
    "    CatBoost_Model = CatBoostRegressor(**CatBoost_Params)\n",
    "    voting_model = VotingRegressor(estimators=[\n",
    "        ('lightgbm', LightGBM_Model),\n",
    "        ('xgboost', XGB_Model),\n",
    "        ('catboost', CatBoost_Model),\n",
    "    ], weights=[4.0, 4.0, 4.0, 5.0])\n",
    "    X = train.drop(['sii'], axis=1)\n",
    "    y = train['sii']\n",
    "\n",
    "    submission1, val_score, _, _, _, _ = TrainML(voting_model, X_sub1, y_sub1, test_sub1)\n",
    "    return val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T10:31:15.927665Z",
     "iopub.status.busy": "2024-12-20T10:31:15.927457Z",
     "iopub.status.idle": "2024-12-20T10:31:15.942766Z",
     "shell.execute_reply": "2024-12-20T10:31:15.942136Z",
     "shell.execute_reply.started": "2024-12-20T10:31:15.927647Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "SVR_Best_Params = {\n",
    "    'C': 0.1,\n",
    "    'epsilon': 0.1,\n",
    "    'kernel': 'rbf',\n",
    "    'gamma': 'scale',\n",
    "}\n",
    "\n",
    "CatBoost_Best_Params = {\n",
    "    'learning_rate': 0.0021172579310639343,\n",
    "    'depth': 6,\n",
    "    'iterations': 130,\n",
    "    'random_seed': SEED,\n",
    "    'verbose': 0,\n",
    "    'l2_leaf_reg': 0.32557701990001503,\n",
    "    'task_type': 'GPU',  \n",
    "    'devices': '0'\n",
    "}\n",
    "\n",
    "XGB_Best_Params = {\n",
    "    'n_estimators': 700,\n",
    "    'max_depth': 4,\n",
    "    'learning_rate': 0.03325152156380898,\n",
    "    'subsample': 0.25295047248406266,\n",
    "    'colsample_bytree': 0.9760859719849787,\n",
    "    'gamma': 0.20085951790463402,\n",
    "    'min_child_weight': 11,\n",
    "    'eval_metric': 'rmse',\n",
    "    'objective': 'reg:squarederror',\n",
    "    'tree_method': 'gpu_hist',\n",
    "    'predictor': 'gpu_predictor',\n",
    "    'gpu_id': 0\n",
    "}\n",
    "\n",
    "LightGBM_Best_Params = {\n",
    "    'max_depth': 3,\n",
    "    'min_data_in_leaf': 40,\n",
    "    'num_leaves': 190,\n",
    "    'learning_rate': 0.05107368421432176,\n",
    "    'feature_fraction': 0.9918350138636185,\n",
    "    'bagging_fraction': 0.9331400899763774,\n",
    "    'bagging_freq': 1,\n",
    "    'lambda_l1': 9.49641646280519,\n",
    "    'lambda_l2': 2.446305429623661,\n",
    "    'min_gain_to_split': 0.05262124930522051,\n",
    "    'device_type': 'gpu',\n",
    "    'gpu_device_id': 0,\n",
    "    'verbosity': -1\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "catboost_model = CatBoostRegressor(**CatBoost_Best_Params)\n",
    "xgb_model = XGBRegressor(**XGB_Best_Params)\n",
    "lightgbm_model = LGBMRegressor(**LightGBM_Best_Params)\n",
    "svr_model = SVR(**SVR_Best_Params)\n",
    "\n",
    "final_voting_model = VotingRegressor(estimators=[\n",
    "    ('lightgbm', lightgbm_model),\n",
    "    ('xgboost', xgb_model),\n",
    "    ('catboost', catboost_model),\n",
    "], weights=[4.0, 4.0, 4.0])\n",
    "\n",
    "X = train.drop(['sii'], axis=1)\n",
    "y = train['sii']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T10:31:15.946554Z",
     "iopub.status.busy": "2024-12-20T10:31:15.946359Z",
     "iopub.status.idle": "2024-12-20T10:31:28.910009Z",
     "shell.execute_reply": "2024-12-20T10:31:28.909289Z",
     "shell.execute_reply.started": "2024-12-20T10:31:15.946537Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Folds: 100%|| 5/5 [00:12<00:00,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Train QWK --> 0.5441\n",
      "Mean Validation QWK ---> 0.3645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPTIMIZED THRESHOLDS [0.5785469  0.88500199 2.83677574]\n",
      "----> || Optimized QWK SCORE :: \u001b[36m\u001b[1m 0.462\u001b[0m\n",
      "Val score sub1 with best parameters: 0.46241743563848414\n"
     ]
    }
   ],
   "source": [
    "submission1, val_score_sub1, _, _, _, _ = TrainML(xgb_model, X_sub1, y_sub1, test_sub1)\n",
    "\n",
    "print(\"Val score sub1 with best parameters:\", val_score_sub1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_submission = submission1\n",
    "final_submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"Submission saved to 'submission.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_submission"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 9643020,
     "sourceId": 81933,
     "sourceType": "competition"
    },
    {
     "datasetId": 921302,
     "sourceId": 7453542,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5742470,
     "sourceId": 9448132,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5742473,
     "sourceId": 9448136,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6324642,
     "sourceId": 10229352,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6327724,
     "sourceId": 10233697,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30823,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
