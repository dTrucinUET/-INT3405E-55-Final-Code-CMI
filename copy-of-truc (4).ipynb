{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9be669f",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-20T05:15:42.213896Z",
     "iopub.status.busy": "2024-12-20T05:15:42.212748Z",
     "iopub.status.idle": "2024-12-20T05:16:10.859413Z",
     "shell.execute_reply": "2024-12-20T05:16:10.858053Z"
    },
    "papermill": {
     "duration": 28.666008,
     "end_time": "2024-12-20T05:16:10.862394",
     "exception": false,
     "start_time": "2024-12-20T05:15:42.196386",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import copy\n",
    "import pickle\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from scipy.optimize import minimize\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "import polars as pl\n",
    "import polars.selectors as cs\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator, FormatStrFormatter, PercentFormatter\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.optimizers import Adam\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from colorama import Fore, Style\n",
    "from IPython.display import clear_output\n",
    "import warnings\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.ensemble import VotingRegressor, RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import shap\n",
    "import plotly.express as px\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.options.display.max_columns = None\n",
    "SEED = 42\n",
    "n_splits = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "989a74dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T05:16:10.893401Z",
     "iopub.status.busy": "2024-12-20T05:16:10.892473Z",
     "iopub.status.idle": "2024-12-20T05:16:10.907387Z",
     "shell.execute_reply": "2024-12-20T05:16:10.906048Z"
    },
    "papermill": {
     "duration": 0.033062,
     "end_time": "2024-12-20T05:16:10.910647",
     "exception": false,
     "start_time": "2024-12-20T05:16:10.877585",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "seed_everything(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0aea8e",
   "metadata": {
    "papermill": {
     "duration": 0.013957,
     "end_time": "2024-12-20T05:16:10.939388",
     "exception": false,
     "start_time": "2024-12-20T05:16:10.925431",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Define function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace1f3f1",
   "metadata": {
    "papermill": {
     "duration": 0.012622,
     "end_time": "2024-12-20T05:16:10.964263",
     "exception": false,
     "start_time": "2024-12-20T05:16:10.951641",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Feature engineer for sub 1,2,3,4,5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b015a05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T05:16:10.993186Z",
     "iopub.status.busy": "2024-12-20T05:16:10.992755Z",
     "iopub.status.idle": "2024-12-20T05:16:11.028153Z",
     "shell.execute_reply": "2024-12-20T05:16:11.026560Z"
    },
    "papermill": {
     "duration": 0.053105,
     "end_time": "2024-12-20T05:16:11.031891",
     "exception": false,
     "start_time": "2024-12-20T05:16:10.978786",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "def feature_engineering_v2(df, selector=None, imputer=None, fit=True):\n",
    "    df = df.loc[:, ~df.columns.duplicated()]\n",
    "    if fit: \n",
    "        y = df['sii']\n",
    "\n",
    "    numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "    season_cols = [col for col in df.columns if 'Season' in col]\n",
    "    pciat_cols = [col for col in df.columns if 'PCIAT' in col and 'Season' not in col]\n",
    "    remaining_numeric_cols = [col for col in numeric_cols if col not in pciat_cols and col not in ['sii']]\n",
    "    X = df[remaining_numeric_cols]\n",
    "    if np.any(np.isinf(X)):\n",
    "        X = X.replace([np.inf, -np.inf], np.nan)\n",
    "    if fit: \n",
    "        imputer = KNNImputer()\n",
    "        imputed_data = imputer.fit_transform(X)\n",
    "        train_imputed = pd.DataFrame(imputed_data, columns=remaining_numeric_cols)\n",
    "        X = train_imputed\n",
    "    else:\n",
    "        X = imputer.transform(X)\n",
    "\n",
    "    if fit:\n",
    "        # estimator = RandomForestRegressor(random_state=42)\n",
    "        # selector = RFECV(estimator, min_features_to_select=5, step=3, cv=5)\n",
    "        selector = SelectKBest(score_func=f_regression, k=30)\n",
    "        X_new = selector.fit_transform(X, y)\n",
    "        selected_features = X.columns[selector.get_support()]\n",
    "    else: \n",
    "        X_new = selector.transform(X)\n",
    "        selected_features = [col for col, selected in zip(remaining_numeric_cols, selector.get_support()) if selected]\n",
    "    df_selected = pd.DataFrame(X_new, columns=selected_features)\n",
    "    return df_selected, selector, imputer\n",
    "\n",
    "def feature_engineering(df):\n",
    "    season_cols = [col for col in df.columns if 'Season' in col]\n",
    "    df = df.drop(season_cols, axis=1) \n",
    "    df['BMI_Age'] = df['Physical-BMI'] * df['Basic_Demos-Age']\n",
    "    df['Internet_Hours_Age'] = df['PreInt_EduHx-computerinternet_hoursday'] * df['Basic_Demos-Age']\n",
    "    df['BMI_Internet_Hours'] = df['Physical-BMI'] * df['PreInt_EduHx-computerinternet_hoursday']\n",
    "    df['BFP_BMI'] = df['BIA-BIA_Fat'] / df['BIA-BIA_BMI']\n",
    "    df['FFMI_BFP'] = df['BIA-BIA_FFMI'] / df['BIA-BIA_Fat']\n",
    "    df['FMI_BFP'] = df['BIA-BIA_FMI'] / df['BIA-BIA_Fat']\n",
    "    df['LST_TBW'] = df['BIA-BIA_LST'] / df['BIA-BIA_TBW']\n",
    "    df['BFP_BMR'] = df['BIA-BIA_Fat'] * df['BIA-BIA_BMR']\n",
    "    df['BFP_DEE'] = df['BIA-BIA_Fat'] * df['BIA-BIA_DEE']\n",
    "    df['BMR_Weight'] = df['BIA-BIA_BMR'] / df['Physical-Weight']\n",
    "    df['DEE_Weight'] = df['BIA-BIA_DEE'] / df['Physical-Weight']\n",
    "    df['SMM_Height'] = df['BIA-BIA_SMM'] / df['Physical-Height']\n",
    "    df['Muscle_to_Fat'] = df['BIA-BIA_SMM'] / df['BIA-BIA_FMI']\n",
    "    df['Hydration_Status'] = df['BIA-BIA_TBW'] / df['Physical-Weight']\n",
    "    df['ICW_TBW'] = df['BIA-BIA_ICW'] / df['BIA-BIA_TBW']\n",
    "    df['BMI_PHR'] = df['Physical-BMI'] * df['Physical-HeartRate']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5772b3b1",
   "metadata": {
    "papermill": {
     "duration": 0.01291,
     "end_time": "2024-12-20T05:16:11.057362",
     "exception": false,
     "start_time": "2024-12-20T05:16:11.044452",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## AutoEncoder for Sub 1,2,3,4,5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b974c53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T05:16:11.087286Z",
     "iopub.status.busy": "2024-12-20T05:16:11.086865Z",
     "iopub.status.idle": "2024-12-20T05:16:11.100953Z",
     "shell.execute_reply": "2024-12-20T05:16:11.099450Z"
    },
    "papermill": {
     "duration": 0.033059,
     "end_time": "2024-12-20T05:16:11.103654",
     "exception": false,
     "start_time": "2024-12-20T05:16:11.070595",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, encoding_dim):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, encoding_dim*3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(encoding_dim*3, encoding_dim*2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(encoding_dim*2, encoding_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_dim, encoding_dim*2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(encoding_dim*2, encoding_dim*3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(encoding_dim*3, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "\n",
    "def perform_autoencoder(df, encoding_dim=50, epochs=50, batch_size=32):\n",
    "    scaler = StandardScaler()\n",
    "    df_scaled = scaler.fit_transform(df)\n",
    "    \n",
    "    data_tensor = torch.FloatTensor(df_scaled)\n",
    "    \n",
    "    input_dim = data_tensor.shape[1]\n",
    "    autoencoder = AutoEncoder(input_dim, encoding_dim)\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(autoencoder.parameters())\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for i in range(0, len(data_tensor), batch_size):\n",
    "            batch = data_tensor[i : i + batch_size]\n",
    "            optimizer.zero_grad()\n",
    "            reconstructed = autoencoder(batch)\n",
    "            loss = criterion(reconstructed, batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}]')    \n",
    "    return autoencoder, scaler\n",
    "\n",
    "def encode_data(autoencoder, scaler, df):\n",
    "    df_scaled = scaler.transform(df)\n",
    "    data_tensor = torch.FloatTensor(df_scaled)\n",
    "    with torch.no_grad():\n",
    "        encoded_data = autoencoder.encoder(data_tensor).numpy()\n",
    "\n",
    "    df_encoded = pd.DataFrame(encoded_data, columns=[f'Enc_{i + 1}' for i in range(encoded_data.shape[1])])\n",
    "    return df_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c1d51c",
   "metadata": {
    "papermill": {
     "duration": 0.014922,
     "end_time": "2024-12-20T05:16:11.133450",
     "exception": false,
     "start_time": "2024-12-20T05:16:11.118528",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## TrainML for Sub 2,3,4,5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82347752",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T05:16:11.164180Z",
     "iopub.status.busy": "2024-12-20T05:16:11.163663Z",
     "iopub.status.idle": "2024-12-20T05:16:11.182397Z",
     "shell.execute_reply": "2024-12-20T05:16:11.181183Z"
    },
    "papermill": {
     "duration": 0.037991,
     "end_time": "2024-12-20T05:16:11.184966",
     "exception": false,
     "start_time": "2024-12-20T05:16:11.146975",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_file(filename, dirname):\n",
    "    df = pd.read_parquet(os.path.join(dirname, filename, 'part-0.parquet'))\n",
    "    df.drop('step', axis=1, inplace=True)\n",
    "    return df.describe().values.reshape(-1), filename.split('=')[1]\n",
    "\n",
    "def load_time_series(dirname) -> pd.DataFrame:\n",
    "    ids = os.listdir(dirname)\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        results = list(tqdm(executor.map(lambda fname: process_file(fname, dirname), ids), total=len(ids)))\n",
    "    stats, indexes = zip(*results)\n",
    "    df = pd.DataFrame(stats, columns=[f\"stat_{i}\" for i in range(len(stats[0]))])\n",
    "    df['id'] = indexes\n",
    "    return df\n",
    "\n",
    "def update(df):\n",
    "    global cat_c\n",
    "    for c in cat_c: \n",
    "        df[c] = df[c].fillna('Missing')\n",
    "        df[c] = df[c].astype('category')\n",
    "    return df\n",
    "\n",
    "def create_mapping(column, dataset):\n",
    "    unique_values = dataset[column].unique()\n",
    "    return {value: idx for idx, value in enumerate(unique_values)}\n",
    "\n",
    "def quadratic_weighted_kappa(y_true, y_pred):\n",
    "    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n",
    "\n",
    "def threshold_Rounder(oof_non_rounded, thresholds):\n",
    "    return np.where(oof_non_rounded < thresholds[0], 0,\n",
    "                    np.where(oof_non_rounded < thresholds[1], 1,\n",
    "                             np.where(oof_non_rounded < thresholds[2], 2, 3)))\n",
    "\n",
    "def evaluate_predictions(thresholds, y_true, oof_non_rounded):\n",
    "    rounded_p = threshold_Rounder(oof_non_rounded, thresholds)\n",
    "    return -quadratic_weighted_kappa(y_true, rounded_p)\n",
    "    \n",
    "def TrainML(model_class, X, y, test_data):\n",
    "    SKF = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
    "    \n",
    "    train_S = []\n",
    "    test_S = []\n",
    "    \n",
    "    oof_non_rounded = np.zeros(len(y), dtype=float) \n",
    "    oof_rounded = np.zeros(len(y), dtype=int) \n",
    "    test_preds = np.zeros((len(test_data), n_splits))\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(tqdm(SKF.split(X, y), desc=\"Training Folds\", total=n_splits)):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[test_idx]\n",
    "        model = clone(model_class)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_val_pred = model.predict(X_val)\n",
    "\n",
    "        oof_non_rounded[test_idx] = y_val_pred\n",
    "        y_val_pred_rounded = y_val_pred.round(0).astype(int)\n",
    "        oof_rounded[test_idx] = y_val_pred_rounded\n",
    "\n",
    "        train_kappa = quadratic_weighted_kappa(y_train, y_train_pred.round(0).astype(int))\n",
    "        val_kappa = quadratic_weighted_kappa(y_val, y_val_pred_rounded)\n",
    "\n",
    "        train_S.append(train_kappa)\n",
    "        test_S.append(val_kappa)\n",
    "        \n",
    "        test_preds[:, fold] = model.predict(test_data)\n",
    "        \n",
    "        print(f\"Fold {fold+1} - Train QWK: {train_kappa:.4f}, Validation QWK: {val_kappa:.4f}\")\n",
    "        clear_output(wait=True)\n",
    "    print(f\"Mean Train QWK --> {np.mean(train_S):.4f}\")\n",
    "    print(f\"Mean Validation QWK ---> {np.mean(test_S):.4f}\")\n",
    "\n",
    "    KappaOPtimizer = minimize(evaluate_predictions,\n",
    "                              x0=[0.5, 1.5, 2.5], args=(y, oof_non_rounded), \n",
    "                              method='Nelder-Mead')\n",
    "    assert KappaOPtimizer.success, \"Optimization did not converge.\"\n",
    "    print('OPTIMIZED THRESHOLDS', KappaOPtimizer.x)\n",
    "    oof_tuned = threshold_Rounder(oof_non_rounded, KappaOPtimizer.x)\n",
    "    tKappa = quadratic_weighted_kappa(y, oof_tuned)\n",
    "\n",
    "    print(f\"----> || Optimized QWK SCORE :: {Fore.CYAN}{Style.BRIGHT} {tKappa:.3f}{Style.RESET_ALL}\")\n",
    "\n",
    "    tpm = test_preds.mean(axis=1)\n",
    "    tpTuned = threshold_Rounder(tpm, KappaOPtimizer.x)\n",
    "    \n",
    "    submission = pd.DataFrame({\n",
    "        'id': sample['id'],\n",
    "        'sii': tpTuned\n",
    "    })\n",
    "    optimized_thresholds = KappaOPtimizer.x\n",
    "    return submission, oof_tuned, oof_non_rounded, y, optimized_thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d79ed762",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T05:16:11.220142Z",
     "iopub.status.busy": "2024-12-20T05:16:11.219686Z",
     "iopub.status.idle": "2024-12-20T05:16:11.238521Z",
     "shell.execute_reply": "2024-12-20T05:16:11.237122Z"
    },
    "papermill": {
     "duration": 0.04116,
     "end_time": "2024-12-20T05:16:11.240983",
     "exception": false,
     "start_time": "2024-12-20T05:16:11.199823",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def TrainML_Sub1(model_class, X, y, test_data):\n",
    "    SKF = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
    "    \n",
    "    train_S = []\n",
    "    test_S = []\n",
    "    \n",
    "    oof_non_rounded = np.zeros(len(y), dtype=float) \n",
    "    oof_rounded = np.zeros(len(y), dtype=int) \n",
    "    test_preds = np.zeros((len(test_data), n_splits))\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(tqdm(SKF.split(X, y), desc=\"Training Folds\", total=n_splits)):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[test_idx]\n",
    "        \n",
    "        # Feature engineering for training and validation\n",
    "        X_train, selector_tr, imputer_tr = feature_engineering_v2(X_train, fit=True)\n",
    "        X_val, _, _ = feature_engineering_v2(X_val, selector_tr, imputer_tr, fit=False)\n",
    "        # Train the model\n",
    "        model = clone(model_class)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_val_pred = model.predict(X_val)\n",
    "\n",
    "        oof_non_rounded[test_idx] = y_val_pred\n",
    "        y_val_pred_rounded = y_val_pred.round(0).astype(int)\n",
    "        oof_rounded[test_idx] = y_val_pred_rounded\n",
    "\n",
    "        train_kappa = quadratic_weighted_kappa(y_train, y_train_pred.round(0).astype(int))\n",
    "        val_kappa = quadratic_weighted_kappa(y_val, y_val_pred_rounded)\n",
    "\n",
    "        train_S.append(train_kappa)\n",
    "        test_S.append(val_kappa)\n",
    "        \n",
    "        # Feature engineering for test data\n",
    "        test_data_fe, _, _ = feature_engineering_v2(test_data, selector_tr, imputer_tr, fit=False)\n",
    "        test_preds[:, fold] = model.predict(test_data_fe)\n",
    "        \n",
    "        print(f\"Fold {fold+1} - Train QWK: {train_kappa:.4f}, Validation QWK: {val_kappa:.4f}\")\n",
    "        clear_output(wait=True)\n",
    "    \n",
    "    print(f\"Mean Train QWK --> {np.mean(train_S):.4f}\")\n",
    "    print(f\"Mean Validation QWK ---> {np.mean(test_S):.4f}\")\n",
    "\n",
    "    KappaOptimizer = minimize(evaluate_predictions,\n",
    "                              x0=[0.5, 1.5, 2.5], args=(y, oof_non_rounded), \n",
    "                              method='Nelder-Mead')\n",
    "    assert KappaOptimizer.success, \"Optimization did not converge.\"\n",
    "    print('OPTIMIZED THRESHOLDS', KappaOptimizer.x)\n",
    "    \n",
    "    oof_tuned = threshold_Rounder(oof_non_rounded, KappaOptimizer.x)\n",
    "    tKappa = quadratic_weighted_kappa(y, oof_tuned)\n",
    "\n",
    "    print(f\"----> || Optimized QWK SCORE :: {Fore.CYAN}{Style.BRIGHT} {tKappa:.3f}{Style.RESET_ALL}\")\n",
    "\n",
    "    tpm = test_preds.mean(axis=1)\n",
    "    tpTuned = threshold_Rounder(tpm, KappaOptimizer.x)\n",
    "    \n",
    "    submission = pd.DataFrame({\n",
    "        'id': sample['id'],\n",
    "        'sii': tpTuned\n",
    "    })\n",
    "    optimized_thresholds = KappaOptimizer.x\n",
    "    return (submission, tKappa, oof_tuned, oof_non_rounded, y, optimized_thresholds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afe629c",
   "metadata": {
    "papermill": {
     "duration": 0.012583,
     "end_time": "2024-12-20T05:16:11.266982",
     "exception": false,
     "start_time": "2024-12-20T05:16:11.254399",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Functions MAE for Sub 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f236dd53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T05:16:11.294810Z",
     "iopub.status.busy": "2024-12-20T05:16:11.294360Z",
     "iopub.status.idle": "2024-12-20T05:16:16.335940Z",
     "shell.execute_reply": "2024-12-20T05:16:16.334493Z"
    },
    "papermill": {
     "duration": 5.059774,
     "end_time": "2024-12-20T05:16:16.339834",
     "exception": false,
     "start_time": "2024-12-20T05:16:11.280060",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from tkinter import E\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from timm.models.vision_transformer import Block\n",
    "import numpy as np\n",
    "import torch, os\n",
    "from torch import nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import math\n",
    "import argparse\n",
    "\n",
    "class MaskEmbed(nn.Module):\n",
    "    def __init__(self, rec_len=25, embed_dim=64, norm_layer=None): \n",
    "        super().__init__()\n",
    "        self.rec_len = rec_len\n",
    "        self.proj = nn.Conv1d(1, embed_dim, kernel_size=1, stride=1)\n",
    "        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, _, L = x.shape\n",
    "        x = self.proj(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ActiveEmbed(nn.Module):\n",
    "    def __init__(self, rec_len=25, embed_dim=64, norm_layer=None): \n",
    "        super().__init__()\n",
    "        self.rec_len = rec_len\n",
    "        self.proj = nn.Conv1d(1, embed_dim, kernel_size=1, stride=1)\n",
    "        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, _, L = x.shape\n",
    "        x = self.proj(x)\n",
    "        x = torch.sin(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "def get_1d_sincos_pos_embed(embed_dim, pos, cls_token=False):\n",
    "    \"\"\"\n",
    "    embed_dim: output dimension for each position\n",
    "    pos: a list of positions to be encoded: size (M,)\n",
    "    out: (M, D)\n",
    "    \"\"\"\n",
    "    assert embed_dim % 2 == 0\n",
    "    omega = np.arange(embed_dim // 2, dtype=np.float32)\n",
    "    omega /= embed_dim / 2.\n",
    "    omega = 1. / 10000**omega  # (D/2,)\n",
    "\n",
    "    pos = np.arange(pos)  # (M,)\n",
    "    out = np.einsum('m,d->md', pos, omega)  # (M, D/2), outer product\n",
    "\n",
    "    emb_sin = np.sin(out) # (M, D/2)\n",
    "    emb_cos = np.cos(out) # (M, D/2)\n",
    "\n",
    "    pos_embed = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)\n",
    "\n",
    "    if cls_token:\n",
    "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
    "\n",
    "    return pos_embed\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch, lr, min_lr, max_epochs, warmup_epochs):\n",
    "    \"\"\"Decay the learning rate with half-cycle cosine after warmup\"\"\"\n",
    "    if epoch < warmup_epochs:\n",
    "        tmp_lr = lr * epoch / warmup_epochs \n",
    "    else:\n",
    "        tmp_lr = min_lr + (lr - min_lr) * 0.5 * \\\n",
    "            (1. + math.cos(math.pi * (epoch - warmup_epochs) / (max_epochs - warmup_epochs)))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        if \"lr_scale\" in param_group:\n",
    "            param_group[\"lr\"] = tmp_lr * param_group[\"lr_scale\"]\n",
    "        else:\n",
    "            param_group[\"lr\"] = tmp_lr\n",
    "    return tmp_lr\n",
    "\n",
    "\n",
    "def get_grad_norm_(parameters, norm_type: float = 2.0) -> torch.Tensor:\n",
    "    if isinstance(parameters, torch.Tensor):\n",
    "        parameters = [parameters]\n",
    "    parameters = [p for p in parameters if p.grad is not None]\n",
    "    norm_type = float(norm_type)\n",
    "    if len(parameters) == 0:\n",
    "        return torch.tensor(0.)\n",
    "    device = parameters[0].grad.device\n",
    "    if norm_type == np.inf:\n",
    "        total_norm = max(p.grad.detach().abs().max().to(device) for p in parameters)\n",
    "    else:\n",
    "        total_norm = torch.norm(torch.stack([torch.norm(p.grad.detach(), norm_type).to(device) for p in parameters]), norm_type)\n",
    "    return total_norm\n",
    "\n",
    "\n",
    "class NativeScaler:\n",
    "    state_dict_key = \"amp_scaler\"\n",
    "    def __init__(self):\n",
    "        self._scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    def __call__(self, loss, optimizer, clip_grad=None, parameters=None, create_graph=False, update_grad=True):\n",
    "        self._scaler.scale(loss).backward(create_graph=create_graph)\n",
    "        if update_grad:\n",
    "            if clip_grad is not None:\n",
    "                assert parameters is not None\n",
    "                self._scaler.unscale_(optimizer)  # unscale the gradients of optimizer's assigned params in-place\n",
    "                norm = torch.nn.utils.clip_grad_norm_(parameters, clip_grad)\n",
    "            else:\n",
    "                self._scaler.unscale_(optimizer)\n",
    "                norm = get_grad_norm_(parameters)\n",
    "            self._scaler.step(optimizer)\n",
    "            self._scaler.update()\n",
    "        else:\n",
    "            norm = None\n",
    "        return norm\n",
    "\n",
    "    def state_dict(self):\n",
    "        return self._scaler.state_dict()\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        self._scaler.load_state_dict(state_dict)\n",
    "\n",
    "\n",
    "\n",
    "class MAEDataset(Dataset):\n",
    "    def __init__(self, X, M):        \n",
    "         self.X = X\n",
    "         self.M = M\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        return self.X[idx], self.M[idx]\n",
    "\n",
    "\n",
    "\n",
    "def get_dataset(dataset : str, path : str):\n",
    "    if dataset in ['climate', 'compression', 'wine', 'yacht', 'spam', 'letter', 'credit', 'raisin', 'bike', 'obesity', 'airfoil', 'blood', 'yeast', 'health', 'review', 'travel']:\n",
    "        df = pd.read_csv(os.path.join(path, 'data', dataset + '.csv'))\n",
    "        last_col = df.columns[-1]\n",
    "        y = df[last_col]\n",
    "        X = df.drop(columns=[last_col])\n",
    "    elif dataset == 'california':\n",
    "        from sklearn.datasets import fetch_california_housing\n",
    "        X, y = fetch_california_housing(as_frame=True, return_X_y=True)\n",
    "    elif dataset == 'diabetes':\n",
    "        from sklearn.datasets import load_diabetes\n",
    "        X, y = load_diabetes(as_frame=True, return_X_y=True)\n",
    "    elif dataset == 'iris':\n",
    "        # only for testing\n",
    "        from sklearn.datasets import load_iris\n",
    "        X, y = load_iris(as_frame=True, return_X_y=True)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "eps = 1e-6\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.nn import TransformerEncoderLayer\n",
    "\n",
    "class MaskedAutoencoder(nn.Module):\n",
    "    def __init__(self, rec_len=25, embed_dim=64, depth=4, num_heads=4,\n",
    "                 decoder_embed_dim=64, decoder_depth=2, decoder_num_heads=4,\n",
    "                 mlp_ratio=4., norm_layer=nn.LayerNorm, norm_field_loss=False,\n",
    "                 encode_func='linear'):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.rec_len = rec_len\n",
    "        self.embed_dim = embed_dim\n",
    "        self.norm_field_loss = norm_field_loss\n",
    "        \n",
    "        # Encoder\n",
    "        if encode_func == 'active':\n",
    "            self.mask_embed = ActiveEmbed(rec_len, embed_dim, norm_layer)\n",
    "        else:\n",
    "            self.mask_embed = MaskEmbed(rec_len, embed_dim, norm_layer)\n",
    "        \n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, rec_len + 1, embed_dim), requires_grad=False)\n",
    "        \n",
    "        encoder_layer = TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=int(embed_dim * mlp_ratio),\n",
    "                        dropout=0.0, batch_first=True)\n",
    "        self.blocks = nn.TransformerEncoder(encoder_layer, depth)\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder_embed = nn.Linear(embed_dim, decoder_embed_dim, bias=True)\n",
    "        self.mask_token = nn.Parameter(torch.zeros(1, 1, decoder_embed_dim))\n",
    "        self.decoder_pos_embed = nn.Parameter(torch.zeros(1, rec_len + 1, decoder_embed_dim), requires_grad=False)\n",
    "        \n",
    "        decoder_layer = TransformerEncoderLayer(d_model=decoder_embed_dim, nhead=decoder_num_heads, dim_feedforward=int(decoder_embed_dim * mlp_ratio),\n",
    "                        dropout=0.0, batch_first=True)\n",
    "        self.decoder_blocks = nn.TransformerEncoder(decoder_layer, decoder_depth)\n",
    "        self.decoder_norm = norm_layer(decoder_embed_dim)\n",
    "        self.decoder_pred = nn.Linear(decoder_embed_dim, 1, bias=True)\n",
    "        \n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        pos_embed = get_1d_sincos_pos_embed(self.pos_embed.shape[-1], self.rec_len, cls_token=True)\n",
    "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
    "        \n",
    "        decoder_pos_embed = get_1d_sincos_pos_embed(self.decoder_pos_embed.shape[-1], self.rec_len, cls_token=True)\n",
    "        self.decoder_pos_embed.data.copy_(torch.from_numpy(decoder_pos_embed).float().unsqueeze(0))\n",
    "        \n",
    "        torch.nn.init.xavier_uniform_(self.mask_embed.proj.weight.view([self.mask_embed.proj.weight.shape[0], -1]))\n",
    "        torch.nn.init.normal_(self.cls_token, std=.02)\n",
    "        torch.nn.init.normal_(self.mask_token, std=.02)\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def random_masking(self, x, m, mask_ratio, training=None):\n",
    "        N, L, D = x.shape\n",
    "        if training is None:\n",
    "            training = self.training\n",
    "        if training:\n",
    "            len_keep = int(L * (1 - mask_ratio))\n",
    "            noise = torch.rand(N, L, device=x.device)\n",
    "            noise[m < 1e-6] = 1\n",
    "            ids_shuffle = torch.argsort(noise, dim=1)\n",
    "            ids_restore = torch.argsort(ids_shuffle, dim=1)\n",
    "            ids_keep = ids_shuffle[:, :len_keep]\n",
    "            mask = torch.ones([N, L], device=x.device)\n",
    "            mask[:, :len_keep] = 0\n",
    "            mask = torch.gather(mask, dim=1, index=ids_restore)\n",
    "            mask = torch.logical_or(mask, ~m.bool())\n",
    "            nask = ~mask\n",
    "            return mask, nask\n",
    "        else:\n",
    "            mask = ~m.bool()\n",
    "            nask = m.bool()\n",
    "            return mask, nask\n",
    "\n",
    "    def forward_encoder(self, x, m, mask_ratio=0.5, training=None):\n",
    "        x = self.mask_embed(x)\n",
    "        x = x + self.pos_embed[:, 1:, :]\n",
    "        mask, nask = self.random_masking(x, m, mask_ratio, training)\n",
    "        x = x * (~mask.unsqueeze(-1)).float()\n",
    "        cls_token = self.cls_token + self.pos_embed[:, :1, :]\n",
    "        cls_tokens = cls_token.expand(x.shape[0], -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        attn_mask = torch.cat((torch.zeros(x.shape[0], 1, device=x.device), mask), dim=1)\n",
    "        x = self.blocks(src=x, src_key_padding_mask=attn_mask.bool())\n",
    "        x = self.norm(x)\n",
    "        return x, mask, nask\n",
    "\n",
    "    def forward_decoder(self, x, mask):\n",
    "        x = self.decoder_embed(x)\n",
    "        x = x + self.decoder_pos_embed\n",
    "        mask_with_cls = torch.cat((torch.zeros(x.shape[0], 1, device=x.device), mask), dim=1)\n",
    "        x = self.blocks(src=x, src_key_padding_mask=mask_with_cls.bool())\n",
    "        \n",
    "        x = self.decoder_norm(x)\n",
    "        x = self.decoder_pred(x)\n",
    "        x = x[:, 1:, :].sigmoid()\n",
    "        return x\n",
    "\n",
    "    def forward_loss(self, data, pred, mask, nask):\n",
    "        target = data.squeeze(dim=1)\n",
    "        loss = (pred.squeeze(dim=2) - target) ** 2\n",
    "        loss = (loss * mask).sum() / (mask.sum() + 1e-6) + (loss * nask).sum() / (nask.sum() + 1e-6)\n",
    "        return loss\n",
    "\n",
    "    def forward(self, data, m, mask_ratio=0.5, training=None):\n",
    "        x, mask, nask = self.forward_encoder(data, m, mask_ratio, training)\n",
    "        pred = self.forward_decoder(x, mask)\n",
    "        loss = self.forward_loss(data, pred, mask, nask)\n",
    "        return loss, pred, mask, nask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6722eb7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T05:16:16.384798Z",
     "iopub.status.busy": "2024-12-20T05:16:16.383869Z",
     "iopub.status.idle": "2024-12-20T05:16:16.430134Z",
     "shell.execute_reply": "2024-12-20T05:16:16.428600Z"
    },
    "papermill": {
     "duration": 0.071547,
     "end_time": "2024-12-20T05:16:16.434029",
     "exception": false,
     "start_time": "2024-12-20T05:16:16.362482",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def eval(model, val_loader, lbl_idx=None):\n",
    "    total_loss = 0.0\n",
    "    cnt = 1e-2\n",
    "    for (samples, masks) in val_loader:\n",
    "        with torch.no_grad():\n",
    "            samples = samples.unsqueeze(dim=1)\n",
    "            samples = samples.to(device, non_blocking=True)\n",
    "            masks = masks.to(device, non_blocking=True)                \n",
    "                \n",
    "            if lbl_idx is not None:\n",
    "                input_samples = samples.clone()[masks[:, lbl_idx].bool()]\n",
    "                input_masks = masks.clone()[masks[:, lbl_idx].bool()]\n",
    "                if len(input_samples) == 0: continue\n",
    "                target = input_samples[:, 0, lbl_idx].detach().clone().reshape(-1)\n",
    "                input_samples[:, :, lbl_idx] = 0.0\n",
    "                input_masks[:, lbl_idx] = 0\n",
    "                \n",
    "                _, pred, _, _ = model(input_samples, input_masks, 0.5, False)\n",
    "\n",
    "                pred=pred[:, lbl_idx, 0].reshape(-1)\n",
    "                loss = -quadratic_weighted_kappa(\n",
    "                  (target.cpu().numpy() * 3.).round(0).astype(int),\n",
    "                  (pred.cpu().numpy() * 3.).round(0).astype(int),\n",
    "                )\n",
    "            else:\n",
    "                loss, _, _, _ = model(samples, masks, mask_ratio=0.5)\n",
    "              \n",
    "        total_loss += loss.item()\n",
    "        cnt += 1\n",
    "    return total_loss / cnt\n",
    "\n",
    "\n",
    "eps = 1e-8\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "from argparse import Namespace\n",
    "remasker_args = Namespace(\n",
    "    batch_size=64,\n",
    "    max_epochs= 600,\n",
    "    accum_iter=1,\n",
    "    mask_ratio=0.5,\n",
    "    embed_dim=32,\n",
    "    depth=6,\n",
    "    decoder_depth=4,\n",
    "    num_heads=4,\n",
    "    mlp_ratio=4.0,\n",
    "    encode_func='linear',\n",
    "    norm_field_loss=False,\n",
    "    weight_decay=0.05,\n",
    "    lr=None, blr=0.001,\n",
    "    min_lr=1e-05,\n",
    "    warmup_epochs=40,\n",
    "    device='cuda', seed=SEED, overwrite=True, pin_mem=True\n",
    ")\n",
    "\n",
    "\n",
    "class ReMasker:\n",
    "\n",
    "    def __init__(self, args=remasker_args):\n",
    "\n",
    "        self.batch_size = args.batch_size\n",
    "        self.accum_iter = args.accum_iter\n",
    "        self.min_lr = args.min_lr\n",
    "        self.norm_field_loss = args.norm_field_loss\n",
    "        self.weight_decay = args.weight_decay\n",
    "        self.lr = args.lr\n",
    "        self.blr = args.blr\n",
    "        self.warmup_epochs = args.warmup_epochs\n",
    "        self.model = None\n",
    "        self.norm_parameters = None\n",
    "\n",
    "        self.embed_dim = args.embed_dim\n",
    "        self.depth = args.depth\n",
    "        self.decoder_depth = args.decoder_depth\n",
    "        self.num_heads = args.num_heads\n",
    "        self.mlp_ratio = args.mlp_ratio\n",
    "        self.max_epochs = args.max_epochs\n",
    "        self.mask_ratio = args.mask_ratio\n",
    "        self.encode_func = args.encode_func\n",
    "\n",
    "    def fit(self, X_raw: pd.DataFrame, X_val=None, lbl_idx=None):\n",
    "        global dbg_var\n",
    "        X = X_raw.clone()\n",
    "\n",
    "        # Parameters\n",
    "        no = len(X)\n",
    "        dim = len(X[0, :])\n",
    "\n",
    "        X = X.cpu()\n",
    "\n",
    "        min_val = np.zeros(dim)\n",
    "        max_val = np.zeros(dim)\n",
    "\n",
    "        for i in range(dim):\n",
    "            min_val[i] = np.nanmin(X[:, i])\n",
    "            max_val[i] = np.nanmax(X[:, i])\n",
    "            X[:, i] = (X[:, i] - min_val[i]) / (max_val[i] - min_val[i] + eps)\n",
    "\n",
    "        self.norm_parameters = {\"min\": min_val, \"max\": max_val}\n",
    "\n",
    "        # Set missing\n",
    "        M = 1 - (1 * (np.isnan(X)))\n",
    "        M = M.float().to(device)\n",
    "\n",
    "        X = torch.nan_to_num(X)\n",
    "        X = X.to(device)\n",
    "\n",
    "        self.model = MaskedAutoencoder(\n",
    "            rec_len=dim,\n",
    "            embed_dim=self.embed_dim,\n",
    "            depth=self.depth,\n",
    "            num_heads=self.num_heads,\n",
    "            decoder_embed_dim=self.embed_dim,\n",
    "            decoder_depth=self.decoder_depth,\n",
    "            decoder_num_heads=self.num_heads,\n",
    "            mlp_ratio=self.mlp_ratio,\n",
    "            norm_layer=partial(nn.LayerNorm, eps=eps),\n",
    "            norm_field_loss=self.norm_field_loss,\n",
    "            encode_func=self.encode_func\n",
    "        )\n",
    "        self.model.to(device)\n",
    "        eff_batch_size = self.batch_size * self.accum_iter\n",
    "        if self.lr is None: \n",
    "            self.lr = self.blr * eff_batch_size / 64\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.lr, betas=(0.9, 0.95))\n",
    "        loss_scaler = NativeScaler()\n",
    "\n",
    "        dataset = MAEDataset(X, M)\n",
    "        dataloader = DataLoader(\n",
    "            dataset, sampler=RandomSampler(dataset),\n",
    "            batch_size=self.batch_size,\n",
    "        )\n",
    "\n",
    "        best_loss = 1e9\n",
    "        best_model = copy.deepcopy(self.model)\n",
    "        for epoch in range(self.max_epochs):\n",
    "            self.model.train()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            total_loss = 0\n",
    "            lbl_loss = 0.\n",
    "\n",
    "            iter = 0\n",
    "            for iter, (samples, masks) in enumerate(dataloader):\n",
    "                if iter % self.accum_iter == 0:\n",
    "                    adjust_learning_rate(optimizer, iter / len(dataloader) + epoch, self.lr, self.min_lr,\n",
    "                                         self.max_epochs, self.warmup_epochs)\n",
    "\n",
    "                samples = samples.unsqueeze(dim=1)\n",
    "                samples = samples.to(device, non_blocking=True)\n",
    "                masks = masks.to(device, non_blocking=True)\n",
    "\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    loss, _, _, _ = self.model(samples, masks, mask_ratio=self.mask_ratio)\n",
    "\n",
    "                    if lbl_idx is not None:\n",
    "                        input_samples = samples.clone()[masks[:, lbl_idx].bool()]\n",
    "                        input_masks = masks.clone()[masks[:, lbl_idx].bool()]\n",
    "                        if len(input_samples) == 0: continue\n",
    "                        \n",
    "                        target = input_samples[:, 0, lbl_idx].detach().clone().reshape(-1)\n",
    "                        input_samples[:, :, lbl_idx] = 0.0\n",
    "                        input_masks[:, lbl_idx] = 0\n",
    "                        \n",
    "                        _, pred, _, _ = self.model(input_samples, input_masks, 0.5, False)\n",
    "                        pred = pred[:, lbl_idx, 0].reshape(-1)\n",
    "                        lbl_weight = 1\n",
    "                        loss += lbl_weight * F.mse_loss(pred, target)\n",
    "                        lbl_loss += F.mse_loss(pred, target).item()\n",
    "                            \n",
    "                        \n",
    "                    loss_value = loss.item()\n",
    "                    total_loss += loss_value\n",
    "                if not math.isfinite(loss_value):\n",
    "                    print(\"Loss is {}, stopping training\".format(loss_value))\n",
    "                    dbg_var = (samples, masks)\n",
    "                    sys.exit(1)\n",
    "\n",
    "                loss /= self.accum_iter\n",
    "                loss_scaler(loss, optimizer, parameters=self.model.parameters(),\n",
    "                            update_grad=(iter + 1) % self.accum_iter == 0)\n",
    "\n",
    "                if (iter + 1) % self.accum_iter == 0:\n",
    "                    optimizer.zero_grad()\n",
    "            total_loss = (total_loss / (iter + 1))\n",
    "            self.model.eval()\n",
    "            val_loss = self.evaluate(X_val, lbl_idx)\n",
    "            if val_loss <= best_loss:\n",
    "                best_loss = val_loss\n",
    "                best_model = copy.deepcopy(self.model)\n",
    "            if (epoch + 1) % max(1, self.max_epochs // 10) == 0 or epoch == 0:\n",
    "                lbl_loss = lbl_loss / (iter + 1)\n",
    "                print(\"Epoch: %d, train;val;best qwk: %.4f;%.4f;%.4f, loss: %.4f, lbl_loss: %.4f, val_loss: %.4f\" % \n",
    "                    (epoch+1, -self.evaluate(X_raw, lbl_idx), -val_loss, -best_loss, total_loss, lbl_loss, val_loss)\n",
    "                )\n",
    "                \n",
    "\n",
    "        self.model = best_model\n",
    "        print(f'Loaded best model with loss={best_loss:.4f}')\n",
    "        return self      \n",
    "      \n",
    "    def evaluate(self, X_raw: torch.Tensor, lbl_idx):\n",
    "        keep_indices = torch.where(~X_raw[:, lbl_idx].isnan())[0]\n",
    "        X_raw = X_raw[keep_indices]\n",
    "        gt = X_raw[:, lbl_idx].cpu().numpy().round(0).astype(int)\n",
    "        X_raw[:, lbl_idx] = float('nan')\n",
    "        yp = self.predict(X_raw, lbl_idx)\n",
    "        yp = yp.cpu().numpy().round(0).astype(int)\n",
    "        return -quadratic_weighted_kappa(gt, yp)\n",
    "      \n",
    "      \n",
    "    def predict(self, X_raw: torch.Tensor, lbl_idx, bs=None):\n",
    "        X_raw = torch.tensor(X_raw, dtype=torch.float32)\n",
    "        \n",
    "        # Normalize the input data\n",
    "        min_val = self.norm_parameters[\"min\"]\n",
    "        max_val = self.norm_parameters[\"max\"]\n",
    "        X = X_raw.clone()\n",
    "        for i in range(X.shape[1]):\n",
    "            X[:, i] = (X[:, i] - min_val[i]) / (max_val[i] - min_val[i] + eps)\n",
    "        \n",
    "        M = (1 - (1 * torch.isnan(X))).float().to(device)\n",
    "        \n",
    "        X = torch.nan_to_num(X)\n",
    "        X = X.to(device)\n",
    "        \n",
    "        if bs == None: bs = self.batch_size\n",
    "        # Prepare DataLoader\n",
    "        dataset = MAEDataset(X, M)\n",
    "        dataloader = DataLoader(dataset, batch_size=bs, shuffle=False)\n",
    "        \n",
    "        # Ensure model is in evaluation mode\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Tensor to hold predictions\n",
    "        predictions = torch.zeros(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_samples, batch_masks in dataloader:\n",
    "                # Prepare input for the model\n",
    "                batch_samples = batch_samples.unsqueeze(dim=1).to(device)\n",
    "                batch_masks = batch_masks.to(device)\n",
    "                \n",
    "                # Forward pass with training=False\n",
    "                _, pred, _, _ = self.model(batch_samples, batch_masks, mask_ratio=0.5, training=False)\n",
    "                \n",
    "                pred = pred.squeeze(dim=2)[:, lbl_idx]\n",
    "                \n",
    "                predictions = torch.cat((predictions, pred), 0)\n",
    "        \n",
    "        return predictions * 3.\n",
    "\n",
    "    def fit_transform(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Imputes the provided dataset using the GAIN strategy.\n",
    "        Args:\n",
    "            X: np.ndarray\n",
    "                A dataset with missing values.\n",
    "        Returns:\n",
    "            Xhat: The imputed dataset.\n",
    "        \"\"\"\n",
    "        X = torch.tensor(X.values, dtype=torch.float32)\n",
    "        return self.fit(X).transform(X).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d54775d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T05:16:16.463094Z",
     "iopub.status.busy": "2024-12-20T05:16:16.462649Z",
     "iopub.status.idle": "2024-12-20T05:16:16.469133Z",
     "shell.execute_reply": "2024-12-20T05:16:16.467860Z"
    },
    "papermill": {
     "duration": 0.025249,
     "end_time": "2024-12-20T05:16:16.472155",
     "exception": false,
     "start_time": "2024-12-20T05:16:16.446906",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def random_extend(arr, k):\n",
    "    indices = np.concatenate([np.random.permutation(len(arr)) for _ in range(10)])[:k]\n",
    "    return arr[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67ae9297",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T05:16:16.498999Z",
     "iopub.status.busy": "2024-12-20T05:16:16.498550Z",
     "iopub.status.idle": "2024-12-20T05:16:16.516683Z",
     "shell.execute_reply": "2024-12-20T05:16:16.515241Z"
    },
    "papermill": {
     "duration": 0.034826,
     "end_time": "2024-12-20T05:16:16.519473",
     "exception": false,
     "start_time": "2024-12-20T05:16:16.484647",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def PerformImpute(imputer_args):\n",
    "    global X_raw, X_tensor_test, num_folds\n",
    "    train_S = []\n",
    "    test_S = []\n",
    "    \n",
    "    KF = KFold(n_splits=num_folds, shuffle=True, random_state=SEED)\n",
    "\n",
    "    oof_non_rounded = []\n",
    "    oof_rounded = []\n",
    "    oof_gt = []\n",
    "    test_preds = np.zeros((len(X_tensor_test), num_folds))\n",
    "\n",
    "    lbl_idx = full_df.columns.get_loc('sii')\n",
    "    \n",
    "    lbled_indices = torch.where(~X_raw[:, lbl_idx].isnan())[0]\n",
    "\n",
    "    pbar = tqdm(KF.split(lbled_indices), desc=\"Training Folds\", total=n_splits)    \n",
    "\n",
    "    for fold, (train_idx_idx, test_idx_idx) in enumerate(pbar):\n",
    "        train_idx = lbled_indices[train_idx_idx]\n",
    "        test_idx = lbled_indices[test_idx_idx]\n",
    "\n",
    "        \n",
    "        X_train = X_raw.clone()\n",
    "        X_train[test_idx, lbl_idx] = float('nan')\n",
    "        \n",
    "        X_val = X_raw[test_idx].clone()\n",
    "\n",
    "        X_train = random_extend(X_train, 9000)\n",
    "        \n",
    "    \n",
    "        train_nonna_indices = torch.where(~X_train[:, lbl_idx].isnan())[0]\n",
    "        val_nonna_indices = torch.where(~X_val[:, lbl_idx].isnan())[0]\n",
    "        if len(train_nonna_indices) == 0 or len(val_nonna_indices)==0: continue\n",
    "    \n",
    "        imputer = ReMasker(imputer_args)\n",
    "        imputer.fit(X_train, X_val, lbl_idx)\n",
    "\n",
    "        y_train_ = X_train[train_nonna_indices, lbl_idx].numpy().astype(int)\n",
    "        y_val_ = X_val[val_nonna_indices, lbl_idx].numpy().astype(int)\n",
    "\n",
    "        X_train[:, lbl_idx] = float('nan')\n",
    "        X_val[:, lbl_idx] = float('nan')\n",
    "        \n",
    "        y_train_pred = imputer.predict(X_train[train_nonna_indices], lbl_idx).cpu().detach().numpy()\n",
    "        y_val_pred = imputer.predict(X_val[val_nonna_indices], lbl_idx).cpu().detach().numpy()\n",
    "        y_test_pred = imputer.predict(X_tensor_test, lbl_idx).cpu().detach().numpy()\n",
    "\n",
    "        oof_non_rounded += [y_val_pred]\n",
    "        y_val_pred_rounded = y_val_pred.round(0).astype(int)\n",
    "        oof_rounded += [y_val_pred_rounded]\n",
    "        oof_gt += [y_val_]\n",
    "        \n",
    "        train_kappa = quadratic_weighted_kappa(y_train_, y_train_pred.round(0).astype(int))\n",
    "        val_kappa = quadratic_weighted_kappa(y_val_, y_val_pred.round(0).astype(int))\n",
    "\n",
    "        train_S.append(train_kappa)\n",
    "        test_S.append(val_kappa)\n",
    "        \n",
    "        \n",
    "        test_preds[:, fold] = y_test_pred\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        pbar.set_description_str(\n",
    "          \"Fold %d, Train MSE: %.4f, Val MSE: %.4f, Train QWK: %.4f, Val QWK: %.4f\" % (\n",
    "              fold + 1,\n",
    "              ((y_train_pred - y_train_) ** 2 / 9.).mean(),\n",
    "              ((y_val_pred - y_val_) ** 2 / 9.).mean(),\n",
    "              train_kappa,\n",
    "              val_kappa\n",
    "          )\n",
    "        )\n",
    "    \n",
    "    \n",
    "    print(f\"Mean Train QWK --> {np.mean(train_S):.4f}\")\n",
    "    print(f\"Mean Validation QWK ---> {np.mean(test_S):.4f}\")\n",
    "\n",
    "    oof_non_rounded = np.concatenate(oof_non_rounded)\n",
    "    oof_gt = np.concatenate(oof_gt)\n",
    "    KappaOPtimizer = minimize(evaluate_predictions,\n",
    "                              x0=[0.5, 1.5, 2.5], args=(oof_gt, oof_non_rounded), \n",
    "                              method='Nelder-Mead')\n",
    "    assert KappaOPtimizer.success, \"Optimization did not converge.\"\n",
    "    \n",
    "    oof_tuned = threshold_Rounder(oof_non_rounded, KappaOPtimizer.x)\n",
    "    tKappa = quadratic_weighted_kappa(oof_gt, oof_tuned)\n",
    "\n",
    "    print(f\"----> || Optimized QWK SCORE :: {Fore.CYAN}{Style.BRIGHT} {tKappa:.3f}{Style.RESET_ALL}\")\n",
    "    \n",
    "    tpm = test_preds.mean(axis=1)\n",
    "    tpTuned = threshold_Rounder(tpm, KappaOPtimizer.x)\n",
    "    \n",
    "    sample_sub_df = pd.read_csv('../input/child-mind-institute-problematic-internet-use/sample_submission.csv')\n",
    "    submission = pd.DataFrame({\n",
    "        'id': sample_sub_df['id'],\n",
    "        'sii': tpTuned\n",
    "    })\n",
    "\n",
    "    return submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eef9d3d",
   "metadata": {
    "papermill": {
     "duration": 0.011776,
     "end_time": "2024-12-20T05:16:16.544477",
     "exception": false,
     "start_time": "2024-12-20T05:16:16.532701",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Define features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff70a8e",
   "metadata": {
    "papermill": {
     "duration": 0.01166,
     "end_time": "2024-12-20T05:16:16.567941",
     "exception": false,
     "start_time": "2024-12-20T05:16:16.556281",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Normal features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f5c6508",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T05:16:16.596185Z",
     "iopub.status.busy": "2024-12-20T05:16:16.594936Z",
     "iopub.status.idle": "2024-12-20T05:16:16.692576Z",
     "shell.execute_reply": "2024-12-20T05:16:16.691184Z"
    },
    "papermill": {
     "duration": 0.115138,
     "end_time": "2024-12-20T05:16:16.695141",
     "exception": false,
     "start_time": "2024-12-20T05:16:16.580003",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/train.csv')\n",
    "test = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/test.csv')\n",
    "sample = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/sample_submission.csv')\n",
    "\n",
    "total_features = list(test.columns)\n",
    "total_features.remove('id')\n",
    "\n",
    "cat_c = ['Basic_Demos-Enroll_Season', 'CGAS-Season', 'Physical-Season', \n",
    "          'Fitness_Endurance-Season', 'FGC-Season', 'BIA-Season', \n",
    "          'PAQ_A-Season', 'PAQ_C-Season', 'SDS-Season', 'PreInt_EduHx-Season']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c149383",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T05:16:16.722482Z",
     "iopub.status.busy": "2024-12-20T05:16:16.722013Z",
     "iopub.status.idle": "2024-12-20T05:16:16.731422Z",
     "shell.execute_reply": "2024-12-20T05:16:16.729341Z"
    },
    "papermill": {
     "duration": 0.026358,
     "end_time": "2024-12-20T05:16:16.734032",
     "exception": false,
     "start_time": "2024-12-20T05:16:16.707674",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n"
     ]
    }
   ],
   "source": [
    "noseason_features = ['Basic_Demos-Age', 'Basic_Demos-Sex',\n",
    "                'CGAS-CGAS_Score', 'Physical-BMI',\n",
    "                'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference',\n",
    "                'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP',\n",
    "                'Fitness_Endurance-Max_Stage',\n",
    "                'Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec',\n",
    "                'FGC-FGC_CU', 'FGC-FGC_CU_Zone', 'FGC-FGC_GSND',\n",
    "                'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone', 'FGC-FGC_PU',\n",
    "                'FGC-FGC_PU_Zone', 'FGC-FGC_SRL', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR',\n",
    "                'FGC-FGC_SRR_Zone', 'FGC-FGC_TL', 'FGC-FGC_TL_Zone',\n",
    "                'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC', 'BIA-BIA_BMI',\n",
    "                'BIA-BIA_BMR', 'BIA-BIA_DEE', 'BIA-BIA_ECW', 'BIA-BIA_FFM',\n",
    "                'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_Fat', 'BIA-BIA_Frame_num',\n",
    "                'BIA-BIA_ICW', 'BIA-BIA_LDM', 'BIA-BIA_LST', 'BIA-BIA_SMM',\n",
    "                'BIA-BIA_TBW', 'PAQ_A-PAQ_A_Total',\n",
    "                'PAQ_C-PAQ_C_Total', 'SDS-SDS_Total_Raw',\n",
    "                'SDS-SDS_Total_T',\n",
    "                'PreInt_EduHx-computerinternet_hoursday', 'BMI_Age','Internet_Hours_Age','BMI_Internet_Hours',\n",
    "                'BFP_BMI', 'FFMI_BFP', 'FMI_BFP', 'LST_TBW', 'BFP_BMR', 'BFP_DEE', 'BMR_Weight', 'DEE_Weight',\n",
    "                'SMM_Height', 'Muscle_to_Fat', 'Hydration_Status', 'ICW_TBW','BMI_PHR']\n",
    "print(len(noseason_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111d8c3a",
   "metadata": {
    "papermill": {
     "duration": 0.01193,
     "end_time": "2024-12-20T05:16:16.758297",
     "exception": false,
     "start_time": "2024-12-20T05:16:16.746367",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Loading timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f074ec7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T05:16:16.786045Z",
     "iopub.status.busy": "2024-12-20T05:16:16.785606Z",
     "iopub.status.idle": "2024-12-20T05:17:53.483374Z",
     "shell.execute_reply": "2024-12-20T05:17:53.482157Z"
    },
    "papermill": {
     "duration": 96.715026,
     "end_time": "2024-12-20T05:17:53.486213",
     "exception": false,
     "start_time": "2024-12-20T05:16:16.771187",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 996/996 [01:36<00:00, 10.35it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  6.65it/s]\n"
     ]
    }
   ],
   "source": [
    "train_ts = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_train.parquet\")\n",
    "test_ts = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2679b531",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T05:17:53.578287Z",
     "iopub.status.busy": "2024-12-20T05:17:53.577856Z",
     "iopub.status.idle": "2024-12-20T05:18:08.575144Z",
     "shell.execute_reply": "2024-12-20T05:18:08.572770Z"
    },
    "papermill": {
     "duration": 15.046093,
     "end_time": "2024-12-20T05:18:08.579486",
     "exception": false,
     "start_time": "2024-12-20T05:17:53.533393",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/100], Loss: 1.4655]\n",
      "Epoch [100/100], Loss: 1.4492]\n"
     ]
    }
   ],
   "source": [
    "df_train = train_ts.drop('id', axis=1)\n",
    "df_test = test_ts.drop('id', axis=1)\n",
    "autoencoder, scaler = perform_autoencoder(df_train, encoding_dim=60, epochs=100, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe09e938",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T05:18:08.652459Z",
     "iopub.status.busy": "2024-12-20T05:18:08.651947Z",
     "iopub.status.idle": "2024-12-20T05:18:08.663619Z",
     "shell.execute_reply": "2024-12-20T05:18:08.662127Z"
    },
    "papermill": {
     "duration": 0.051116,
     "end_time": "2024-12-20T05:18:08.666670",
     "exception": false,
     "start_time": "2024-12-20T05:18:08.615554",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "40957cfa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T05:18:08.735220Z",
     "iopub.status.busy": "2024-12-20T05:18:08.734751Z",
     "iopub.status.idle": "2024-12-20T05:18:08.760404Z",
     "shell.execute_reply": "2024-12-20T05:18:08.759109Z"
    },
    "papermill": {
     "duration": 0.063082,
     "end_time": "2024-12-20T05:18:08.763503",
     "exception": false,
     "start_time": "2024-12-20T05:18:08.700421",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_ts_encoded = encode_data(autoencoder, scaler, df_train)\n",
    "test_ts_encoded = encode_data(autoencoder, scaler, df_test)\n",
    "test_ts_encoded.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff46192c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T05:18:08.833852Z",
     "iopub.status.busy": "2024-12-20T05:18:08.833414Z",
     "iopub.status.idle": "2024-12-20T05:18:08.885189Z",
     "shell.execute_reply": "2024-12-20T05:18:08.883906Z"
    },
    "papermill": {
     "duration": 0.089403,
     "end_time": "2024-12-20T05:18:08.887738",
     "exception": false,
     "start_time": "2024-12-20T05:18:08.798335",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Enc_1</th>\n",
       "      <th>Enc_2</th>\n",
       "      <th>Enc_3</th>\n",
       "      <th>Enc_4</th>\n",
       "      <th>Enc_5</th>\n",
       "      <th>Enc_6</th>\n",
       "      <th>Enc_7</th>\n",
       "      <th>Enc_8</th>\n",
       "      <th>Enc_9</th>\n",
       "      <th>Enc_10</th>\n",
       "      <th>Enc_11</th>\n",
       "      <th>Enc_12</th>\n",
       "      <th>Enc_13</th>\n",
       "      <th>Enc_14</th>\n",
       "      <th>Enc_15</th>\n",
       "      <th>Enc_16</th>\n",
       "      <th>Enc_17</th>\n",
       "      <th>Enc_18</th>\n",
       "      <th>Enc_19</th>\n",
       "      <th>Enc_20</th>\n",
       "      <th>Enc_21</th>\n",
       "      <th>Enc_22</th>\n",
       "      <th>Enc_23</th>\n",
       "      <th>Enc_24</th>\n",
       "      <th>Enc_25</th>\n",
       "      <th>Enc_26</th>\n",
       "      <th>Enc_27</th>\n",
       "      <th>Enc_28</th>\n",
       "      <th>Enc_29</th>\n",
       "      <th>Enc_30</th>\n",
       "      <th>Enc_31</th>\n",
       "      <th>Enc_32</th>\n",
       "      <th>Enc_33</th>\n",
       "      <th>Enc_34</th>\n",
       "      <th>Enc_35</th>\n",
       "      <th>Enc_36</th>\n",
       "      <th>Enc_37</th>\n",
       "      <th>Enc_38</th>\n",
       "      <th>Enc_39</th>\n",
       "      <th>Enc_40</th>\n",
       "      <th>Enc_41</th>\n",
       "      <th>Enc_42</th>\n",
       "      <th>Enc_43</th>\n",
       "      <th>Enc_44</th>\n",
       "      <th>Enc_45</th>\n",
       "      <th>Enc_46</th>\n",
       "      <th>Enc_47</th>\n",
       "      <th>Enc_48</th>\n",
       "      <th>Enc_49</th>\n",
       "      <th>Enc_50</th>\n",
       "      <th>Enc_51</th>\n",
       "      <th>Enc_52</th>\n",
       "      <th>Enc_53</th>\n",
       "      <th>Enc_54</th>\n",
       "      <th>Enc_55</th>\n",
       "      <th>Enc_56</th>\n",
       "      <th>Enc_57</th>\n",
       "      <th>Enc_58</th>\n",
       "      <th>Enc_59</th>\n",
       "      <th>Enc_60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.337389</td>\n",
       "      <td>7.215616</td>\n",
       "      <td>4.216683</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.344681</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.045033</td>\n",
       "      <td>0.898455</td>\n",
       "      <td>2.708214</td>\n",
       "      <td>3.399186</td>\n",
       "      <td>2.038562</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.918493</td>\n",
       "      <td>1.856204</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.471048</td>\n",
       "      <td>9.064348</td>\n",
       "      <td>2.624146</td>\n",
       "      <td>2.383941</td>\n",
       "      <td>6.034007</td>\n",
       "      <td>6.280801</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.305665</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.107492</td>\n",
       "      <td>5.810652</td>\n",
       "      <td>4.795111</td>\n",
       "      <td>1.981038</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.866778</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.708674</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.812943</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.403038</td>\n",
       "      <td>7.655363</td>\n",
       "      <td>4.856868</td>\n",
       "      <td>4.566550</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.363611</td>\n",
       "      <td>0.852052</td>\n",
       "      <td>2.313890</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.275120</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.269929</td>\n",
       "      <td>3.051205</td>\n",
       "      <td>6.881392</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.864849</td>\n",
       "      <td>0.490974</td>\n",
       "      <td>0.685794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.844508</td>\n",
       "      <td>5.394163</td>\n",
       "      <td>2.186373</td>\n",
       "      <td>2.682933</td>\n",
       "      <td>4.056297</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.080273</td>\n",
       "      <td>5.332508</td>\n",
       "      <td>2.287547</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.950637</td>\n",
       "      <td>1.646377</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.572919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.725673</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.378885</td>\n",
       "      <td>4.879403</td>\n",
       "      <td>3.119649</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.350994</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.392251</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.614659</td>\n",
       "      <td>0.250168</td>\n",
       "      <td>2.762118</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.877998</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.524518</td>\n",
       "      <td>4.918088</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.952121</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.409425</td>\n",
       "      <td>6.154183</td>\n",
       "      <td>6.145087</td>\n",
       "      <td>4.371527</td>\n",
       "      <td>9.287868</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Enc_1     Enc_2     Enc_3     Enc_4  Enc_5     Enc_6  Enc_7     Enc_8  \\\n",
       "0  0.000000  5.337389  7.215616  4.216683    0.0  5.344681    0.0  1.045033   \n",
       "1  6.864849  0.490974  0.685794  0.000000    0.0  0.000000    0.0  3.844508   \n",
       "\n",
       "      Enc_9    Enc_10    Enc_11    Enc_12  Enc_13  Enc_14    Enc_15    Enc_16  \\\n",
       "0  0.898455  2.708214  3.399186  2.038562     0.0     0.0  3.918493  1.856204   \n",
       "1  5.394163  2.186373  2.682933  4.056297     0.0     0.0  3.080273  5.332508   \n",
       "\n",
       "     Enc_17    Enc_18    Enc_19    Enc_20    Enc_21    Enc_22    Enc_23  \\\n",
       "0  0.000000  0.471048  9.064348  2.624146  2.383941  6.034007  6.280801   \n",
       "1  2.287547  0.000000  5.950637  1.646377  0.000000  0.000000  0.000000   \n",
       "\n",
       "   Enc_24    Enc_25    Enc_26  Enc_27    Enc_28    Enc_29    Enc_30    Enc_31  \\\n",
       "0     0.0  0.000000  5.305665     0.0  0.000000  7.107492  5.810652  4.795111   \n",
       "1     0.0  2.572919  0.000000     0.0  7.725673  0.000000  3.378885  4.879403   \n",
       "\n",
       "     Enc_32  Enc_33  Enc_34  Enc_35    Enc_36  Enc_37    Enc_38  Enc_39  \\\n",
       "0  1.981038     0.0     0.0     0.0  5.866778     0.0  2.708674     0.0   \n",
       "1  3.119649     0.0     0.0     0.0  5.350994     0.0  3.392251     0.0   \n",
       "\n",
       "   Enc_40    Enc_41  Enc_42    Enc_43    Enc_44    Enc_45    Enc_46  Enc_47  \\\n",
       "0     0.0  8.812943     0.0  6.403038  7.655363  4.856868  4.566550     0.0   \n",
       "1     0.0  0.000000     0.0  0.000000  0.614659  0.250168  2.762118     0.0   \n",
       "\n",
       "     Enc_48    Enc_49    Enc_50    Enc_51  Enc_52    Enc_53  Enc_54    Enc_55  \\\n",
       "0  7.363611  0.852052  2.313890  0.000000     0.0  0.275120     0.0  5.269929   \n",
       "1  2.877998  0.000000  0.524518  4.918088     0.0  0.952121     0.0  6.409425   \n",
       "\n",
       "     Enc_56    Enc_57    Enc_58    Enc_59  Enc_60  \n",
       "0  3.051205  6.881392  0.000000  0.000000     0.0  \n",
       "1  6.154183  6.145087  4.371527  9.287868     0.0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ts_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f3cc10b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T05:18:08.958446Z",
     "iopub.status.busy": "2024-12-20T05:18:08.957971Z",
     "iopub.status.idle": "2024-12-20T05:18:08.965199Z",
     "shell.execute_reply": "2024-12-20T05:18:08.964092Z"
    },
    "papermill": {
     "duration": 0.044708,
     "end_time": "2024-12-20T05:18:08.967564",
     "exception": false,
     "start_time": "2024-12-20T05:18:08.922856",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_ts_encoded[\"id\"]=train_ts[\"id\"]\n",
    "test_ts_encoded['id']=test_ts[\"id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd62568b",
   "metadata": {
    "papermill": {
     "duration": 0.033196,
     "end_time": "2024-12-20T05:18:09.033393",
     "exception": false,
     "start_time": "2024-12-20T05:18:09.000197",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Features timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "91e8b356",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T05:18:09.104137Z",
     "iopub.status.busy": "2024-12-20T05:18:09.103673Z",
     "iopub.status.idle": "2024-12-20T05:18:09.109766Z",
     "shell.execute_reply": "2024-12-20T05:18:09.108463Z"
    },
    "papermill": {
     "duration": 0.043081,
     "end_time": "2024-12-20T05:18:09.112540",
     "exception": false,
     "start_time": "2024-12-20T05:18:09.069459",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "time_series_cols = train_ts.columns.tolist()\n",
    "time_series_cols.remove(\"id\")\n",
    "time_encoded_cols = train_ts_encoded.columns.tolist()\n",
    "time_encoded_cols.remove(\"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9520b8",
   "metadata": {
    "papermill": {
     "duration": 0.03445,
     "end_time": "2024-12-20T05:18:09.181114",
     "exception": false,
     "start_time": "2024-12-20T05:18:09.146664",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Submission 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cf70eaee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T05:18:09.251417Z",
     "iopub.status.busy": "2024-12-20T05:18:09.250943Z",
     "iopub.status.idle": "2024-12-20T05:18:54.884831Z",
     "shell.execute_reply": "2024-12-20T05:18:54.883243Z"
    },
    "papermill": {
     "duration": 45.673253,
     "end_time": "2024-12-20T05:18:54.887674",
     "exception": false,
     "start_time": "2024-12-20T05:18:09.214421",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip -q install /kaggle/input/pytorchtabnet/pytorch_tabnet-4.1.0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a1a63502",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T05:18:54.957864Z",
     "iopub.status.busy": "2024-12-20T05:18:54.957396Z",
     "iopub.status.idle": "2024-12-20T05:18:54.980060Z",
     "shell.execute_reply": "2024-12-20T05:18:54.979152Z"
    },
    "papermill": {
     "duration": 0.059654,
     "end_time": "2024-12-20T05:18:54.982756",
     "exception": false,
     "start_time": "2024-12-20T05:18:54.923102",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pytorch_tabnet.tab_model import TabNetRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "04079e79",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T05:18:55.050427Z",
     "iopub.status.busy": "2024-12-20T05:18:55.049995Z",
     "iopub.status.idle": "2024-12-20T05:18:55.068741Z",
     "shell.execute_reply": "2024-12-20T05:18:55.067584Z"
    },
    "papermill": {
     "duration": 0.055519,
     "end_time": "2024-12-20T05:18:55.071433",
     "exception": false,
     "start_time": "2024-12-20T05:18:55.015914",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_tabnet.callbacks import Callback\n",
    "import os\n",
    "import torch\n",
    "from pytorch_tabnet.callbacks import Callback\n",
    "\n",
    "class TabNetWrapper(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.model = TabNetRegressor(**kwargs)\n",
    "        self.kwargs = kwargs\n",
    "        self.imputer = SimpleImputer(strategy='median')\n",
    "        self.best_model_path = 'best_tabnet_model.pt'\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        X_imputed = self.imputer.fit_transform(X)\n",
    "        \n",
    "        if hasattr(y, 'values'):\n",
    "            y = y.values\n",
    "            \n",
    "        X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "            X_imputed, \n",
    "            y, \n",
    "            test_size=0.2,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        history = self.model.fit(\n",
    "            X_train=X_train,\n",
    "            y_train=y_train.reshape(-1, 1),\n",
    "            eval_set=[(X_valid, y_valid.reshape(-1, 1))],\n",
    "            eval_name=['valid'],\n",
    "            eval_metric=['mse'],\n",
    "            max_epochs=200,\n",
    "            patience=20,\n",
    "            batch_size=1024,\n",
    "            virtual_batch_size=128,\n",
    "            num_workers=0,\n",
    "            drop_last=False,\n",
    "            callbacks=[\n",
    "                TabNetPretrainedModelCheckpoint(\n",
    "                    filepath=self.best_model_path,\n",
    "                    monitor='valid_mse',\n",
    "                    mode='min',\n",
    "                    save_best_only=True,\n",
    "                    verbose=True\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        if os.path.exists(self.best_model_path):\n",
    "            self.model.load_model(self.best_model_path)\n",
    "            os.remove(self.best_model_path)  \n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X_imputed = self.imputer.transform(X)\n",
    "        return self.model.predict(X_imputed).flatten()\n",
    "    \n",
    "    def __deepcopy__(self, memo):\n",
    "        cls = self.__class__\n",
    "        result = cls.__new__(cls)\n",
    "        memo[id(self)] = result\n",
    "        for k, v in self.__dict__.items():\n",
    "            setattr(result, k, deepcopy(v, memo))\n",
    "        return result\n",
    "\n",
    "TabNet_Params = {\n",
    "    'n_d': 64,              # Width of the decision prediction layer\n",
    "    'n_a': 64,              # Width of the attention embedding for each step\n",
    "    'n_steps': 5,           # Number of steps in the architecture\n",
    "    'gamma': 1.5,           # Coefficient for feature selection regularization\n",
    "    'n_independent': 2,     # Number of independent GLU layer in each GLU block\n",
    "    'n_shared': 2,          # Number of shared GLU layer in each GLU block\n",
    "    'lambda_sparse': 1e-4,  # Sparsity regularization\n",
    "    'optimizer_fn': torch.optim.Adam,\n",
    "    'optimizer_params': dict(lr=2e-2, weight_decay=1e-5),\n",
    "    'mask_type': 'entmax',\n",
    "    'scheduler_params': dict(mode=\"min\", patience=10, min_lr=1e-5, factor=0.5),\n",
    "    'scheduler_fn': torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "    'verbose': 1,\n",
    "    'device_name': 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "}\n",
    "\n",
    "class TabNetPretrainedModelCheckpoint(Callback):\n",
    "    def __init__(self, filepath, monitor='val_loss', mode='min', \n",
    "                 save_best_only=True, verbose=1):\n",
    "        super().__init__()  # Initialize parent class\n",
    "        self.filepath = filepath\n",
    "        self.monitor = monitor\n",
    "        self.mode = mode\n",
    "        self.save_best_only = save_best_only\n",
    "        self.verbose = verbose\n",
    "        self.best = float('inf') if mode == 'min' else -float('inf')\n",
    "        \n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.model = self.trainer  # Use trainer itself as model\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        current = logs.get(self.monitor)\n",
    "        if current is None:\n",
    "            return\n",
    "        \n",
    "        # Check if current metric is better than best\n",
    "        if (self.mode == 'min' and current < self.best) or \\\n",
    "           (self.mode == 'max' and current > self.best):\n",
    "            if self.verbose:\n",
    "                print(f'\\nEpoch {epoch}: {self.monitor} improved from {self.best:.4f} to {current:.4f}')\n",
    "            self.best = current\n",
    "            if self.save_best_only:\n",
    "                self.model.save_model(self.filepath)  # Save the entire model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2a83cca3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T05:18:55.140924Z",
     "iopub.status.busy": "2024-12-20T05:18:55.140494Z",
     "iopub.status.idle": "2024-12-20T05:18:55.193639Z",
     "shell.execute_reply": "2024-12-20T05:18:55.192111Z"
    },
    "papermill": {
     "duration": 0.092286,
     "end_time": "2024-12-20T05:18:55.198157",
     "exception": false,
     "start_time": "2024-12-20T05:18:55.105871",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_sub1 = pd.merge(train, train_ts_encoded, how=\"left\", on='id')\n",
    "test_sub1 = pd.merge(test, test_ts_encoded, how=\"left\", on='id')\n",
    "train_sub1 = train_sub1.dropna(subset='sii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "11d6031e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T05:18:55.276696Z",
     "iopub.status.busy": "2024-12-20T05:18:55.276300Z",
     "iopub.status.idle": "2024-12-20T05:18:55.281554Z",
     "shell.execute_reply": "2024-12-20T05:18:55.280340Z"
    },
    "papermill": {
     "duration": 0.043062,
     "end_time": "2024-12-20T05:18:55.283805",
     "exception": false,
     "start_time": "2024-12-20T05:18:55.240743",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_sub1 = train_sub1\n",
    "y_sub1 = train_sub1['sii']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b7be4b19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T05:18:55.353059Z",
     "iopub.status.busy": "2024-12-20T05:18:55.352656Z",
     "iopub.status.idle": "2024-12-20T05:19:38.189146Z",
     "shell.execute_reply": "2024-12-20T05:19:38.187536Z"
    },
    "papermill": {
     "duration": 42.874845,
     "end_time": "2024-12-20T05:19:38.192569",
     "exception": false,
     "start_time": "2024-12-20T05:18:55.317724",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip -q install optuna\n",
    "\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aadc9dfb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T05:19:38.347739Z",
     "iopub.status.busy": "2024-12-20T05:19:38.347309Z",
     "iopub.status.idle": "2024-12-20T05:19:38.363338Z",
     "shell.execute_reply": "2024-12-20T05:19:38.362005Z"
    },
    "papermill": {
     "duration": 0.054986,
     "end_time": "2024-12-20T05:19:38.366112",
     "exception": false,
     "start_time": "2024-12-20T05:19:38.311126",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def objective_sub1(trial):\n",
    "    CatBoost_Params = {\n",
    "        'learning_rate': trial.suggest_float('catboost_learning_rate', 1e-3, 0.3, log=True),\n",
    "        'depth': trial.suggest_int('catboost_depth', 4, 10),\n",
    "        'random_seed': SEED,\n",
    "        'verbose': 0,\n",
    "        'l2_leaf_reg': trial.suggest_float('catboost_l2_leaf_reg', 0.01, 10.0, log=True),\n",
    "        'iterations': trial.suggest_int('catboost_iterations', 100, 400, 10),\n",
    "        #'task_type': 'GPU',  \n",
    "        #'devices': '0'      \n",
    "    }\n",
    "    XGB_Params = {\n",
    "        'n_estimators': trial.suggest_int('xgb_n_estimators', 500, 1500, 100),\n",
    "        'max_depth': trial.suggest_int('xgb_max_depth', 1, 10),\n",
    "        'learning_rate': trial.suggest_float('xgb_learning_rate', 0.01, 0.1, log=True),\n",
    "        'subsample': trial.suggest_float('xgb_subsample', 0.1, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('xgb_colsample_bytree', 0.05, 1.0),\n",
    "        'gamma': trial.suggest_float('xgb_gamma', 1e-2, 1.0),\n",
    "        'min_child_weight': trial.suggest_int('xgb_min_child_weight', 1, 100),\n",
    "        'eval_metric': 'rmse',\n",
    "        'objective': 'reg:squarederror',\n",
    "        #'tree_method': 'gpu_hist',\n",
    "        #'predictor': 'gpu_predictor',\n",
    "        #'gpu_id': 0\n",
    "    }\n",
    "    Params = {\n",
    "        'max_depth': trial.suggest_int('lightgbm_max_depth', 3, 12),  # Avoid overly shallow or deep trees\n",
    "        'min_data_in_leaf': trial.suggest_int('lightgbm_min_data_in_leaf', 5, 50),  # Balance between overfitting and splits\n",
    "        'num_leaves': trial.suggest_int('lightgbm_num_leaves', 16, 256),  # Limit complexity to avoid splits failing\n",
    "        'learning_rate': trial.suggest_float('lightgbm_learning_rate', 0.01, 0.1),\n",
    "        'feature_fraction': trial.suggest_float('lightgbm_feature_fraction', 0.7, 1.0),  # Allow feature subsampling tuning\n",
    "        'bagging_fraction': trial.suggest_float('lightgbm_bagging_fraction', 0.7, 1.0),  # Improve generalization\n",
    "        'bagging_freq': trial.suggest_int('lightgbm_bagging_freq', 1, 5),\n",
    "        'lambda_l1': trial.suggest_float('lightgbm_lambda_l1', 0.0, 10.0),  # Regularization tuning\n",
    "        'lambda_l2': trial.suggest_float('lightgbm_lambda_l2', 0.0, 10.0),\n",
    "        'min_gain_to_split': trial.suggest_float('lightgbm_min_gain_to_split', 0.0, 0.1),  # Ensure splits happen\n",
    "        #'device_type': 'gpu',\n",
    "        #'gpu_device_id': 0,\n",
    "        'verbosity': -1  \n",
    "    }\n",
    "    TabNet_Params = {\n",
    "        'n_d': 64,\n",
    "        'n_a': 64,\n",
    "        'n_steps': 5,\n",
    "        'gamma': 1.5,\n",
    "        'n_independent': 2,\n",
    "        'n_shared': 2,\n",
    "        'lambda_sparse': 1e-4,\n",
    "        'optimizer_fn': torch.optim.Adam,\n",
    "        'optimizer_params': dict(lr=2e-2, weight_decay=1e-5),\n",
    "        'mask_type': 'entmax',\n",
    "        'scheduler_params': dict(mode=\"min\", patience=10, min_lr=1e-5, factor=0.5),\n",
    "        'scheduler_fn': torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "        'verbose': -1,\n",
    "        'device_name': 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    }\n",
    "    LightGBM_Model = LGBMRegressor(**Params)\n",
    "    XGB_Model = XGBRegressor(**XGB_Params)\n",
    "    CatBoost_Model = CatBoostRegressor(**CatBoost_Params)\n",
    "    TabNet_Model = TabNetWrapper(**TabNet_Params) \n",
    "    voting_model = VotingRegressor(estimators=[\n",
    "        ('lightgbm', LightGBM_Model),\n",
    "        ('xgboost', XGB_Model),\n",
    "        ('catboost', CatBoost_Model),\n",
    "        ('tabnetModel', TabNet_Model),\n",
    "    ], weights=[4.0, 4.0, 4.0, 5.0])\n",
    "    X = train.drop(['sii'], axis=1)\n",
    "    y = train['sii']\n",
    "\n",
    "    submission2, val_score, _, _, _, _ = TrainML_Sub1(voting_model, X_sub1, y_sub1, test_sub1)\n",
    "    return val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3aaf57fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T05:19:38.433574Z",
     "iopub.status.busy": "2024-12-20T05:19:38.433113Z",
     "iopub.status.idle": "2024-12-20T05:19:38.437681Z",
     "shell.execute_reply": "2024-12-20T05:19:38.436635Z"
    },
    "papermill": {
     "duration": 0.041171,
     "end_time": "2024-12-20T05:19:38.439788",
     "exception": false,
     "start_time": "2024-12-20T05:19:38.398617",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# study_sub1 = optuna.create_study(direction='maximize')\n",
    "# study_sub1.optimize(objective_sub1, n_trials=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b1b183e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T05:19:38.507645Z",
     "iopub.status.busy": "2024-12-20T05:19:38.507217Z",
     "iopub.status.idle": "2024-12-20T05:19:38.529390Z",
     "shell.execute_reply": "2024-12-20T05:19:38.527917Z"
    },
    "papermill": {
     "duration": 0.059988,
     "end_time": "2024-12-20T05:19:38.532381",
     "exception": false,
     "start_time": "2024-12-20T05:19:38.472393",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# best_params_sub1 = study_sub1.best_params\n",
    "# print(\"Best hyperparameters:\", best_params_sub1)\n",
    "\n",
    "CatBoost_Best_Params = {\n",
    "    'learning_rate': 0.0021172579310639343,\n",
    "    'depth': 6,\n",
    "    'iterations': 130,\n",
    "    'random_seed': SEED,\n",
    "    'verbose': 0,\n",
    "    'l2_leaf_reg': 0.32557701990001503,\n",
    "}\n",
    "\n",
    "XGB_Best_Params = {\n",
    "    'n_estimators': 700,\n",
    "    'max_depth': 4,\n",
    "    'learning_rate': 0.03325152156380898,\n",
    "    'subsample': 0.25295047248406266,\n",
    "    'colsample_bytree': 0.9760859719849787,\n",
    "    'gamma': 0.20085951790463402,\n",
    "    'min_child_weight': 11,\n",
    "    'eval_metric': 'rmse',\n",
    "    'objective': 'reg:squarederror',\n",
    "}\n",
    "\n",
    "LightGBM_Best_Params = {\n",
    "    'max_depth': 3,\n",
    "    'min_data_in_leaf': 40,\n",
    "    'num_leaves': 190,\n",
    "    'learning_rate': 0.05107368421432176,\n",
    "    'feature_fraction': 0.9918350138636185,\n",
    "    'bagging_fraction': 0.9331400899763774,\n",
    "    'bagging_freq': 1,\n",
    "    'lambda_l1': 9.49641646280519,\n",
    "    'lambda_l2': 2.446305429623661,\n",
    "    'min_gain_to_split': 0.05262124930522051,\n",
    "    'verbosity': -1\n",
    "}\n",
    "\n",
    "catboost_model = CatBoostRegressor(**CatBoost_Best_Params)\n",
    "xgb_model = XGBRegressor(**XGB_Best_Params)\n",
    "lightgbm_model = LGBMRegressor(**LightGBM_Best_Params)\n",
    "\n",
    "final_voting_model = VotingRegressor(estimators=[\n",
    "    ('lightgbm', lightgbm_model),\n",
    "    ('xgboost', xgb_model),\n",
    "    ('catboost', catboost_model),\n",
    "], weights=[4.0, 4.0, 4.0])\n",
    "\n",
    "X = train.drop(['sii'], axis=1)\n",
    "y = train['sii']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eaa068ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T05:19:38.600717Z",
     "iopub.status.busy": "2024-12-20T05:19:38.600315Z",
     "iopub.status.idle": "2024-12-20T05:19:49.513380Z",
     "shell.execute_reply": "2024-12-20T05:19:49.512169Z"
    },
    "papermill": {
     "duration": 10.950368,
     "end_time": "2024-12-20T05:19:49.515798",
     "exception": false,
     "start_time": "2024-12-20T05:19:38.565430",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Folds: 100%|██████████| 5/5 [00:10<00:00,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Train QWK --> 0.4516\n",
      "Mean Validation QWK ---> 0.3644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPTIMIZED THRESHOLDS [0.5580865  1.00878806 2.87177879]\n",
      "----> || Optimized QWK SCORE :: \u001b[36m\u001b[1m 0.478\u001b[0m\n",
      "Val score sub2 with best parameters: 0.4783086216364635\n"
     ]
    }
   ],
   "source": [
    "submission1, val_score_sub1, _, _, _, _ = TrainML_Sub1(lightgbm_model, X_sub1, y_sub1, test_sub1)\n",
    "\n",
    "print(\"Val score sub2 with best parameters:\", val_score_sub1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d15da8",
   "metadata": {
    "papermill": {
     "duration": 0.032513,
     "end_time": "2024-12-20T05:19:49.583403",
     "exception": false,
     "start_time": "2024-12-20T05:19:49.550890",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Submission 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "75f00160",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T05:19:49.651942Z",
     "iopub.status.busy": "2024-12-20T05:19:49.651515Z",
     "iopub.status.idle": "2024-12-20T05:19:49.658789Z",
     "shell.execute_reply": "2024-12-20T05:19:49.657568Z"
    },
    "papermill": {
     "duration": 0.045012,
     "end_time": "2024-12-20T05:19:49.661026",
     "exception": false,
     "start_time": "2024-12-20T05:19:49.616014",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "Params = {\n",
    "    'learning_rate': 0.046,\n",
    "    'max_depth': 12,\n",
    "    'num_leaves': 478,\n",
    "    'min_data_in_leaf': 13,\n",
    "    'feature_fraction': 0.893,\n",
    "    'bagging_fraction': 0.784,\n",
    "    'bagging_freq': 4,\n",
    "    'lambda_l1': 10,  \n",
    "    'lambda_l2': 0.01, \n",
    "    'device': 'cpu'\n",
    "}\n",
    "\n",
    "XGB_Params = {\n",
    "    'learning_rate': 0.05,\n",
    "    'max_depth': 6,\n",
    "    'n_estimators': 200,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'reg_alpha': 1,  \n",
    "    'reg_lambda': 5, \n",
    "    'random_state': SEED,\n",
    "    'tree_method': 'gpu_hist',\n",
    "}\n",
    "\n",
    "CatBoost_Params = {\n",
    "    'learning_rate': 0.05,\n",
    "    'depth': 6,\n",
    "    'iterations': 200,\n",
    "    'random_seed': SEED,\n",
    "    'verbose': 0,\n",
    "    'l2_leaf_reg': 10,\n",
    "    'task_type': 'GPU'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "668fb575",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T05:19:49.729672Z",
     "iopub.status.busy": "2024-12-20T05:19:49.729288Z",
     "iopub.status.idle": "2024-12-20T05:19:49.743413Z",
     "shell.execute_reply": "2024-12-20T05:19:49.742128Z"
    },
    "papermill": {
     "duration": 0.051174,
     "end_time": "2024-12-20T05:19:49.745889",
     "exception": false,
     "start_time": "2024-12-20T05:19:49.694715",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "Light = LGBMRegressor(**Params, random_state=SEED, verbose=-1, n_estimators=300)\n",
    "XGB_Model = XGBRegressor(**XGB_Params)\n",
    "CatBoost_Model = CatBoostRegressor(**CatBoost_Params)\n",
    "TabNet_Model = TabNetWrapper(**TabNet_Params) \n",
    "voting_model = VotingRegressor(estimators=[\n",
    "    ('lightgbm', Light),\n",
    "    ('xgboost', XGB_Model),\n",
    "    ('catboost', CatBoost_Model),\n",
    "    ('tabnet', TabNet_Model)\n",
    "],weights=[4.0,4.0,5.0,4.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "20d2db00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T05:19:49.814599Z",
     "iopub.status.busy": "2024-12-20T05:19:49.814206Z",
     "iopub.status.idle": "2024-12-20T05:19:57.891602Z",
     "shell.execute_reply": "2024-12-20T05:19:57.890484Z"
    },
    "papermill": {
     "duration": 8.114892,
     "end_time": "2024-12-20T05:19:57.894322",
     "exception": false,
     "start_time": "2024-12-20T05:19:49.779430",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_sub2 = pd.merge(train, train_ts_encoded, how=\"left\", on='id')\n",
    "test_sub2 = pd.merge(test, test_ts_encoded, how=\"left\", on='id')\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "numeric_cols = train.select_dtypes(include=['float64', 'int64']).columns\n",
    "imputed_data = imputer.fit_transform(train_sub2[numeric_cols])\n",
    "train_imputed = pd.DataFrame(imputed_data, columns=numeric_cols)\n",
    "train_imputed['sii'] = train_imputed['sii'].round().astype(int)\n",
    "\n",
    "for col in train_sub2.columns:\n",
    "    if col not in numeric_cols:\n",
    "        train_imputed[col] = train_sub2[col]\n",
    "        \n",
    "train_sub2 = train_imputed\n",
    "\n",
    "train_sub2 = feature_engineering(train_sub2)\n",
    "test_sub2 = feature_engineering(test_sub2)\n",
    "\n",
    "train_sub2 = train_sub2.drop('id', axis=1)\n",
    "test_sub2  = test_sub2.drop('id', axis=1)\n",
    "\n",
    "features_sub2 = noseason_features + time_encoded_cols\n",
    "\n",
    "train_sub2 = train_sub2.dropna(subset='sii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fbfe17d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T05:19:57.962594Z",
     "iopub.status.busy": "2024-12-20T05:19:57.962207Z",
     "iopub.status.idle": "2024-12-20T05:19:57.977655Z",
     "shell.execute_reply": "2024-12-20T05:19:57.976582Z"
    },
    "papermill": {
     "duration": 0.052975,
     "end_time": "2024-12-20T05:19:57.979990",
     "exception": false,
     "start_time": "2024-12-20T05:19:57.927015",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if np.any(np.isinf(train_sub2)):\n",
    "    train_sub2 = train_sub2.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "X_sub2 = train_sub2[features_sub2]\n",
    "y_sub2 = train_sub2['sii']\n",
    "test_sub2 = test_sub2[features_sub2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "111d11a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T05:19:58.047031Z",
     "iopub.status.busy": "2024-12-20T05:19:58.046628Z",
     "iopub.status.idle": "2024-12-20T05:19:58.051643Z",
     "shell.execute_reply": "2024-12-20T05:19:58.050544Z"
    },
    "papermill": {
     "duration": 0.041016,
     "end_time": "2024-12-20T05:19:58.053747",
     "exception": false,
     "start_time": "2024-12-20T05:19:58.012731",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# submission2, _, _, _, _ = TrainML(voting_model, X_sub2, y_sub2, test_sub2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "54334ba8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T05:19:58.123091Z",
     "iopub.status.busy": "2024-12-20T05:19:58.122671Z",
     "iopub.status.idle": "2024-12-20T05:19:58.127387Z",
     "shell.execute_reply": "2024-12-20T05:19:58.126301Z"
    },
    "papermill": {
     "duration": 0.042748,
     "end_time": "2024-12-20T05:19:58.129436",
     "exception": false,
     "start_time": "2024-12-20T05:19:58.086688",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# submission2.to_csv(\"submission2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c080475",
   "metadata": {
    "papermill": {
     "duration": 0.032633,
     "end_time": "2024-12-20T05:19:58.194749",
     "exception": false,
     "start_time": "2024-12-20T05:19:58.162116",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Submission 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "02d08e2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T05:19:58.262555Z",
     "iopub.status.busy": "2024-12-20T05:19:58.262165Z",
     "iopub.status.idle": "2024-12-20T05:19:58.283400Z",
     "shell.execute_reply": "2024-12-20T05:19:58.282423Z"
    },
    "papermill": {
     "duration": 0.058316,
     "end_time": "2024-12-20T05:19:58.286231",
     "exception": false,
     "start_time": "2024-12-20T05:19:58.227915",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_sub3 = pd.merge(train, train_ts, how=\"left\", on='id')\n",
    "test_sub3 = pd.merge(test, test_ts, how=\"left\", on='id')\n",
    "\n",
    "train_sub3 = train_sub3.drop('id', axis=1)\n",
    "test_sub3 = test_sub3.drop('id', axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "32aa6d7e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T05:19:58.368936Z",
     "iopub.status.busy": "2024-12-20T05:19:58.367910Z",
     "iopub.status.idle": "2024-12-20T05:19:58.372905Z",
     "shell.execute_reply": "2024-12-20T05:19:58.371842Z"
    },
    "papermill": {
     "duration": 0.042652,
     "end_time": "2024-12-20T05:19:58.375612",
     "exception": false,
     "start_time": "2024-12-20T05:19:58.332960",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "features_sub3 = total_features + time_series_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8b64e5f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T05:19:58.451041Z",
     "iopub.status.busy": "2024-12-20T05:19:58.450652Z",
     "iopub.status.idle": "2024-12-20T05:19:58.459682Z",
     "shell.execute_reply": "2024-12-20T05:19:58.458599Z"
    },
    "papermill": {
     "duration": 0.046156,
     "end_time": "2024-12-20T05:19:58.462428",
     "exception": false,
     "start_time": "2024-12-20T05:19:58.416272",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_sub3 = train_sub3.dropna(subset='sii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0e7f8147",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T05:19:58.544495Z",
     "iopub.status.busy": "2024-12-20T05:19:58.544093Z",
     "iopub.status.idle": "2024-12-20T05:19:58.614407Z",
     "shell.execute_reply": "2024-12-20T05:19:58.613364Z"
    },
    "papermill": {
     "duration": 0.108098,
     "end_time": "2024-12-20T05:19:58.617465",
     "exception": false,
     "start_time": "2024-12-20T05:19:58.509367",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_sub3 = update(train_sub3)\n",
    "test_sub3 = update(test_sub3)\n",
    "\n",
    "for col in cat_c:\n",
    "    mapping = create_mapping(col, train_sub3)\n",
    "    mappingTe = create_mapping(col, test_sub3)\n",
    "    \n",
    "    train_sub3[col] = train_sub3[col].replace(mapping).astype(int)\n",
    "    test_sub3[col] = test_sub3[col].replace(mappingTe).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "37584242",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T05:19:58.699004Z",
     "iopub.status.busy": "2024-12-20T05:19:58.698607Z",
     "iopub.status.idle": "2024-12-20T05:19:58.708391Z",
     "shell.execute_reply": "2024-12-20T05:19:58.707267Z"
    },
    "papermill": {
     "duration": 0.051807,
     "end_time": "2024-12-20T05:19:58.710682",
     "exception": false,
     "start_time": "2024-12-20T05:19:58.658875",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_sub3 = train_sub3[features_sub3]\n",
    "y_sub3 = train_sub3['sii']\n",
    "test_sub3 = test_sub3[features_sub3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e70b6526",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T05:19:58.788226Z",
     "iopub.status.busy": "2024-12-20T05:19:58.787122Z",
     "iopub.status.idle": "2024-12-20T05:19:58.799800Z",
     "shell.execute_reply": "2024-12-20T05:19:58.798625Z"
    },
    "papermill": {
     "duration": 0.0575,
     "end_time": "2024-12-20T05:19:58.802223",
     "exception": false,
     "start_time": "2024-12-20T05:19:58.744723",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def objective_sub3(trial):\n",
    "    CatBoost_Params = {\n",
    "        'learning_rate': trial.suggest_float('catboost_learning_rate', 1e-3, 0.3, log=True),\n",
    "        'depth': trial.suggest_int('catboost_depth', 4, 10),\n",
    "        'iterations': 200,\n",
    "        'random_seed': SEED,\n",
    "        'verbose': 0,\n",
    "        'l2_leaf_reg': trial.suggest_float('catboost_l2_leaf_reg', 0.01, 10.0, log=True),\n",
    "        'task_type': 'GPU',  \n",
    "        'devices': '0',\n",
    "        'cat_features': cat_c\n",
    "    }\n",
    "    XGB_Params = {\n",
    "        'n_estimators': trial.suggest_int('xgb_max_depth', 200, 1000),\n",
    "        'max_depth': trial.suggest_int('xgb_max_depth', 1, 10),\n",
    "        'learning_rate': trial.suggest_float('xgb_learning_rate', 0.01, 0.1, log=True),\n",
    "        'subsample': trial.suggest_float('xgb_subsample', 0.1, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('xgb_colsample_bytree', 0.05, 1.0),\n",
    "        'gamma': trial.suggest_float('xgb_gamma', 1e-2, 1.0),\n",
    "        'min_child_weight': trial.suggest_int('xgb_min_child_weight', 1, 100),\n",
    "        'eval_metric': 'rmse',\n",
    "        'objective': 'reg:squarederror',\n",
    "        'tree_method': 'gpu_hist',\n",
    "        'predictor': 'gpu_predictor',\n",
    "        'gpu_id': 0,\n",
    "        'random_state': SEED\n",
    "    }\n",
    "    Params = {\n",
    "        'max_depth': trial.suggest_int('lightgbm_max_depth', 1, 12),\n",
    "        'min_data_in_leaf': trial.suggest_int('lightgbm_min_data_in_leaf', 1, 100),\n",
    "        'num_leaves': trial.suggest_int('lightgbm_num_leaves', 8, 500),\n",
    "        'learning_rate': trial.suggest_float('lightgbm_learning_rate', 0.01, 0.1),\n",
    "        'feature_fraction': 0.893,\n",
    "        'bagging_fraction': 0.784,\n",
    "        'bagging_freq': 4,\n",
    "        'lambda_l1': 10,\n",
    "        'lambda_l2': 0.01,\n",
    "        'device_type': 'gpu',\n",
    "        'gpu_device_id': 0, \n",
    "        'verbosity': -1\n",
    "    }\n",
    "    LightGBM_Model = LGBMRegressor(**Params)    \n",
    "    XGB_Model = XGBRegressor(**XGB_Params)\n",
    "    CatBoost_Model = CatBoostRegressor(**CatBoost_Params)\n",
    "    voting_model = VotingRegressor(estimators=[\n",
    "        ('lightgbm', LightGBM_Model),\n",
    "        ('xgboost', XGB_Model),\n",
    "        ('catboost', CatBoost_Model),\n",
    "    ])\n",
    "    X = train.drop(['sii'], axis=1)\n",
    "    y = train['sii']\n",
    "\n",
    "    submission3, val_score, _, _, _ = TrainML(voting_model, X_sub3, y_sub3, test_sub3)\n",
    "    return val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d3447cba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T05:19:58.873766Z",
     "iopub.status.busy": "2024-12-20T05:19:58.873260Z",
     "iopub.status.idle": "2024-12-20T05:19:58.878393Z",
     "shell.execute_reply": "2024-12-20T05:19:58.877290Z"
    },
    "papermill": {
     "duration": 0.042764,
     "end_time": "2024-12-20T05:19:58.880564",
     "exception": false,
     "start_time": "2024-12-20T05:19:58.837800",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# study_sub3 = optuna.create_study(direction='maximize')\n",
    "# study_sub3.optimize(objective_sub3, n_trials=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "53f8aa98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T05:19:58.949860Z",
     "iopub.status.busy": "2024-12-20T05:19:58.949220Z",
     "iopub.status.idle": "2024-12-20T05:19:58.966263Z",
     "shell.execute_reply": "2024-12-20T05:19:58.965147Z"
    },
    "papermill": {
     "duration": 0.054074,
     "end_time": "2024-12-20T05:19:58.968587",
     "exception": false,
     "start_time": "2024-12-20T05:19:58.914513",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# best_params_sub3 = study_sub3.best_params\n",
    "# print(\"Best hyperparameters:\", best_params_sub3)\n",
    "'''\n",
    "CatBoost_Best_Params = {\n",
    "    'learning_rate': best_params_sub2['catboost_learning_rate'],\n",
    "    'depth': best_params_sub2['catboost_depth'],\n",
    "    'iterations': 200,\n",
    "    'random_seed': SEED,\n",
    "    'verbose': 0,\n",
    "    'l2_leaf_reg': best_params_sub2['catboost_l2_leaf_reg'],\n",
    "    'task_type': 'GPU',  \n",
    "    'devices': '0',\n",
    "    'cat_features': cat_c\n",
    "}\n",
    "\n",
    "XGB_Best_Params = {\n",
    "    'n_estimators': 1000,\n",
    "    'max_depth': best_params_sub2['xgb_max_depth'],\n",
    "    'learning_rate': best_params_sub2['xgb_learning_rate'],\n",
    "    'subsample': best_params_sub2['xgb_subsample'],\n",
    "    'colsample_bytree': best_params_sub2['xgb_colsample_bytree'],\n",
    "    'gamma': best_params_sub2['xgb_gamma'],\n",
    "    'min_child_weight': best_params_sub2['xgb_min_child_weight'],\n",
    "    'eval_metric': 'rmse',\n",
    "    'objective': 'reg:squarederror',\n",
    "    'tree_method': 'gpu_hist',\n",
    "    'predictor': 'gpu_predictor',\n",
    "    'gpu_id': 0,\n",
    "    'random_state': SEED,\n",
    "}\n",
    "\n",
    "LightGBM_Best_Params = {\n",
    "    'max_depth': best_params_sub2['lightgbm_max_depth'],\n",
    "    'min_data_in_leaf': best_params_sub2['lightgbm_min_data_in_leaf'],\n",
    "    'num_leaves': best_params_sub2['lightgbm_num_leaves'],\n",
    "    'learning_rate': best_params_sub2['lightgbm_learning_rate'],\n",
    "    'feature_fraction': best_params_sub2['lightgbm_feature_fraction'],\n",
    "    'bagging_fraction': best_params_sub2['bagging_fraction'],\n",
    "    'bagging_freq': best_params_sub2['bagging_freq'],\n",
    "    'lambda_l1': best_params_sub2['lambda_l1'],\n",
    "    'lambda_l2': best_params_sub2['lambda_l2'],\n",
    "    'device_type': 'gpu',\n",
    "    'gpu_device_id': 0,\n",
    "    'verbosity': -1\n",
    "} '''\n",
    "Light = LGBMRegressor(**LightGBM_Best_Params, random_state=SEED, verbose=-1, n_estimators=300)\n",
    "XGB_Model = XGBRegressor(**XGB_Best_Params)\n",
    "CatBoost_Model = CatBoostRegressor(**CatBoost_Best_Params)\n",
    "\n",
    "# Combine models using Voting Regressor\n",
    "voting_model = VotingRegressor(estimators=[\n",
    "    ('lightgbm', Light),\n",
    "    ('xgboost', XGB_Model),\n",
    "    ('catboost', CatBoost_Model)\n",
    "])\n",
    "X = train.drop(['sii'], axis=1)\n",
    "y = train['sii']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d73b43f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T05:19:59.036361Z",
     "iopub.status.busy": "2024-12-20T05:19:59.035944Z",
     "iopub.status.idle": "2024-12-20T05:19:59.040574Z",
     "shell.execute_reply": "2024-12-20T05:19:59.039547Z"
    },
    "papermill": {
     "duration": 0.040884,
     "end_time": "2024-12-20T05:19:59.042695",
     "exception": false,
     "start_time": "2024-12-20T05:19:59.001811",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# submission3, val_score_sub3, _, _, _ = TrainML(voting_model, X_sub3, y_sub3, test_sub3)\n",
    "# print(\"Val score sub3 with best parameters:\", val_score_sub3)\n",
    "# submission3.to_csv(\"submission3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a7f591",
   "metadata": {
    "papermill": {
     "duration": 0.035568,
     "end_time": "2024-12-20T05:19:59.111712",
     "exception": false,
     "start_time": "2024-12-20T05:19:59.076144",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Submission 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6fde9d6b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T05:19:59.181672Z",
     "iopub.status.busy": "2024-12-20T05:19:59.180508Z",
     "iopub.status.idle": "2024-12-20T05:19:59.191085Z",
     "shell.execute_reply": "2024-12-20T05:19:59.190039Z"
    },
    "papermill": {
     "duration": 0.048247,
     "end_time": "2024-12-20T05:19:59.193804",
     "exception": false,
     "start_time": "2024-12-20T05:19:59.145557",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "ensemble = VotingRegressor(estimators=[\n",
    "    ('lgb', Pipeline(steps=[('imputer', imputer), ('regressor', LGBMRegressor(random_state=SEED))])),\n",
    "    ('xgb', Pipeline(steps=[('imputer', imputer), ('regressor', XGBRegressor(random_state=SEED))])),\n",
    "    ('cat', Pipeline(steps=[('imputer', imputer), ('regressor', CatBoostRegressor(random_state=SEED, silent=True))])),\n",
    "    ('rf', Pipeline(steps=[('imputer', imputer), ('regressor', RandomForestRegressor(random_state=SEED))])),\n",
    "    ('gb', Pipeline(steps=[('imputer', imputer), ('regressor', GradientBoostingRegressor(random_state=SEED))]))\n",
    "])\n",
    "\n",
    "# submission4, val_score_sub4, _, _, _= TrainML(ensemble, X_sub3, y_sub3, test_sub3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279dc0e7",
   "metadata": {
    "papermill": {
     "duration": 0.032869,
     "end_time": "2024-12-20T05:19:59.259590",
     "exception": false,
     "start_time": "2024-12-20T05:19:59.226721",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Submission 5: MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f1c2515f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T05:19:59.328710Z",
     "iopub.status.busy": "2024-12-20T05:19:59.328242Z",
     "iopub.status.idle": "2024-12-20T05:19:59.514381Z",
     "shell.execute_reply": "2024-12-20T05:19:59.513299Z"
    },
    "papermill": {
     "duration": 0.224551,
     "end_time": "2024-12-20T05:19:59.516967",
     "exception": false,
     "start_time": "2024-12-20T05:19:59.292416",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "time_series_cols = train_ts.columns.tolist()\n",
    "time_series_cols.remove(\"id\")\n",
    "featuresCols = ['Basic_Demos-Enroll_Season', 'Basic_Demos-Age', 'Basic_Demos-Sex',\n",
    "                'CGAS-Season', 'CGAS-CGAS_Score', 'Physical-Season', 'Physical-BMI',\n",
    "                'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference',\n",
    "                'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP',\n",
    "                'Fitness_Endurance-Season', 'Fitness_Endurance-Max_Stage',\n",
    "                'Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec',\n",
    "                'FGC-Season', 'FGC-FGC_CU', 'FGC-FGC_CU_Zone', 'FGC-FGC_GSND',\n",
    "                'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone', 'FGC-FGC_PU',\n",
    "                'FGC-FGC_PU_Zone', 'FGC-FGC_SRL', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR',\n",
    "                'FGC-FGC_SRR_Zone', 'FGC-FGC_TL', 'FGC-FGC_TL_Zone', 'BIA-Season',\n",
    "                'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC', 'BIA-BIA_BMI',\n",
    "                'BIA-BIA_BMR', 'BIA-BIA_DEE', 'BIA-BIA_ECW', 'BIA-BIA_FFM',\n",
    "                'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_Fat', 'BIA-BIA_Frame_num',\n",
    "                'BIA-BIA_ICW', 'BIA-BIA_LDM', 'BIA-BIA_LST', 'BIA-BIA_SMM',\n",
    "                'BIA-BIA_TBW', 'PAQ_A-Season', 'PAQ_A-PAQ_A_Total', 'PAQ_C-Season',\n",
    "                'PAQ_C-PAQ_C_Total', 'SDS-Season', 'SDS-SDS_Total_Raw',\n",
    "                'SDS-SDS_Total_T', 'PreInt_EduHx-Season',\n",
    "                'PreInt_EduHx-computerinternet_hoursday', 'sii',\n",
    "               \n",
    "                #  'PCIAT-Season', 'PCIAT-PCIAT_01', 'PCIAT-PCIAT_02', 'PCIAT-PCIAT_03', 'PCIAT-PCIAT_04',\n",
    "                # 'PCIAT-PCIAT_05', 'PCIAT-PCIAT_06', 'PCIAT-PCIAT_07', 'PCIAT-PCIAT_08',\n",
    "                # 'PCIAT-PCIAT_09', 'PCIAT-PCIAT_10', 'PCIAT-PCIAT_11', 'PCIAT-PCIAT_12',\n",
    "                # 'PCIAT-PCIAT_13', 'PCIAT-PCIAT_14', 'PCIAT-PCIAT_15', 'PCIAT-PCIAT_16',\n",
    "                # 'PCIAT-PCIAT_17', 'PCIAT-PCIAT_18', 'PCIAT-PCIAT_19', 'PCIAT-PCIAT_20', 'PCIAT-PCIAT_Total',\n",
    "]\n",
    "cat_c = ['Basic_Demos-Enroll_Season', 'CGAS-Season', 'Physical-Season', \n",
    "          'Fitness_Endurance-Season', 'FGC-Season', 'BIA-Season', \n",
    "          'PAQ_A-Season', 'PAQ_C-Season', 'SDS-Season', 'PreInt_EduHx-Season',\n",
    "        # 'PCIAT-Season',\n",
    "        ]\n",
    "train = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/train.csv')\n",
    "test = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/test.csv')\n",
    "train = pd.merge(train, train_ts, how=\"left\", on='id')\n",
    "test = pd.merge(test, test_ts, how=\"left\", on='id')\n",
    "\n",
    "train = train.drop('id', axis=1)\n",
    "test = test.drop('id', axis=1)\n",
    "\n",
    "featuresCols += time_series_cols\n",
    "\n",
    "train = train[featuresCols]\n",
    "train = update(train)\n",
    "test = update(test)\n",
    "for col in cat_c:\n",
    "    if col in train.columns:\n",
    "        mapping = create_mapping(col, train)\n",
    "        train[col] = train[col].replace(mapping).astype(int)\n",
    "    if col in test.columns:\n",
    "        mappingTe = create_mapping(col, test)\n",
    "        test[col] = test[col].replace(mappingTe).astype(int)\n",
    "\n",
    "\n",
    "full_df = pd.concat([train,test])\n",
    "X_raw = torch.tensor(full_df.to_numpy()).float()\n",
    "random_extend(X_raw, 5000).shape\n",
    "\n",
    "for c in full_df.columns:\n",
    "    if c not in test.columns:\n",
    "        test[c] = float('nan')\n",
    "\n",
    "test = test[full_df.columns]\n",
    "\n",
    "X_tensor_test = torch.tensor(test.to_numpy()).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5ef7720b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T05:19:59.586468Z",
     "iopub.status.busy": "2024-12-20T05:19:59.586020Z",
     "iopub.status.idle": "2024-12-20T05:19:59.592378Z",
     "shell.execute_reply": "2024-12-20T05:19:59.591238Z"
    },
    "papermill": {
     "duration": 0.043796,
     "end_time": "2024-12-20T05:19:59.594674",
     "exception": false,
     "start_time": "2024-12-20T05:19:59.550878",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3a09fe5c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T05:19:59.662930Z",
     "iopub.status.busy": "2024-12-20T05:19:59.662538Z",
     "iopub.status.idle": "2024-12-20T05:19:59.671063Z",
     "shell.execute_reply": "2024-12-20T05:19:59.669934Z"
    },
    "papermill": {
     "duration": 0.045972,
     "end_time": "2024-12-20T05:19:59.673727",
     "exception": false,
     "start_time": "2024-12-20T05:19:59.627755",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_epochs = 100\n",
    "num_folds = 2\n",
    "warmup_epochs = max(1, max_epochs // 10)\n",
    "imputer_args = Namespace(\n",
    "    batch_size=64,\n",
    "    max_epochs= max_epochs,\n",
    "    accum_iter=1,\n",
    "    mask_ratio=0.38,\n",
    "    embed_dim=16,\n",
    "    depth=6,\n",
    "    decoder_depth=4,\n",
    "    num_heads=4,\n",
    "    mlp_ratio=4.0,\n",
    "    encode_func='linear',\n",
    "    norm_field_loss=False,\n",
    "    weight_decay=0.05,\n",
    "    lr=None, blr=0.001,\n",
    "    min_lr=1e-05,\n",
    "    warmup_epochs=warmup_epochs,\n",
    "    device='cuda', seed=SEED, overwrite=True, pin_mem=True\n",
    ")\n",
    "\n",
    "\n",
    "SEED = random.randint(1, int(2e9))\n",
    "np.random.seed(SEED)\n",
    "indices = np.random.permutation(len(X_raw))\n",
    "X_raw = X_raw[indices]\n",
    "# submission5 = PerformImpute(imputer_args)\n",
    "# submission5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b863c2",
   "metadata": {
    "papermill": {
     "duration": 0.032582,
     "end_time": "2024-12-20T05:19:59.751404",
     "exception": false,
     "start_time": "2024-12-20T05:19:59.718822",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ce9f21ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T05:19:59.820960Z",
     "iopub.status.busy": "2024-12-20T05:19:59.820535Z",
     "iopub.status.idle": "2024-12-20T05:19:59.831951Z",
     "shell.execute_reply": "2024-12-20T05:19:59.830912Z"
    },
    "papermill": {
     "duration": 0.047824,
     "end_time": "2024-12-20T05:19:59.834213",
     "exception": false,
     "start_time": "2024-12-20T05:19:59.786389",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority voting completed and saved to 'Final_Submission.csv'\n"
     ]
    }
   ],
   "source": [
    "\"\"\"sub1 = submission1\n",
    "sub2 = submission2\n",
    "sub3 = submission3\n",
    "sub4 = submission4\n",
    "sub5 = submission5\n",
    "\n",
    "sub1 = sub1.sort_values(by='id').reset_index(drop=True)\n",
    "sub2 = sub2.sort_values(by='id').reset_index(drop=True)\n",
    "sub3 = sub3.sort_values(by='id').reset_index(drop=True)\n",
    "sub4 = sub4.sort_values(by='id').reset_index(drop=True)\n",
    "sub5 = sub5.sort_values(by='id').reset_index(drop=True)\n",
    "\n",
    "\n",
    "combined = pd.DataFrame({\n",
    "    'id': sub1['id'],\n",
    "    'sii_1': sub1['sii'],\n",
    "    'sii_2': sub2['sii'],\n",
    "    'sii_3': sub3['sii'],\n",
    "    'sii_4': sub4['sii'],\n",
    "    'sii_5': sub5['sii'],\n",
    "})\n",
    "\n",
    "def majority_vote(row):\n",
    "    return row.mode()[0]\n",
    "\n",
    "combined['final_sii'] = combined[['sii_1', 'sii_2', 'sii_3', 'sii_4']].apply(majority_vote, axis=1)\n",
    "\n",
    "final_submission = combined[['id', 'final_sii']].rename(columns={'final_sii': 'sii'})\n",
    "\"\"\"\n",
    "final_submission = submission1\n",
    "final_submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"Majority voting completed and saved to 'Final_Submission.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "43b4c3d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T05:19:59.901395Z",
     "iopub.status.busy": "2024-12-20T05:19:59.900485Z",
     "iopub.status.idle": "2024-12-20T05:19:59.910843Z",
     "shell.execute_reply": "2024-12-20T05:19:59.909937Z"
    },
    "papermill": {
     "duration": 0.045916,
     "end_time": "2024-12-20T05:19:59.912848",
     "exception": false,
     "start_time": "2024-12-20T05:19:59.866932",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sii</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00008ff9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000fd460</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00105258</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00115b9f</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0016bb22</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>001f3379</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0038ba98</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0068a485</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0069fbed</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0083e397</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0087dd65</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>00abe655</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>00ae59c9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>00af6387</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>00bd4359</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>00c0cd71</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>00d56d4b</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>00d9913d</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>00e6167c</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>00ebc35d</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  sii\n",
       "0   00008ff9    0\n",
       "1   000fd460    0\n",
       "2   00105258    1\n",
       "3   00115b9f    0\n",
       "4   0016bb22    2\n",
       "5   001f3379    1\n",
       "6   0038ba98    1\n",
       "7   0068a485    0\n",
       "8   0069fbed    2\n",
       "9   0083e397    2\n",
       "10  0087dd65    1\n",
       "11  00abe655    1\n",
       "12  00ae59c9    1\n",
       "13  00af6387    1\n",
       "14  00bd4359    2\n",
       "15  00c0cd71    1\n",
       "16  00d56d4b    0\n",
       "17  00d9913d    0\n",
       "18  00e6167c    0\n",
       "19  00ebc35d    1"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_submission"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 9643020,
     "sourceId": 81933,
     "sourceType": "competition"
    },
    {
     "datasetId": 921302,
     "sourceId": 7453542,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 264.389133,
   "end_time": "2024-12-20T05:20:02.962731",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-20T05:15:38.573598",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
