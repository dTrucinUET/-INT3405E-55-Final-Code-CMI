{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T10:29:40.613231Z",
     "iopub.status.busy": "2024-12-20T10:29:40.612998Z",
     "iopub.status.idle": "2024-12-20T10:29:57.450536Z",
     "shell.execute_reply": "2024-12-20T10:29:57.449871Z",
     "shell.execute_reply.started": "2024-12-20T10:29:40.613198Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import copy\n",
    "import pickle\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from scipy.optimize import minimize\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "import polars as pl\n",
    "import polars.selectors as cs\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator, FormatStrFormatter, PercentFormatter\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.optimizers import Adam\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from colorama import Fore, Style\n",
    "from IPython.display import clear_output\n",
    "import warnings\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.ensemble import VotingRegressor, RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import shap\n",
    "import plotly.express as px\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.options.display.max_columns = None\n",
    "SEED = 42\n",
    "n_splits = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-20T10:29:57.452522Z",
     "iopub.status.busy": "2024-12-20T10:29:57.451913Z",
     "iopub.status.idle": "2024-12-20T10:29:57.456875Z",
     "shell.execute_reply": "2024-12-20T10:29:57.456023Z",
     "shell.execute_reply.started": "2024-12-20T10:29:57.452499Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import gc\n",
    "import os\n",
    "import sys\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import torch\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "\n",
    "# TRAIN_OR_TEST = \"train\"\n",
    "\n",
    "# paths = glob(\n",
    "#     f\"/kaggle/input/child-mind-institute-problematic-internet-use/series_{TRAIN_OR_TEST}.parquet/id=*/part-0.parquet\"\n",
    "# )\n",
    "# print(len(paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T10:29:57.458340Z",
     "iopub.status.busy": "2024-12-20T10:29:57.458011Z",
     "iopub.status.idle": "2024-12-20T10:29:57.480713Z",
     "shell.execute_reply": "2024-12-20T10:29:57.480031Z",
     "shell.execute_reply.started": "2024-12-20T10:29:57.458312Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "seed_everything(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineer for sub 1,2,3,4,5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T10:29:57.481715Z",
     "iopub.status.busy": "2024-12-20T10:29:57.481516Z",
     "iopub.status.idle": "2024-12-20T10:29:57.525373Z",
     "shell.execute_reply": "2024-12-20T10:29:57.524832Z",
     "shell.execute_reply.started": "2024-12-20T10:29:57.481698Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "def feature_engineering_v2(df, selector=None, imputer=None, fit=True):\n",
    "    df = df.loc[:, ~df.columns.duplicated()]\n",
    "    if fit: \n",
    "        y = df['sii']\n",
    "\n",
    "    numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "    season_cols = [col for col in df.columns if 'Season' in col]\n",
    "    pciat_cols = [col for col in df.columns if 'PCIAT' in col and 'Season' not in col]\n",
    "    remaining_numeric_cols = [col for col in numeric_cols if col not in pciat_cols and col not in ['sii']]\n",
    "    X = df[remaining_numeric_cols]\n",
    "    if np.any(np.isinf(X)):\n",
    "        X = X.replace([np.inf, -np.inf], np.nan)\n",
    "    if fit: \n",
    "        imputer = KNNImputer()\n",
    "        imputed_data = imputer.fit_transform(X)\n",
    "        train_imputed = pd.DataFrame(imputed_data, columns=remaining_numeric_cols)\n",
    "        X = train_imputed\n",
    "    else:\n",
    "        X = imputer.transform(X)\n",
    "\n",
    "    if fit:\n",
    "        # estimator = RandomForestRegressor(random_state=42)\n",
    "        # selector = RFECV(estimator, min_features_to_select=5, step=3, cv=5)\n",
    "        selector = SelectKBest(score_func=f_regression, k=30)\n",
    "        X_new = selector.fit_transform(X, y)\n",
    "        selected_features = X.columns[selector.get_support()]\n",
    "    else: \n",
    "        X_new = selector.transform(X)\n",
    "        selected_features = [col for col, selected in zip(remaining_numeric_cols, selector.get_support()) if selected]\n",
    "    df_selected = pd.DataFrame(X_new, columns=selected_features)\n",
    "    return df_selected, selector, imputer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoEncoder for Sub 1,2,3,4,5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T10:29:57.526195Z",
     "iopub.status.busy": "2024-12-20T10:29:57.526013Z",
     "iopub.status.idle": "2024-12-20T10:29:57.534049Z",
     "shell.execute_reply": "2024-12-20T10:29:57.533355Z",
     "shell.execute_reply.started": "2024-12-20T10:29:57.526179Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, encoding_dim):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, encoding_dim*3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(encoding_dim*3, encoding_dim*2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(encoding_dim*2, encoding_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_dim, encoding_dim*2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(encoding_dim*2, encoding_dim*3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(encoding_dim*3, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "\n",
    "def perform_autoencoder(df, encoding_dim=50, epochs=50, batch_size=32):\n",
    "    scaler = StandardScaler()\n",
    "    df_scaled = scaler.fit_transform(df)\n",
    "    \n",
    "    data_tensor = torch.FloatTensor(df_scaled)\n",
    "    \n",
    "    input_dim = data_tensor.shape[1]\n",
    "    autoencoder = AutoEncoder(input_dim, encoding_dim)\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(autoencoder.parameters())\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for i in range(0, len(data_tensor), batch_size):\n",
    "            batch = data_tensor[i : i + batch_size]\n",
    "            optimizer.zero_grad()\n",
    "            reconstructed = autoencoder(batch)\n",
    "            loss = criterion(reconstructed, batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}]')    \n",
    "    return autoencoder, scaler\n",
    "\n",
    "def encode_data(autoencoder, scaler, df):\n",
    "    df_scaled = scaler.transform(df)\n",
    "    data_tensor = torch.FloatTensor(df_scaled)\n",
    "    with torch.no_grad():\n",
    "        encoded_data = autoencoder.encoder(data_tensor).numpy()\n",
    "\n",
    "    df_encoded = pd.DataFrame(encoded_data, columns=[f'Enc_{i + 1}' for i in range(encoded_data.shape[1])])\n",
    "    return df_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TrainML for Sub 2,3,4,5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T10:29:57.536393Z",
     "iopub.status.busy": "2024-12-20T10:29:57.536207Z",
     "iopub.status.idle": "2024-12-20T10:29:57.553790Z",
     "shell.execute_reply": "2024-12-20T10:29:57.553105Z",
     "shell.execute_reply.started": "2024-12-20T10:29:57.536376Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def process_file(filename, dirname):\n",
    "    df = pd.read_parquet(os.path.join(dirname, filename, 'part-0.parquet'))\n",
    "    df.drop('step', axis=1, inplace=True)\n",
    "    return df.describe().values.reshape(-1), filename.split('=')[1]\n",
    "\n",
    "def load_time_series(dirname) -> pd.DataFrame:\n",
    "    ids = os.listdir(dirname)\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        results = list(tqdm(executor.map(lambda fname: process_file(fname, dirname), ids), total=len(ids)))\n",
    "    stats, indexes = zip(*results)\n",
    "    df = pd.DataFrame(stats, columns=[f\"stat_{i}\" for i in range(len(stats[0]))])\n",
    "    df['id'] = indexes\n",
    "    return df\n",
    "\n",
    "def update(df):\n",
    "    global cat_c\n",
    "    for c in cat_c: \n",
    "        df[c] = df[c].fillna('Missing')\n",
    "        df[c] = df[c].astype('category')\n",
    "    return df\n",
    "\n",
    "def create_mapping(column, dataset):\n",
    "    unique_values = dataset[column].unique()\n",
    "    return {value: idx for idx, value in enumerate(unique_values)}\n",
    "\n",
    "def quadratic_weighted_kappa(y_true, y_pred):\n",
    "    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n",
    "\n",
    "def threshold_Rounder(oof_non_rounded, thresholds):\n",
    "    return np.where(oof_non_rounded < thresholds[0], 0,\n",
    "                    np.where(oof_non_rounded < thresholds[1], 1,\n",
    "                             np.where(oof_non_rounded < thresholds[2], 2, 3)))\n",
    "\n",
    "def evaluate_predictions(thresholds, y_true, oof_non_rounded):\n",
    "    rounded_p = threshold_Rounder(oof_non_rounded, thresholds)\n",
    "    return -quadratic_weighted_kappa(y_true, rounded_p)\n",
    "    \n",
    "def TrainML(model_class, X, y, test_data):\n",
    "    SKF = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
    "    \n",
    "    train_S = []\n",
    "    test_S = []\n",
    "    \n",
    "    oof_non_rounded = np.zeros(len(y), dtype=float) \n",
    "    oof_rounded = np.zeros(len(y), dtype=int) \n",
    "    test_preds = np.zeros((len(test_data), n_splits))\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(tqdm(SKF.split(X, y), desc=\"Training Folds\", total=n_splits)):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[test_idx]\n",
    "        model = clone(model_class)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_val_pred = model.predict(X_val)\n",
    "\n",
    "        oof_non_rounded[test_idx] = y_val_pred\n",
    "        y_val_pred_rounded = y_val_pred.round(0).astype(int)\n",
    "        oof_rounded[test_idx] = y_val_pred_rounded\n",
    "\n",
    "        train_kappa = quadratic_weighted_kappa(y_train, y_train_pred.round(0).astype(int))\n",
    "        val_kappa = quadratic_weighted_kappa(y_val, y_val_pred_rounded)\n",
    "\n",
    "        train_S.append(train_kappa)\n",
    "        test_S.append(val_kappa)\n",
    "        \n",
    "        test_preds[:, fold] = model.predict(test_data)\n",
    "        \n",
    "        print(f\"Fold {fold+1} - Train QWK: {train_kappa:.4f}, Validation QWK: {val_kappa:.4f}\")\n",
    "        clear_output(wait=True)\n",
    "    print(f\"Mean Train QWK --> {np.mean(train_S):.4f}\")\n",
    "    print(f\"Mean Validation QWK ---> {np.mean(test_S):.4f}\")\n",
    "\n",
    "    KappaOPtimizer = minimize(evaluate_predictions,\n",
    "                              x0=[0.5, 1.5, 2.5], args=(y, oof_non_rounded), \n",
    "                              method='Nelder-Mead')\n",
    "    assert KappaOPtimizer.success, \"Optimization did not converge.\"\n",
    "    print('OPTIMIZED THRESHOLDS', KappaOPtimizer.x)\n",
    "    oof_tuned = threshold_Rounder(oof_non_rounded, KappaOPtimizer.x)\n",
    "    tKappa = quadratic_weighted_kappa(y, oof_tuned)\n",
    "\n",
    "    print(f\"----> || Optimized QWK SCORE :: {Fore.CYAN}{Style.BRIGHT} {tKappa:.3f}{Style.RESET_ALL}\")\n",
    "\n",
    "    tpm = test_preds.mean(axis=1)\n",
    "    tpTuned = threshold_Rounder(tpm, KappaOPtimizer.x)\n",
    "    \n",
    "    submission = pd.DataFrame({\n",
    "        'id': sample['id'],\n",
    "        'sii': tpTuned\n",
    "    })\n",
    "    optimized_thresholds = KappaOPtimizer.x\n",
    "    return submission, oof_tuned, oof_non_rounded, y, optimized_thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T10:29:57.555134Z",
     "iopub.status.busy": "2024-12-20T10:29:57.554926Z",
     "iopub.status.idle": "2024-12-20T10:29:57.573411Z",
     "shell.execute_reply": "2024-12-20T10:29:57.572708Z",
     "shell.execute_reply.started": "2024-12-20T10:29:57.555117Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def TrainML_Sub1(model_class, X, y, test_data):\n",
    "    SKF = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
    "    \n",
    "    train_S = []\n",
    "    test_S = []\n",
    "    \n",
    "    oof_non_rounded = np.zeros(len(y), dtype=float) \n",
    "    oof_rounded = np.zeros(len(y), dtype=int) \n",
    "    test_preds = np.zeros((len(test_data), n_splits))\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(tqdm(SKF.split(X, y), desc=\"Training Folds\", total=n_splits)):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[test_idx]\n",
    "        \n",
    "        # Feature engineering for training and validation\n",
    "        X_train, selector_tr, imputer_tr = feature_engineering_v2(X_train, fit=True)\n",
    "        X_val, _, _ = feature_engineering_v2(X_val, selector_tr, imputer_tr, fit=False)\n",
    "        # Train the model\n",
    "        model = clone(model_class)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_val_pred = model.predict(X_val)\n",
    "\n",
    "        oof_non_rounded[test_idx] = y_val_pred\n",
    "        y_val_pred_rounded = y_val_pred.round(0).astype(int)\n",
    "        oof_rounded[test_idx] = y_val_pred_rounded\n",
    "\n",
    "        train_kappa = quadratic_weighted_kappa(y_train, y_train_pred.round(0).astype(int))\n",
    "        val_kappa = quadratic_weighted_kappa(y_val, y_val_pred_rounded)\n",
    "\n",
    "        train_S.append(train_kappa)\n",
    "        test_S.append(val_kappa)\n",
    "        \n",
    "        # Feature engineering for test data\n",
    "        test_data_fe, _, _ = feature_engineering_v2(test_data, selector_tr, imputer_tr, fit=False)\n",
    "        test_preds[:, fold] = model.predict(test_data_fe)\n",
    "        \n",
    "        print(f\"Fold {fold+1} - Train QWK: {train_kappa:.4f}, Validation QWK: {val_kappa:.4f}\")\n",
    "        clear_output(wait=True)\n",
    "    \n",
    "    print(f\"Mean Train QWK --> {np.mean(train_S):.4f}\")\n",
    "    print(f\"Mean Validation QWK ---> {np.mean(test_S):.4f}\")\n",
    "\n",
    "    KappaOptimizer = minimize(evaluate_predictions,\n",
    "                              x0=[0.5, 1.5, 2.5], args=(y, oof_non_rounded), \n",
    "                              method='Nelder-Mead')\n",
    "    assert KappaOptimizer.success, \"Optimization did not converge.\"\n",
    "    print('OPTIMIZED THRESHOLDS', KappaOptimizer.x)\n",
    "    \n",
    "    oof_tuned = threshold_Rounder(oof_non_rounded, KappaOptimizer.x)\n",
    "    tKappa = quadratic_weighted_kappa(y, oof_tuned)\n",
    "\n",
    "    print(f\"----> || Optimized QWK SCORE :: {Fore.CYAN}{Style.BRIGHT} {tKappa:.3f}{Style.RESET_ALL}\")\n",
    "\n",
    "    tpm = test_preds.mean(axis=1)\n",
    "    tpTuned = threshold_Rounder(tpm, KappaOptimizer.x)\n",
    "    \n",
    "    submission = pd.DataFrame({\n",
    "        'id': sample['id'],\n",
    "        'sii': tpTuned\n",
    "    })\n",
    "    optimized_thresholds = KappaOptimizer.x\n",
    "    return (submission, tKappa, oof_tuned, oof_non_rounded, y, optimized_thresholds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T10:29:57.574339Z",
     "iopub.status.busy": "2024-12-20T10:29:57.574140Z",
     "iopub.status.idle": "2024-12-20T10:29:57.660998Z",
     "shell.execute_reply": "2024-12-20T10:29:57.660211Z",
     "shell.execute_reply.started": "2024-12-20T10:29:57.574321Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/train.csv')\n",
    "test = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/test.csv')\n",
    "sample = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/sample_submission.csv')\n",
    "\n",
    "total_features = list(test.columns)\n",
    "total_features.remove('id')\n",
    "\n",
    "cat_c = ['Basic_Demos-Enroll_Season', 'CGAS-Season', 'Physical-Season', \n",
    "          'Fitness_Endurance-Season', 'FGC-Season', 'BIA-Season', \n",
    "          'PAQ_A-Season', 'PAQ_C-Season', 'SDS-Season', 'PreInt_EduHx-Season']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T10:29:57.662067Z",
     "iopub.status.busy": "2024-12-20T10:29:57.661831Z",
     "iopub.status.idle": "2024-12-20T10:29:57.667101Z",
     "shell.execute_reply": "2024-12-20T10:29:57.666345Z",
     "shell.execute_reply.started": "2024-12-20T10:29:57.662044Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n"
     ]
    }
   ],
   "source": [
    "noseason_features = ['Basic_Demos-Age', 'Basic_Demos-Sex',\n",
    "                'CGAS-CGAS_Score', 'Physical-BMI',\n",
    "                'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference',\n",
    "                'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP',\n",
    "                'Fitness_Endurance-Max_Stage',\n",
    "                'Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec',\n",
    "                'FGC-FGC_CU', 'FGC-FGC_CU_Zone', 'FGC-FGC_GSND',\n",
    "                'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone', 'FGC-FGC_PU',\n",
    "                'FGC-FGC_PU_Zone', 'FGC-FGC_SRL', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR',\n",
    "                'FGC-FGC_SRR_Zone', 'FGC-FGC_TL', 'FGC-FGC_TL_Zone',\n",
    "                'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC', 'BIA-BIA_BMI',\n",
    "                'BIA-BIA_BMR', 'BIA-BIA_DEE', 'BIA-BIA_ECW', 'BIA-BIA_FFM',\n",
    "                'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_Fat', 'BIA-BIA_Frame_num',\n",
    "                'BIA-BIA_ICW', 'BIA-BIA_LDM', 'BIA-BIA_LST', 'BIA-BIA_SMM',\n",
    "                'BIA-BIA_TBW', 'PAQ_A-PAQ_A_Total',\n",
    "                'PAQ_C-PAQ_C_Total', 'SDS-SDS_Total_Raw',\n",
    "                'SDS-SDS_Total_T',\n",
    "                'PreInt_EduHx-computerinternet_hoursday', 'BMI_Age','Internet_Hours_Age','BMI_Internet_Hours',\n",
    "                'BFP_BMI', 'FFMI_BFP', 'FMI_BFP', 'LST_TBW', 'BFP_BMR', 'BFP_DEE', 'BMR_Weight', 'DEE_Weight',\n",
    "                'SMM_Height', 'Muscle_to_Fat', 'Hydration_Status', 'ICW_TBW','BMI_PHR']\n",
    "print(len(noseason_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T10:29:57.668063Z",
     "iopub.status.busy": "2024-12-20T10:29:57.667832Z",
     "iopub.status.idle": "2024-12-20T10:31:05.743645Z",
     "shell.execute_reply": "2024-12-20T10:31:05.742877Z",
     "shell.execute_reply.started": "2024-12-20T10:29:57.668045Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 996/996 [01:07<00:00, 14.68it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 12.14it/s]\n"
     ]
    }
   ],
   "source": [
    "train_ts = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_train.parquet\")\n",
    "test_ts = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T10:31:05.744825Z",
     "iopub.status.busy": "2024-12-20T10:31:05.744500Z",
     "iopub.status.idle": "2024-12-20T10:31:15.842604Z",
     "shell.execute_reply": "2024-12-20T10:31:15.841665Z",
     "shell.execute_reply.started": "2024-12-20T10:31:05.744793Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/100], Loss: 1.4613]\n",
      "Epoch [100/100], Loss: 1.4550]\n"
     ]
    }
   ],
   "source": [
    "df_train = train_ts.drop('id', axis=1)\n",
    "df_test = test_ts.drop('id', axis=1)\n",
    "autoencoder, scaler = perform_autoencoder(df_train, encoding_dim=60, epochs=100, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T10:31:15.844417Z",
     "iopub.status.busy": "2024-12-20T10:31:15.843551Z",
     "iopub.status.idle": "2024-12-20T10:31:15.855430Z",
     "shell.execute_reply": "2024-12-20T10:31:15.854731Z",
     "shell.execute_reply.started": "2024-12-20T10:31:15.844391Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_ts_encoded = encode_data(autoencoder, scaler, df_train)\n",
    "test_ts_encoded = encode_data(autoencoder, scaler, df_test)\n",
    "test_ts_encoded.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T10:31:15.856734Z",
     "iopub.status.busy": "2024-12-20T10:31:15.856342Z",
     "iopub.status.idle": "2024-12-20T10:31:15.871239Z",
     "shell.execute_reply": "2024-12-20T10:31:15.870490Z",
     "shell.execute_reply.started": "2024-12-20T10:31:15.856701Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_ts_encoded[\"id\"]=train_ts[\"id\"]\n",
    "test_ts_encoded['id']=test_ts[\"id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T10:31:15.872143Z",
     "iopub.status.busy": "2024-12-20T10:31:15.871930Z",
     "iopub.status.idle": "2024-12-20T10:31:15.889298Z",
     "shell.execute_reply": "2024-12-20T10:31:15.888669Z",
     "shell.execute_reply.started": "2024-12-20T10:31:15.872112Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "time_series_cols = train_ts.columns.tolist()\n",
    "time_series_cols.remove(\"id\")\n",
    "time_encoded_cols = train_ts_encoded.columns.tolist()\n",
    "time_encoded_cols.remove(\"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T10:31:15.890335Z",
     "iopub.status.busy": "2024-12-20T10:31:15.890048Z",
     "iopub.status.idle": "2024-12-20T10:31:15.922524Z",
     "shell.execute_reply": "2024-12-20T10:31:15.921963Z",
     "shell.execute_reply.started": "2024-12-20T10:31:15.890300Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_sub1 = pd.merge(train, train_ts_encoded, how=\"left\", on='id')\n",
    "test_sub1 = pd.merge(test, test_ts_encoded, how=\"left\", on='id')\n",
    "train_sub1 = train_sub1.dropna(subset='sii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T10:31:15.923315Z",
     "iopub.status.busy": "2024-12-20T10:31:15.923137Z",
     "iopub.status.idle": "2024-12-20T10:31:15.926728Z",
     "shell.execute_reply": "2024-12-20T10:31:15.925839Z",
     "shell.execute_reply.started": "2024-12-20T10:31:15.923298Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X_sub1 = train_sub1\n",
    "y_sub1 = train_sub1['sii']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T10:31:15.927665Z",
     "iopub.status.busy": "2024-12-20T10:31:15.927457Z",
     "iopub.status.idle": "2024-12-20T10:31:15.942766Z",
     "shell.execute_reply": "2024-12-20T10:31:15.942136Z",
     "shell.execute_reply.started": "2024-12-20T10:31:15.927647Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "CatBoost_Best_Params = {\n",
    "    'learning_rate': 0.0021172579310639343,\n",
    "    'depth': 6,\n",
    "    'iterations': 130,\n",
    "    'random_seed': SEED,\n",
    "    'verbose': 0,\n",
    "    'l2_leaf_reg': 0.32557701990001503,\n",
    "}\n",
    "\n",
    "XGB_Best_Params = {\n",
    "    'n_estimators': 700,\n",
    "    'max_depth': 4,\n",
    "    'learning_rate': 0.03325152156380898,\n",
    "    'subsample': 0.25295047248406266,\n",
    "    'colsample_bytree': 0.9760859719849787,\n",
    "    'gamma': 0.20085951790463402,\n",
    "    'min_child_weight': 11,\n",
    "    'eval_metric': 'rmse',\n",
    "    'objective': 'reg:squarederror',\n",
    "    # 'tree_method': 'gpu_hist',\n",
    "    # 'predictor': 'gpu_predictor',\n",
    "    # 'gpu_id': 0\n",
    "}\n",
    "\n",
    "LightGBM_Best_Params = {\n",
    "    'max_depth': 3,\n",
    "    'min_data_in_leaf': 40,\n",
    "    'num_leaves': 190,\n",
    "    'learning_rate': 0.05107368421432176,\n",
    "    'feature_fraction': 0.9918350138636185,\n",
    "    'bagging_fraction': 0.9331400899763774,\n",
    "    'bagging_freq': 1,\n",
    "    'lambda_l1': 9.49641646280519,\n",
    "    'lambda_l2': 2.446305429623661,\n",
    "    'min_gain_to_split': 0.05262124930522051,\n",
    "    # 'device_type': 'gpu',\n",
    "    # 'gpu_device_id': 0,\n",
    "    'verbosity': -1\n",
    "}\n",
    "\n",
    "svm\n",
    "\n",
    "catboost_model = CatBoostRegressor(**CatBoost_Best_Params)\n",
    "xgb_model = XGBRegressor(**XGB_Best_Params)\n",
    "lightgbm_model = LGBMRegressor(**LightGBM_Best_Params)\n",
    "# tabnet_model = TabNetWrapper(\n",
    "#     n_d=64, n_a=64, n_steps=5, gamma=1.5, n_independent=2, n_shared=2, \n",
    "#     lambda_sparse=1e-4, optimizer_fn=torch.optim.Adam,\n",
    "#     optimizer_params=dict(lr=2e-2, weight_decay=1e-5), mask_type='entmax',\n",
    "#     scheduler_params=dict(mode=\"min\", patience=10, min_lr=1e-5, factor=0.5),\n",
    "#     scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau, verbose=1,\n",
    "#     device_name='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# )\n",
    "\n",
    "final_voting_model = VotingRegressor(estimators=[\n",
    "    ('lightgbm', lightgbm_model),\n",
    "    ('xgboost', xgb_model),\n",
    "    ('catboost', catboost_model),\n",
    "    # ('tabnet', tabnet_model)\n",
    "], weights=[4.0, 4.0, 4.0])\n",
    "\n",
    "X = train.drop(['sii'], axis=1)\n",
    "y = train['sii']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T10:31:15.946554Z",
     "iopub.status.busy": "2024-12-20T10:31:15.946359Z",
     "iopub.status.idle": "2024-12-20T10:31:28.910009Z",
     "shell.execute_reply": "2024-12-20T10:31:28.909289Z",
     "shell.execute_reply.started": "2024-12-20T10:31:15.946537Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Folds: 100%|██████████| 5/5 [00:12<00:00,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Train QWK --> 0.5441\n",
      "Mean Validation QWK ---> 0.3645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPTIMIZED THRESHOLDS [0.5785469  0.88500199 2.83677574]\n",
      "----> || Optimized QWK SCORE :: \u001b[36m\u001b[1m 0.462\u001b[0m\n",
      "Val score sub1 with best parameters: 0.46241743563848414\n"
     ]
    }
   ],
   "source": [
    "submission1, val_score_sub1, _, _, _, _ = TrainML_Sub1(catboost_, X_sub1, y_sub1, test_sub1)\n",
    "\n",
    "print(\"Val score sub1 with best parameters:\", val_score_sub1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submision 2: Sleep Detection Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, prepare the features used in my sleep detection model. Please refer to the implementation by [@tatamikenn](https://www.kaggle.com/tatamikenn) [here](https://www.kaggle.com/code/tatamikenn/sleep-hdcza-a-pure-heuristic-approach-lb-0-447).\n",
    "\n",
    "This pipeline processes accelerometer data for sleep detection, utilizing time-series datasets. It generates features to identify sleep episodes, static periods, and motion patterns, inspired by @tatamikenn's implementation.\n",
    "\n",
    "### transform Function\n",
    "The transform function processes input data to generate features for analysis. It breaks down the timestamp into components like year, month, day, hour, and weekday. It also groups data by night, adjusting the timestamp if necessary, and creates a unique night_group identifier for each night. Additionally, a cumulative step count (norm_step) is computed for each group to facilitate sequential analysis.\n",
    "\n",
    "### transform_series Function\n",
    "enhances the transform function by adding a new feature: detecting clipped ENMO values. It flags instances where the enmo (motion metric) is zero, marking potential data quality issues.\n",
    "\n",
    "### transform_events Function\n",
    "processes event data by adding a night column and pivoting the data. The events are rearranged by series_id, group_id, and night to simplify time-series analysis.\n",
    "\n",
    "### add_feature Function\n",
    "This function generates advanced features for sleep detection, including:\n",
    "\n",
    "Difference Features: Computes the differences in anglez (angular motion) and enmo (motion magnitude).\n",
    "\n",
    "Rolling Median: Calculates rolling medians of anglez_diff and enmo_diff over a 5-minute window.\n",
    "\n",
    "Critical Threshold: Determines static periods by evaluating anglez_diff variability over a day.\n",
    "\n",
    "Static and Sleep Blocks: Flags periods with minimal motion (is_static) and identifies sleep blocks over 30-minute windows.\n",
    "\n",
    "Sleep Episodes: Detects continuous sleep episodes, identifies the longest one, and flags interruptions in sleep.\n",
    "\n",
    "### create_heuristic Function\n",
    "\n",
    "The main function processes raw data files by converting timestamps and applying transformations. It calls the transform_series function to prepare the data and the add_feature function to generate sleep-related features. Finally, it saves the processed data into .parquet files for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T10:31:28.911824Z",
     "iopub.status.busy": "2024-12-20T10:31:28.911561Z",
     "iopub.status.idle": "2024-12-20T10:31:28.915494Z",
     "shell.execute_reply": "2024-12-20T10:31:28.914504Z",
     "shell.execute_reply.started": "2024-12-20T10:31:28.911794Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "MAX_FILE = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-12-20T10:31:28.916594Z",
     "iopub.status.busy": "2024-12-20T10:31:28.916382Z",
     "iopub.status.idle": "2024-12-20T10:31:28.935621Z",
     "shell.execute_reply": "2024-12-20T10:31:28.935000Z",
     "shell.execute_reply.started": "2024-12-20T10:31:28.916560Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def transform(df, night_offset=20):\n",
    "    return (\n",
    "        df.with_columns(\n",
    "            [\n",
    "                (pl.col(\"timestamp\").dt.year() - 2000).cast(pl.Int8).alias(\"year\"),\n",
    "                pl.col(\"timestamp\").dt.month().cast(pl.Int8).alias(\"month\"),\n",
    "                pl.col(\"timestamp\").dt.day().cast(pl.Int8).alias(\"day\"),\n",
    "                pl.col(\"timestamp\").dt.hour().cast(pl.Int8).alias(\"hour\"),\n",
    "                pl.col(\"timestamp\").dt.minute().cast(pl.Int8).alias(\"minute\"),\n",
    "                pl.col(\"timestamp\").dt.second().cast(pl.Int8).alias(\"second\"),\n",
    "                pl.col(\"timestamp\").dt.weekday().cast(pl.Int8).alias(\"weekday\"),\n",
    "            ]\n",
    "        )\n",
    "        .with_columns( \n",
    "            pl.when(pl.col(\"hour\") < night_offset)\n",
    "            .then(pl.col(\"timestamp\"))\n",
    "            .otherwise(pl.col(\"timestamp\") + pl.duration(days=1))\n",
    "            .dt.date()\n",
    "            .alias(\"night_group\"),\n",
    "        )\n",
    "        .with_columns(\n",
    "            [\n",
    "                (\n",
    "                    pl.col(\"series_id\") + pl.lit(\"_\") + pl.col(\"night_group\").cast(pl.Datetime).dt.strftime(\"%Y%m%d\")\n",
    "                ).alias(\"group_id\"),\n",
    "            ]\n",
    "        )\n",
    "        .with_columns(\n",
    "            [\n",
    "                pl.col(\"timestamp\").cum_count().over(\"group_id\").alias(\"norm_step\"),\n",
    "            ]\n",
    "        )\n",
    "        .drop([\"night_group\"])\n",
    "    )\n",
    "\n",
    "\n",
    "def transform_series(df):\n",
    "    return transform(df).with_columns(\n",
    "        [\n",
    "            (pl.col(\"enmo\") == 0).alias(\"is_enmo_clipped\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def transform_events(df):\n",
    "    return (\n",
    "        transform(df)\n",
    "        .with_columns(\n",
    "            [\n",
    "                pl.col(\"night\").cast(pl.UInt32).alias(\"night\"),\n",
    "            ]\n",
    "        )\n",
    "        .pivot([\"step\", \"timestamp\", \"tz_offset\"], [\"series_id\", \"group_id\", \"night\"], \"event\")\n",
    "    )\n",
    "\n",
    "\n",
    "def add_feature(\n",
    "    df,\n",
    "    day_group_col=\"group_id\",\n",
    "    term1=(5 * 60) // 5,\n",
    "    term2=(30 * 60) // 5,\n",
    "    term3=(60 * 60) // 5,\n",
    "    min_threshold=0.005,\n",
    "    max_threshold=0.04,\n",
    "    center=True,\n",
    "):\n",
    "    return (\n",
    "        df.with_columns(\n",
    "            [\n",
    "                pl.col(\"anglez\").diff(1).abs().alias(\"anglez_diff\"),\n",
    "                pl.col(\"enmo\").diff(1).abs().alias(\"enmo_diff\"),\n",
    "            ]\n",
    "        )\n",
    "        .with_columns(\n",
    "            [\n",
    "                pl.col(\"anglez_diff\")\n",
    "                .rolling_median(term1, center=center)  # 5 min window\n",
    "                .alias(\"anglez_diff_median_5min\"),\n",
    "                pl.col(\"enmo_diff\")\n",
    "                .rolling_median(term1, center=center)  # 5 min window\n",
    "                .alias(\"enmo_diff_median_5min\"),\n",
    "            ]\n",
    "        )\n",
    "        .with_columns(\n",
    "            [\n",
    "                pl.col(\"anglez_diff_median_5min\")\n",
    "                .quantile(0.1)\n",
    "                .clip(min_threshold, max_threshold)\n",
    "                .over(day_group_col)\n",
    "                .alias(\"critical_threshold\")\n",
    "            ]\n",
    "        )\n",
    "        .with_columns([(pl.col(\"anglez_diff_median_5min\") < pl.col(\"critical_threshold\") * 15).alias(\"is_static\")])\n",
    "        .with_columns(\n",
    "            [\n",
    "                pl.col(\"is_static\").cast(pl.Int32).rolling_sum(term2, center=center).alias(\"is_static_sum_30min\"),\n",
    "            ]\n",
    "        )\n",
    "        .with_columns([(pl.col(\"is_static_sum_30min\") == ((30 * 60) // 5)).alias(\"tmp\")])\n",
    "        .with_columns(\n",
    "            [\n",
    "                pl.col(\"tmp\").shift(term2 // 2).alias(\"tmp_left\"),\n",
    "                pl.col(\"tmp\").shift(-(term2 // 2)).alias(\"tmp_right\"),\n",
    "            ]\n",
    "        )\n",
    "        .with_columns(\n",
    "            [\n",
    "                (pl.col(\"tmp_left\") | pl.col(\"tmp_right\")).alias(\"is_sleep_block\"),\n",
    "            ]\n",
    "        )\n",
    "        .drop([\"tmp\", \"tmp_left\", \"tmp_right\"])\n",
    "        .with_columns([pl.col(\"is_sleep_block\").not_().alias(\"is_gap\")])\n",
    "        .with_columns([pl.col(\"is_gap\").cast(pl.Int32).rolling_sum(term3, center=center).alias(\"gap_length\")])\n",
    "        .with_columns([(pl.col(\"gap_length\") == term3).alias(\"tmp\")])\n",
    "        .with_columns(\n",
    "            [\n",
    "                pl.col(\"tmp\").shift(term3 // 2).alias(\"tmp_left\"),\n",
    "                pl.col(\"tmp\").shift(-(term3 // 2)).alias(\"tmp_right\"),\n",
    "            ]\n",
    "        )\n",
    "        .with_columns(\n",
    "            [\n",
    "                (pl.col(\"tmp_left\") | pl.col(\"tmp_right\")).alias(\"is_large_gap\"),\n",
    "            ]\n",
    "        )\n",
    "        .drop([\"tmp\", \"tmp_left\", \"tmp_right\"])\n",
    "        .with_columns([pl.col(\"is_large_gap\").not_().alias(\"is_sleep_episode\")])\n",
    "        #\n",
    "        # extract longest sleep episode\n",
    "        #\n",
    "        .with_columns(\n",
    "            [\n",
    "                # extract false->true transition\n",
    "                (\n",
    "                    (\n",
    "                        pl.col(\"is_sleep_episode\")\n",
    "                        & pl.col(\"is_sleep_episode\").shift(1, fill_value=pl.lit(False)).not_()\n",
    "                    )\n",
    "                    .cum_sum()\n",
    "                    .over(\"group_id\")\n",
    "                ).alias(\"sleep_episode_id\")\n",
    "            ]\n",
    "        )\n",
    "        .with_columns(\n",
    "            [pl.col(\"is_sleep_episode\").sum().over([\"group_id\", \"sleep_episode_id\"]).alias(\"sleep_episode_length\")]\n",
    "        )\n",
    "        .with_columns([pl.col(\"sleep_episode_length\").max().over([\"group_id\"]).alias(\"max_sleep_episode_length\")])\n",
    "        .with_columns(\n",
    "            [\n",
    "                (\n",
    "                    pl.col(\"is_sleep_episode\") & (pl.col(\"sleep_episode_length\") == pl.col(\"max_sleep_episode_length\"))\n",
    "                ).alias(\"is_longest_sleep_episode\")\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "use_columns = [\n",
    "    \"series_id\",\n",
    "    \"step\",\n",
    "    \"is_longest_sleep_episode\",\n",
    "    \"is_sleep_block\",\n",
    "    \"is_gap\",\n",
    "    \"is_large_gap\",\n",
    "    \"is_sleep_episode\",\n",
    "    \"is_static\",\n",
    "]\n",
    "\n",
    "def create_heuristic(paths, train_or_test):\n",
    "    i = 0\n",
    "    for path in tqdm(paths):\n",
    "        i += 1\n",
    "        if (i == MAX_FILE):\n",
    "            break\n",
    "        sdf = pl.read_parquet(path)\n",
    "    \n",
    "        # dummy timestamp\n",
    "        sdf = sdf.with_columns((pl.col(\"time_of_day\") == 0).cast(pl.Int32).cum_sum().alias(\"day_offset\"))\n",
    "        sdf = sdf.with_columns(\n",
    "            (\n",
    "                datetime.datetime(2020, 1, 1)\n",
    "                + (pl.col(\"day_offset\") * 86400_000_000 + pl.col(\"time_of_day\") / 1000).cast(pl.Duration(\"us\"))\n",
    "            ).alias(\"timestamp\")\n",
    "        )\n",
    "    \n",
    "        sdf = sdf.with_columns(pl.lit(path.split(\"/\")[-2]).alias(\"series_id\"))\n",
    "        sdf = sdf.sort(\"step\")\n",
    "        sdf = transform_series(sdf)\n",
    "        sdf = add_feature(sdf)\n",
    "        sdf = sdf[use_columns].fill_null(False)\n",
    "    \n",
    "        sidf = path.split(\"/\")[-2]\n",
    "        save_path = f\"/kaggle/working/heuristic_features/{train_or_test}/{sidf}.parquet\"\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        sdf.write_parquet(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Description\n",
    "The detection function is designed to detect sleep patterns from accelerometer data using a Transformer-GRU model. It processes the input data by extracting key features such as motion metrics, static periods, and sleep episodes, then applies a pre-trained ensemble model to generate predictions for sleep onset and wakeup times.\n",
    "\n",
    "### 2. Input Handling and Setup\n",
    "The function begins by loading paths to the input data (series_train.parquet) and reading model configurations from a config.yaml file. It initializes the Transformer-GRU model (ZzzTransformerGRUModel) and loads pre-trained weights to enable ensemble inference.\n",
    "\n",
    "### 3. Feature Engineering\n",
    "Several key features are extracted from the input data. Motion metrics such as anglez and enmo are calculated along with their differences (anglez_diff, enmo_diff). Static periods are identified by features like same_count (repeated motion) and large_diff_count (sudden changes). The longest sleep episodes and blocks are flagged from the heuristic features, providing insight into sleep patterns.\n",
    "\n",
    "### 4. Splitting Data into Blocks\n",
    "To facilitate efficient processing, the time-series data is divided into smaller blocks based on the configured BLOCK_SIZE. This approach ensures proper alignment with the model's patch size for input during inference.\n",
    "\n",
    "### 5. Model Inference\n",
    "The processed data is converted into a patch-based dataset (ZzzPatchDataset), which is fed into the Transformer-GRU model for inference. The ensemble model generates predictions for sleep onset and wakeup times, which are stored for further analysis.\n",
    "\n",
    "### 6. Aggregating and Saving Predictions\n",
    "Once predictions are made, they are grouped by series_id and step. These grouped predictions are aggregated to produce final outputs. The results are saved as .parquet files, organized by the train_or_test mode, ensuring easy retrieval and further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T10:31:28.936620Z",
     "iopub.status.busy": "2024-12-20T10:31:28.936338Z",
     "iopub.status.idle": "2024-12-20T10:31:30.958670Z",
     "shell.execute_reply": "2024-12-20T10:31:30.958010Z",
     "shell.execute_reply.started": "2024-12-20T10:31:28.936591Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if True:\n",
    "    sys.path.append(\"/kaggle/input/cmi-2023-src\")\n",
    "    from consts import ANGLEZ_MEAN, ANGLEZ_STD, ENMO_MEAN, ENMO_STD\n",
    "    from torch_models.dataset import ZzzPatchDataset\n",
    "    from torch_models.models import ZzzConv1dGRUModel, ZzzTransformerGRUModel, ZzzWaveGRUModel\n",
    "\n",
    "    from utils.feature_contena import Features\n",
    "    from utils.lightning_utils import MyLightningDataModule, MyLightningModule\n",
    "    from utils.set_seed import seed_base_torch\n",
    "    from utils.torch_template import EnsembleModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T10:31:30.960371Z",
     "iopub.status.busy": "2024-12-20T10:31:30.959481Z",
     "iopub.status.idle": "2024-12-20T10:31:30.977996Z",
     "shell.execute_reply": "2024-12-20T10:31:30.977130Z",
     "shell.execute_reply.started": "2024-12-20T10:31:30.960339Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def detection(paths=f\"/kaggle/input/child-mind-institute-problematic-internet-use/series_train.parquet/id=*/part-0.parquet\", train_or_test=\"train\"):\n",
    "    MODEL_NAME = \"patch_transformer_gru\"\n",
    "    \n",
    "    PACKAGE_DIR = Path(\"/kaggle/input/cmi-2023-src\")\n",
    "    CFG = yaml.safe_load(open(PACKAGE_DIR / \"config.yaml\", \"r\"))\n",
    "    BLOCK_SIZE = CFG[MODEL_NAME][\"execution\"][\"block_size\"]\n",
    "    \n",
    "    CFG[\"output_dir\"] = f\"/kaggle/input/cmi-2023-output/{CFG[MODEL_NAME]['execution']['best_exp_id']}\"\n",
    "    \n",
    "    seed_base_torch(CFG[\"env\"][\"seed\"])\n",
    "    \n",
    "    DEVICE = \"cuda\"\n",
    "    \n",
    "    files = glob(\n",
    "        paths\n",
    "    )\n",
    "    \n",
    "    features = Features()\n",
    "    features.add_num_features([\"anglez\", \"enmo\"])\n",
    "    features.add_num_features([\"anglez_diff\", \"enmo_diff\"])\n",
    "    features.add_num_features([\"same_count\"])\n",
    "    features.add_num_features([\"large_diff_count\"])\n",
    "    features.add_num_features([\"same_count_shift_plus\", \"same_count_shift_minus\"])\n",
    "    features.add_num_features([\"is_longest_sleep_episode\", \"is_sleep_block\"])\n",
    "    \n",
    "    # transformer + gru\n",
    "    model = ZzzTransformerGRUModel(\n",
    "        max_len=BLOCK_SIZE // CFG[MODEL_NAME][\"execution\"][\"patch_size\"],\n",
    "        input_numerical_size=len(features.all_features()) * CFG[MODEL_NAME][\"execution\"][\"patch_size\"],\n",
    "        **CFG[MODEL_NAME][\"params\"],\n",
    "    )\n",
    "    trn_models = [\n",
    "        MyLightningModule.load_from_checkpoint(\n",
    "            os.path.join(\"/kaggle/input/cmi-2023-output/exp_160\", f\"logs/best_model_fold{fold}.ckpt\"),\n",
    "            model=model,\n",
    "            map_location=torch.device(DEVICE),\n",
    "        ).to(DEVICE)\n",
    "        for fold in range(5 if len(files) > 100 else 1)\n",
    "    ]\n",
    "    \n",
    "    models = trn_models\n",
    "    model = EnsembleModel(models).to(DEVICE)\n",
    "    model.eval()\n",
    "    \n",
    "    all_oof_dfs = []\n",
    "    i = 0\n",
    "    for file in tqdm(files):\n",
    "        # load file\n",
    "        i += 1\n",
    "        if (i == MAX_FILE):\n",
    "            break\n",
    "        df = pd.read_parquet(file)\n",
    "        if len(df) < BLOCK_SIZE:\n",
    "            continue\n",
    "        time_of_days = df[\"time_of_day\"].values\n",
    "    \n",
    "        # same_count\n",
    "        DAY_STEPS = 12 * 60 * 24\n",
    "        n_days = int(len(df) // DAY_STEPS) + 1\n",
    "        df[\"same_count\"] = 0\n",
    "        for day in range(-n_days, n_days + 1):\n",
    "            if day == 0:\n",
    "                continue\n",
    "            df[\"_anglez_diff\"] = df[\"anglez\"].diff(DAY_STEPS * day)\n",
    "            df[\"_anglez_diff\"] = df[\"_anglez_diff\"].fillna(1)\n",
    "            df[\"same_count\"] += (df[\"_anglez_diff\"] == 0).astype(int)\n",
    "        df[\"same_count\"] = (df[\"same_count\"].clip(0, 5) - 2.5) / 2.5\n",
    "    \n",
    "        SHIFT_STEPS = 12 * 60 * 6  # 6h\n",
    "        df[\"same_count_shift_plus\"] = df[\"same_count\"].shift(SHIFT_STEPS).fillna(1.0).astype(np.float16)\n",
    "        df[\"same_count_shift_minus\"] = df[\"same_count\"].shift(-SHIFT_STEPS).fillna(1.0).astype(np.float16)\n",
    "    \n",
    "        # features\n",
    "        df[\"anglez_diffabs\"] = df[\"anglez\"].diff().abs().fillna(0)\n",
    "        df[\"large_diff\"] = (df[\"anglez_diffabs\"] > 5).astype(int)\n",
    "        df[\"large_diff_count\"] = df[\"large_diff\"].rolling(10, center=True).mean().fillna(0)\n",
    "        df[\"large_diff_count\"] = (df[\"large_diff_count\"] - 0.5) * 2\n",
    "    \n",
    "        # normalize\n",
    "        df[\"anglez\"] = (df[\"anglez\"] - ANGLEZ_MEAN) / ANGLEZ_STD\n",
    "        df[\"enmo\"] = (df[\"enmo\"] - ENMO_MEAN) / ENMO_STD\n",
    "        df[\"anglez_diff\"] = df[\"anglez\"].diff().fillna(0)\n",
    "        df[\"enmo_diff\"] = df[\"enmo\"].diff().fillna(0)\n",
    "    \n",
    "        # heuristic_features by @bilzard\n",
    "        sid = file.split(\"/\")[-2]\n",
    "        df[\"series_id\"] = sid\n",
    "        path = f\"/kaggle/working/heuristic_features/{train_or_test}/{sid}.parquet\"\n",
    "        hdf = pd.read_parquet(path)\n",
    "        df = pd.concat([df, hdf.drop(columns=[\"series_id\", \"step\"])], axis=1)\n",
    "        df[[\"is_longest_sleep_episode\", \"is_sleep_block\"]] = df[[\"is_longest_sleep_episode\", \"is_sleep_block\"]] * 2 - 1\n",
    "    \n",
    "        # split\n",
    "        dfs = []\n",
    "        df = df.sort_values(\"step\").reset_index(drop=True)\n",
    "        for start in range(0, len(df), BLOCK_SIZE // 8):\n",
    "            end = start + BLOCK_SIZE\n",
    "            if end > len(df):\n",
    "                end = len(df) - len(df) % CFG[MODEL_NAME][\"execution\"][\"patch_size\"]\n",
    "                start = end - BLOCK_SIZE\n",
    "                assert start >= 0\n",
    "            assert df.iloc[start][\"step\"] % CFG[MODEL_NAME][\"execution\"][\"patch_size\"] == 0\n",
    "            dfs.append(df.iloc[start:end])\n",
    "        gc.collect()\n",
    "    \n",
    "        # inference\n",
    "        train_dataset = ZzzPatchDataset(\n",
    "            dfs, mode=\"test\", features=features, patch_size=CFG[MODEL_NAME][\"execution\"][\"patch_size\"]\n",
    "        )\n",
    "        valid_dataset = ZzzPatchDataset(\n",
    "            dfs, mode=\"test\", features=features, patch_size=CFG[MODEL_NAME][\"execution\"][\"patch_size\"]\n",
    "        )\n",
    "        data_module = MyLightningDataModule(train_dataset, valid_dataset, batch_size=64)\n",
    "        preds = []\n",
    "        with torch.no_grad():\n",
    "            for X in data_module.val_dataloader():\n",
    "                pred = torch.sigmoid(model(X.to(\"cuda\"))).detach().cpu().numpy() * 10\n",
    "                preds.append(pred)\n",
    "    \n",
    "        oof_dfs = []\n",
    "        for pred, df in zip(np.vstack(preds), dfs):\n",
    "            df = df.iloc[\n",
    "                CFG[MODEL_NAME][\"execution\"][\"patch_size\"] // 2 : len(df) : CFG[MODEL_NAME][\"execution\"][\"patch_size\"]\n",
    "            ].reset_index(drop=True)\n",
    "            df[[\"wakeup_oof\", \"onset_oof\"]] = pred\n",
    "            oof_dfs.append(df[[\"series_id\", \"step\", \"wakeup_oof\", \"onset_oof\"]])\n",
    "    \n",
    "        oof_df = pd.concat(oof_dfs)\n",
    "        oof_df = oof_df.groupby([\"series_id\", \"step\"]).mean().reset_index().sort_values([\"series_id\", \"step\"])\n",
    "        oof_df = oof_df[[\"series_id\", \"step\", \"wakeup_oof\", \"onset_oof\"]]\n",
    "        oof_df[\"step\"] = oof_df[\"step\"].astype(int)\n",
    "    \n",
    "        del preds, oof_dfs\n",
    "        gc.collect()\n",
    "    \n",
    "        train = oof_df.reset_index(drop=True)\n",
    "        train[\"time_of_day\"] = time_of_days[\n",
    "            CFG[MODEL_NAME][\"execution\"][\"patch_size\"] // 2 :: CFG[MODEL_NAME][\"execution\"][\"patch_size\"]\n",
    "        ][: len(train)]\n",
    "        all_oof_dfs.append(train[[\"series_id\", \"step\", \"wakeup_oof\", \"onset_oof\", \"time_of_day\"]])\n",
    "        # del dfs, df\n",
    "        gc.collect()\n",
    "\n",
    "    # save\n",
    "    for df in tqdm(all_oof_dfs):\n",
    "        save_path = f\"/kaggle/working/features/sleep_detection/{train_or_test}/{df['series_id'].iloc[0]}.parquet\"\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        df.to_parquet(save_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function processes sleep detection data by extracting and aggregating meaningful features from sensor and time-series datasets. It reads multiple parquet files containing step and sensor data, filters for valid time intervals, and interpolates missing data points. The function identifies sleep onset and wakeup periods, calculates related metrics (e.g., sleep duration, activity levels, and light intensity), and stores these in a structured (csv in working space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T10:31:30.999927Z",
     "iopub.status.busy": "2024-12-20T10:31:30.999637Z",
     "iopub.status.idle": "2024-12-20T10:31:31.014795Z",
     "shell.execute_reply": "2024-12-20T10:31:31.014027Z",
     "shell.execute_reply.started": "2024-12-20T10:31:30.999899Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "time_of_day_max = 86400000000000\n",
    "# all_files = sorted(glob(\"/kaggle/working/features/sleep_detection/*.parquet\"))\n",
    "# len(all_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T10:31:31.015735Z",
     "iopub.status.busy": "2024-12-20T10:31:31.015512Z",
     "iopub.status.idle": "2024-12-20T10:31:31.033866Z",
     "shell.execute_reply": "2024-12-20T10:31:31.033167Z",
     "shell.execute_reply.started": "2024-12-20T10:31:31.015716Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def feature_engineering(paths=\"/kaggle/working/features/sleep_detection/train/*.parquet\", data_paths=\"/kaggle/input/child-mind-institute-problematic-internet-use/series_train.parquet\", train_or_test=\"train\"):\n",
    "    features = []\n",
    "    debug_count = 0\n",
    "    all_files = sorted(glob(paths))\n",
    "    i = 0\n",
    "    for file in tqdm(all_files):\n",
    "        i += 1\n",
    "        if (i == MAX_FILE):\n",
    "            break\n",
    "        df = pl.read_parquet(file)\n",
    "        df = df.with_columns(pl.col(\"step\").cast(pl.UInt32)).drop(\"time_of_day\")\n",
    "        sid = df[\"series_id\"][0]\n",
    "    \n",
    "        sensor_df = pl.read_parquet(\n",
    "            f\"{data_paths}/{sid}/part-0.parquet\"\n",
    "        ).with_columns((pl.col(\"time_of_day\") == 0).cum_sum().alias(\"day\"))\n",
    "    \n",
    "        feature = {\n",
    "            \"id\": sid,\n",
    "            \"length\": df.shape[0],\n",
    "            \"day\": sensor_df[\"relative_date_PCIAT\"].max() - sensor_df[\"relative_date_PCIAT\"].min(),\n",
    "        }\n",
    "    \n",
    "        # skip if time step is not 5sec\n",
    "        diffs = sensor_df[\"time_of_day\"].diff().drop_nulls().unique()\n",
    "        if set(diffs) != set([-86395000000000, 5000000000]):\n",
    "            features.append(feature)\n",
    "            continue\n",
    "    \n",
    "        sensor_df = (\n",
    "            sensor_df.join(df, on=\"step\", how=\"left\")\n",
    "            .sort(\"step\")\n",
    "            .with_columns(\n",
    "                pl.col(\"onset_oof\").interpolate(),\n",
    "                pl.col(\"wakeup_oof\").interpolate(),\n",
    "            )\n",
    "        )\n",
    "    \n",
    "        # onset = 15:00~3:00, wakeup = 3:00~15:00\n",
    "        onset_start = time_of_day_max / 24 * 15  # 15:00\n",
    "        onset_end = time_of_day_max / 24 * 3  # 3:00\n",
    "        sensor_df = sensor_df.with_columns(\n",
    "            ((pl.col(\"time_of_day\") > onset_start) | (pl.col(\"time_of_day\") < onset_end)).alias(\"onset_duration\"),\n",
    "        ).with_columns(\n",
    "            pl.col(\"onset_duration\").cast(pl.Int32).diff().fill_null(0).abs().cum_sum().alias(\"onset_wakeup_duration\")\n",
    "        )\n",
    "    \n",
    "        # get sleep period\n",
    "        sleep_info = []\n",
    "        for _, df in sensor_df.group_by(\"onset_wakeup_duration\", maintain_order=True):\n",
    "            is_onset = df[\"onset_duration\"][0]\n",
    "            if is_onset:\n",
    "                max_idx = df[\"onset_oof\"].arg_max()\n",
    "                if max_idx is None:\n",
    "                    continue\n",
    "                max_score = df[\"onset_oof\"][max_idx]\n",
    "                step = df[\"step\"][max_idx]\n",
    "    \n",
    "                # date\n",
    "                start_time = df[\"time_of_day\"][0] / time_of_day_max * 24\n",
    "                if start_time >= 15:\n",
    "                    day = df[\"day\"][0]\n",
    "                    week_day = df[\"weekday\"][0]\n",
    "                else:\n",
    "                    day = df[\"day\"][0] - 1\n",
    "                    week_day = df[\"weekday\"][0] - 1\n",
    "                    if week_day == 0:\n",
    "                        week_day = 7\n",
    "            else:\n",
    "                max_idx = df[\"wakeup_oof\"].arg_max()\n",
    "                if max_idx is None:\n",
    "                    continue\n",
    "                max_score = df[\"wakeup_oof\"][max_idx]\n",
    "                step = df[\"step\"][max_idx]\n",
    "    \n",
    "                # date\n",
    "                start_time = df[\"time_of_day\"][0] / time_of_day_max * 24\n",
    "                day = df[\"day\"][0] - 1\n",
    "                week_day = df[\"weekday\"][0] - 1\n",
    "    \n",
    "            info = {\n",
    "                \"day\": day,\n",
    "                \"weekday\": week_day,\n",
    "                \"type\": \"onset\" if is_onset else \"wakeup\",\n",
    "                \"step\": step,\n",
    "                \"max_score\": max_score,\n",
    "                \"time\": df[\"time_of_day\"][max_idx] / time_of_day_max * 24,\n",
    "            }\n",
    "            sleep_info.append(info)\n",
    "        sleep_df = pl.DataFrame(sleep_info)\n",
    "    \n",
    "        # merge\n",
    "        sleep_df = (\n",
    "            sleep_df.filter(pl.col(\"type\") == \"onset\")\n",
    "            .drop(\"type\")\n",
    "            .rename(\n",
    "                {\n",
    "                    \"max_score\": \"onset_score\",\n",
    "                    \"step\": \"onset_step\",\n",
    "                    \"time\": \"onset_time\",\n",
    "                }\n",
    "            )\n",
    "            .join(\n",
    "                sleep_df.filter(pl.col(\"type\") == \"wakeup\")\n",
    "                .drop([\"type\", \"weekday\"])\n",
    "                .rename(\n",
    "                    {\n",
    "                        \"max_score\": \"wakeup_score\",\n",
    "                        \"step\": \"wakeup_step\",\n",
    "                        \"time\": \"wakeup_time\",\n",
    "                    }\n",
    "                ),\n",
    "                on=\"day\",\n",
    "            )\n",
    "        ).select(\n",
    "            [\"day\", \"weekday\", \"onset_time\", \"wakeup_time\", \"onset_step\", \"wakeup_step\", \"onset_score\", \"wakeup_score\"]\n",
    "        )\n",
    "    \n",
    "        # feature engineering\n",
    "        sleep_lengths = []  # wakeup - onset\n",
    "        sleep_enmo_mean = []  \n",
    "        sleep_enmo_std = []  \n",
    "        sleep_light_mean = []\n",
    "        sleep_light_std = [] \n",
    "        for i in range(len(sleep_df)):\n",
    "            # sleep period\n",
    "            start = sleep_df[\"onset_step\"][i]\n",
    "            end = sleep_df[\"wakeup_step\"][i]\n",
    "            if sleep_df[\"onset_score\"][i] < 1 or sleep_df[\"wakeup_score\"][i] < 1:\n",
    "                sleep_lengths.append(np.nan)\n",
    "                sleep_enmo_mean.append(np.nan)\n",
    "                sleep_enmo_std.append(np.nan)\n",
    "                sleep_light_mean.append(np.nan)\n",
    "                sleep_light_std.append(np.nan)\n",
    "                continue\n",
    "    \n",
    "            # sleep length\n",
    "            length = end - start\n",
    "            sleep_lengths.append(length * 5 / 60 / 60)  # hour\n",
    "    \n",
    "            # enmo\n",
    "            enmo_mean = sensor_df[\"enmo\"][start:end].mean()\n",
    "            enmo_std = sensor_df[\"enmo\"][start:end].std()\n",
    "            sleep_enmo_mean.append(enmo_mean)\n",
    "            sleep_enmo_std.append(enmo_std)\n",
    "    \n",
    "            # light\n",
    "            light_mean = sensor_df[\"light\"][start:end].mean()\n",
    "            light_std = sensor_df[\"light\"][start:end].std()\n",
    "            sleep_light_mean.append(light_mean)\n",
    "            sleep_light_std.append(light_std)\n",
    "            \n",
    "        sleep_df = sleep_df.with_columns(\n",
    "            pl.DataFrame(\n",
    "                {\n",
    "                    \"sleep_length\": sleep_lengths,\n",
    "                    \"sleep_enmo_mean\": sleep_enmo_mean,\n",
    "                    \"sleep_enmo_std\": sleep_enmo_std,\n",
    "                    \"sleep_light_mean\": sleep_light_mean,\n",
    "                    \"sleep_light_std\": sleep_light_std,\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # leave only high confidence periods\n",
    "        sleep_df = sleep_df.filter((pl.col(\"wakeup_score\") > 1) & (pl.col(\"onset_score\") > 1))\n",
    "        if debug_count < 3:\n",
    "            display(sleep_df.head())\n",
    "        debug_count += 1\n",
    "            \n",
    "    \n",
    "        # agg\n",
    "        feature.update(\n",
    "            {\n",
    "                \"sleep_measurement_count\": sleep_df.shape[0],\n",
    "                \"sleep_length_mean\": sleep_df[\"sleep_length\"].mean(),\n",
    "                \"sleep_length_std\": sleep_df[\"sleep_length\"].std(),\n",
    "                \"sleep_start_mean\": sleep_df[\"onset_time\"].mean(),\n",
    "                \"sleep_start_std\": sleep_df[\"onset_time\"].std(),\n",
    "                \"sleep_end_mean\": sleep_df[\"wakeup_time\"].mean(),\n",
    "                \"sleep_end_std\": sleep_df[\"wakeup_time\"].std(),\n",
    "                \"sleep_enmo_mean_mean\": sleep_df[\"sleep_enmo_mean\"].mean(),\n",
    "                \"sleep_enmo_mean_std\": sleep_df[\"sleep_enmo_mean\"].std(),\n",
    "                \"sleep_enmo_std_mean\": sleep_df[\"sleep_enmo_std\"].mean(),\n",
    "                \"sleep_enmo_std_std\": sleep_df[\"sleep_enmo_std\"].std(),\n",
    "                \"sleep_light_mean_mean\": sleep_df[\"sleep_light_mean\"].mean(),\n",
    "                \"sleep_light_mean_std\": sleep_df[\"sleep_light_mean\"].std(),\n",
    "                \"sleep_light_std_mean\": sleep_df[\"sleep_light_std\"].mean(),\n",
    "                \"sleep_light_std_std\": sleep_df[\"sleep_light_std\"].std(),\n",
    "            }\n",
    "        )\n",
    "        features.append(feature)\n",
    "    output_dir = f\"/kaggle/working/features/{train_or_test}\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    feature_df = pl.DataFrame(features).with_columns(pl.col(\"id\").str.slice(3, 8))\n",
    "    feature_df.write_csv(f\"/kaggle/working/features/{train_or_test}/sleep_features.csv\")\n",
    "    print(feature_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T10:31:31.034707Z",
     "iopub.status.busy": "2024-12-20T10:31:31.034513Z",
     "iopub.status.idle": "2024-12-20T10:31:37.761671Z",
     "shell.execute_reply": "2024-12-20T10:31:37.760792Z",
     "shell.execute_reply.started": "2024-12-20T10:31:31.034690Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  3.25it/s]\n",
      "100%|██████████| 2/2 [00:04<00:00,  2.49s/it]\n",
      "100%|██████████| 2/2 [00:00<00:00, 66.05it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 13)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>day</th><th>weekday</th><th>onset_time</th><th>wakeup_time</th><th>onset_step</th><th>wakeup_step</th><th>onset_score</th><th>wakeup_score</th><th>sleep_length</th><th>sleep_enmo_mean</th><th>sleep_enmo_std</th><th>sleep_light_mean</th><th>sleep_light_std</th></tr><tr><td>i64</td><td>i64</td><td>f64</td><td>f64</td><td>i64</td><td>i64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>0</td><td>2</td><td>22.091667</td><td>7.041667</td><td>7854</td><td>14298</td><td>5.939998</td><td>6.883044</td><td>8.95</td><td>0.003588</td><td>0.008487</td><td>2.06153</td><td>0.52031</td></tr><tr><td>1</td><td>3</td><td>22.641667</td><td>8.141667</td><td>25530</td><td>32370</td><td>7.395478</td><td>2.857819</td><td>9.5</td><td>0.002427</td><td>0.006132</td><td>2.714605</td><td>1.259364</td></tr><tr><td>2</td><td>4</td><td>21.575</td><td>7.558333</td><td>42042</td><td>49230</td><td>5.715631</td><td>6.638568</td><td>9.983333</td><td>0.003959</td><td>0.007149</td><td>6.441472</td><td>2.73119</td></tr><tr><td>3</td><td>5</td><td>23.141667</td><td>8.241667</td><td>60450</td><td>67002</td><td>8.010484</td><td>3.987538</td><td>9.1</td><td>0.006016</td><td>0.007928</td><td>9.246452</td><td>14.259801</td></tr><tr><td>4</td><td>6</td><td>22.925</td><td>7.008333</td><td>77574</td><td>83394</td><td>8.050978</td><td>2.848315</td><td>8.083333</td><td>0.009862</td><td>0.012524</td><td>0.511077</td><td>0.28813</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 13)\n",
       "┌─────┬─────────┬────────────┬─────────────┬───┬────────────┬────────────┬────────────┬────────────┐\n",
       "│ day ┆ weekday ┆ onset_time ┆ wakeup_time ┆ … ┆ sleep_enmo ┆ sleep_enmo ┆ sleep_ligh ┆ sleep_ligh │\n",
       "│ --- ┆ ---     ┆ ---        ┆ ---         ┆   ┆ _mean      ┆ _std       ┆ t_mean     ┆ t_std      │\n",
       "│ i64 ┆ i64     ┆ f64        ┆ f64         ┆   ┆ ---        ┆ ---        ┆ ---        ┆ ---        │\n",
       "│     ┆         ┆            ┆             ┆   ┆ f64        ┆ f64        ┆ f64        ┆ f64        │\n",
       "╞═════╪═════════╪════════════╪═════════════╪═══╪════════════╪════════════╪════════════╪════════════╡\n",
       "│ 0   ┆ 2       ┆ 22.091667  ┆ 7.041667    ┆ … ┆ 0.003588   ┆ 0.008487   ┆ 2.06153    ┆ 0.52031    │\n",
       "│ 1   ┆ 3       ┆ 22.641667  ┆ 8.141667    ┆ … ┆ 0.002427   ┆ 0.006132   ┆ 2.714605   ┆ 1.259364   │\n",
       "│ 2   ┆ 4       ┆ 21.575     ┆ 7.558333    ┆ … ┆ 0.003959   ┆ 0.007149   ┆ 6.441472   ┆ 2.73119    │\n",
       "│ 3   ┆ 5       ┆ 23.141667  ┆ 8.241667    ┆ … ┆ 0.006016   ┆ 0.007928   ┆ 9.246452   ┆ 14.259801  │\n",
       "│ 4   ┆ 6       ┆ 22.925     ┆ 7.008333    ┆ … ┆ 0.009862   ┆ 0.012524   ┆ 0.511077   ┆ 0.28813    │\n",
       "└─────┴─────────┴────────────┴─────────────┴───┴────────────┴────────────┴────────────┴────────────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 17.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (2, 18)\n",
      "┌──────────┬────────┬──────┬─────────────┬───┬─────────────┬─────────────┬────────────┬────────────┐\n",
      "│ id       ┆ length ┆ day  ┆ sleep_measu ┆ … ┆ sleep_light ┆ sleep_light ┆ sleep_ligh ┆ sleep_ligh │\n",
      "│ ---      ┆ ---    ┆ ---  ┆ rement_coun ┆   ┆ _mean_mean  ┆ _mean_std   ┆ t_std_mean ┆ t_std_std  │\n",
      "│ str      ┆ i64    ┆ f64  ┆ t           ┆   ┆ ---         ┆ ---         ┆ ---        ┆ ---        │\n",
      "│          ┆        ┆      ┆ ---         ┆   ┆ f64         ┆ f64         ┆ f64        ┆ f64        │\n",
      "│          ┆        ┆      ┆ i64         ┆   ┆             ┆             ┆            ┆            │\n",
      "╞══════════╪════════╪══════╪═════════════╪═══╪═════════════╪═════════════╪════════════╪════════════╡\n",
      "│ 00115b9f ┆ 3610   ┆ 44.0 ┆ null        ┆ … ┆ null        ┆ null        ┆ null       ┆ null       │\n",
      "│ 001f3379 ┆ 33033  ┆ 23.0 ┆ 7           ┆ … ┆ 4.917133    ┆ 4.370878    ┆ 3.281733   ┆ 4.990464   │\n",
      "└──────────┴────────┴──────┴─────────────┴───┴─────────────┴─────────────┴────────────┴────────────┘\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "create_heuristic(paths=glob(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_test.parquet/id=*/part-0.parquet\"), train_or_test=\"test\")\n",
    "detection(paths=\"/kaggle/input/child-mind-institute-problematic-internet-use/series_test.parquet/id=*/part-0.parquet\", train_or_test=\"test\")\n",
    "feature_engineering(paths=\"/kaggle/working/features/sleep_detection/test/*.parquet\", data_paths=\"/kaggle/input/child-mind-institute-problematic-internet-use/series_test.parquet\", train_or_test=\"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T10:31:37.762617Z",
     "iopub.status.busy": "2024-12-20T10:31:37.762403Z",
     "iopub.status.idle": "2024-12-20T10:31:37.769206Z",
     "shell.execute_reply": "2024-12-20T10:31:37.768488Z",
     "shell.execute_reply.started": "2024-12-20T10:31:37.762591Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import copy\n",
    "import pickle\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from scipy.optimize import minimize\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "import polars as pl\n",
    "import polars.selectors as cs\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator, FormatStrFormatter, PercentFormatter\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.optimizers import Adam\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from colorama import Fore, Style\n",
    "from IPython.display import clear_output\n",
    "import warnings\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.ensemble import VotingRegressor, RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.options.display.max_columns = None\n",
    "SEED = 42\n",
    "n_splits = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T10:46:06.542546Z",
     "iopub.status.busy": "2024-12-20T10:46:06.542220Z",
     "iopub.status.idle": "2024-12-20T10:46:06.551987Z",
     "shell.execute_reply": "2024-12-20T10:46:06.550987Z",
     "shell.execute_reply.started": "2024-12-20T10:46:06.542510Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def TrainML_sub2(model_class, X, y, test_data):\n",
    "    SKF = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
    "    train_S = []\n",
    "    test_S = []\n",
    "    \n",
    "    oof_non_rounded = np.zeros(len(y), dtype=float) \n",
    "    oof_rounded = np.zeros(len(y), dtype=int) \n",
    "    test_preds = np.zeros((len(test_data), n_splits))\n",
    "    \n",
    "    X[np.isinf(X)] = 0.0\n",
    "    test_data[np.isinf(test_data)] = 0.0\n",
    "    \n",
    "    for fold, (train_idx, test_idx) in enumerate(tqdm(SKF.split(X, y), desc=\"Training Folds\", total=n_splits)):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[test_idx]\n",
    "        model = clone(model_class)\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_val_pred = model.predict(X_val)\n",
    "\n",
    "        oof_non_rounded[test_idx] = y_val_pred\n",
    "        y_val_pred_rounded = y_val_pred.round(0).astype(int)\n",
    "        oof_rounded[test_idx] = y_val_pred_rounded\n",
    "\n",
    "        train_kappa = quadratic_weighted_kappa(y_train, y_train_pred.round(0).astype(int))\n",
    "        val_kappa = quadratic_weighted_kappa(y_val, y_val_pred_rounded)\n",
    "\n",
    "        train_S.append(train_kappa)\n",
    "        test_S.append(val_kappa)\n",
    "        \n",
    "        test_preds[:, fold] = model.predict(test_data)\n",
    "        \n",
    "        print(f\"Fold {fold+1} - Train QWK: {train_kappa:.4f}, Validation QWK: {val_kappa:.4f}\")\n",
    "        clear_output(wait=True)\n",
    "    print(f\"Mean Train QWK --> {np.mean(train_S):.4f}\")\n",
    "    print(f\"Mean Validation QWK ---> {np.mean(test_S):.4f}\")\n",
    "\n",
    "    KappaOPtimizer = minimize(evaluate_predictions,\n",
    "                              x0=[0.5, 1.5, 2.5], args=(y, oof_non_rounded), \n",
    "                              method='Nelder-Mead')\n",
    "    assert KappaOPtimizer.success, \"Optimization did not converge.\"\n",
    "    print('OPTIMIZED THRESHOLDS', KappaOPtimizer.x)\n",
    "    oof_tuned = threshold_Rounder(oof_non_rounded, KappaOPtimizer.x)\n",
    "    tKappa = quadratic_weighted_kappa(y, oof_tuned)\n",
    "\n",
    "    print(f\"----> || Optimized QWK SCORE :: {Fore.CYAN}{Style.BRIGHT} {tKappa:.3f}{Style.RESET_ALL}\")\n",
    "\n",
    "    tpm = test_preds.mean(axis=1)\n",
    "    tpTuned = threshold_Rounder(tpm, KappaOPtimizer.x)\n",
    "    \n",
    "    submission = pd.DataFrame({\n",
    "        'id': sample['id'],\n",
    "        'sii': tpTuned\n",
    "    })\n",
    "    optimized_thresholds = KappaOPtimizer.x\n",
    "    return submission, oof_tuned, oof_non_rounded, y, optimized_thresholds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-12-20T10:36:49.070011Z",
     "iopub.status.busy": "2024-12-20T10:36:49.069800Z",
     "iopub.status.idle": "2024-12-20T10:36:49.109305Z",
     "shell.execute_reply": "2024-12-20T10:36:49.108137Z",
     "shell.execute_reply.started": "2024-12-20T10:36:49.069994Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/kaggle/working/features/test/sleep_features.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-83e618667001>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain_sleep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/kaggle/input/sleep-detection/sleep_features.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtest_sleep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/kaggle/working/features/test/sleep_features.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    946\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1447\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1448\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1706\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    861\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    864\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/working/features/test/sleep_features.csv'"
     ]
    }
   ],
   "source": [
    "train_sleep = pd.read_csv(\"/kaggle/input/sleep-detection/sleep_features.csv\")\n",
    "test_sleep = pd.read_csv(\"/kaggle/working/features/test/sleep_features.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-20T10:36:49.109884Z",
     "iopub.status.idle": "2024-12-20T10:36:49.110170Z",
     "shell.execute_reply": "2024-12-20T10:36:49.110059Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sleep_cols = train_sleep.columns.tolist()\n",
    "sleep_cols.remove(\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T10:36:53.058455Z",
     "iopub.status.busy": "2024-12-20T10:36:53.058157Z",
     "iopub.status.idle": "2024-12-20T10:36:53.226234Z",
     "shell.execute_reply": "2024-12-20T10:36:53.225279Z",
     "shell.execute_reply.started": "2024-12-20T10:36:53.058431Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "rm -rf /kaggle/working/features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T10:36:53.228004Z",
     "iopub.status.busy": "2024-12-20T10:36:53.227763Z",
     "iopub.status.idle": "2024-12-20T10:36:53.391704Z",
     "shell.execute_reply": "2024-12-20T10:36:53.390568Z",
     "shell.execute_reply.started": "2024-12-20T10:36:53.227981Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "rm -rf /kaggle/working/heuristic_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T10:36:53.393600Z",
     "iopub.status.busy": "2024-12-20T10:36:53.393354Z",
     "iopub.status.idle": "2024-12-20T10:36:53.399894Z",
     "shell.execute_reply": "2024-12-20T10:36:53.399112Z",
     "shell.execute_reply.started": "2024-12-20T10:36:53.393577Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def feature_engineering(df):\n",
    "    season_cols = [col for col in df.columns if 'Season' in col]\n",
    "    df = df.drop(season_cols, axis=1) \n",
    "    df[\"Feat_0\"] = df[\"Physical-Height\"] * df[\"PAQ_C-PAQ_C_Total\"]\n",
    "    df[\"Feat_1\"] = df[\"FGC-FGC_TL_Zone\"] * df[\"Physical-Height\"]\n",
    "    df[\"Feat_2\"] = df[\"PreInt_EduHx-computerinternet_hoursday\"] * df[\"BIA-BIA_Activity_Level_num\"]\n",
    "    df[\"Feat_3\"] = df[\"Fitness_Endurance-Time_Sec\"] / df[\"PreInt_EduHx-computerinternet_hoursday\"]\n",
    "    df[\"Feat_4\"] = df[\"CGAS-CGAS_Score\"] / df[\"FGC-FGC_CU_Zone\"]\n",
    "    df[\"Feat_5\"] = df[\"Basic_Demos-Age\"] / df[\"FGC-FGC_SRR_Zone\"]\n",
    "    df[\"Feat_7\"] = df[\"PAQ_C-PAQ_C_Total\"] * df[\"BIA-BIA_Frame_num\"]\n",
    "    df[\"Feat_9\"] = df[\"FGC-FGC_GSD\"] / df[\"SDS-SDS_Total_Raw\"]\n",
    "    df[\"Feat_10\"] = df[\"PAQ_A-PAQ_A_Total\"] / df[\"PreInt_EduHx-computerinternet_hoursday\"]\n",
    "    df[\"Feat_11\"] = df[\"BIA-BIA_LDM\"] / df[\"PreInt_EduHx-computerinternet_hoursday\"]\n",
    "    df[\"Feat_14\"] = df[\"BIA-BIA_BMI\"] / df[\"SDS-SDS_Total_Raw\"]\n",
    "    df[\"Feat_15\"] = df[\"Physical-Height\"] * df[\"SDS-SDS_Total_T\"]\n",
    "    df[\"Feat_16\"] = df[\"Physical-Height\"] * df[\"Physical-Height\"]\n",
    "    df[\"Feat_17\"] = df[\"FGC-FGC_SRL_Zone\"] / df[\"Physical-Weight\"]\n",
    "    df[\"Feat_18\"] = df[\"Basic_Demos-Sex\"] * df[\"Basic_Demos-Sex\"]\n",
    "    df[\"Feat_19\"] = df[\"FGC-FGC_GSND_Zone\"] / df[\"BIA-BIA_Fat\"]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T10:33:42.086473Z",
     "iopub.status.busy": "2024-12-20T10:33:42.086166Z",
     "iopub.status.idle": "2024-12-20T10:33:42.091096Z",
     "shell.execute_reply": "2024-12-20T10:33:42.090099Z",
     "shell.execute_reply.started": "2024-12-20T10:33:42.086449Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "noseason_features = ['Basic_Demos-Age', 'Basic_Demos-Sex',\n",
    "                'CGAS-CGAS_Score', 'Physical-BMI',\n",
    "                'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference',\n",
    "                'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP',\n",
    "                'Fitness_Endurance-Max_Stage',\n",
    "                'Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec',\n",
    "                'FGC-FGC_CU', 'FGC-FGC_CU_Zone', 'FGC-FGC_GSND',\n",
    "                'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone', 'FGC-FGC_PU',\n",
    "                'FGC-FGC_PU_Zone', 'FGC-FGC_SRL', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR',\n",
    "                'FGC-FGC_SRR_Zone', 'FGC-FGC_TL', 'FGC-FGC_TL_Zone',\n",
    "                'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC', 'BIA-BIA_BMI',\n",
    "                'BIA-BIA_BMR', 'BIA-BIA_DEE', 'BIA-BIA_ECW', 'BIA-BIA_FFM',\n",
    "                'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_Fat', 'BIA-BIA_Frame_num',\n",
    "                'BIA-BIA_ICW', 'BIA-BIA_LDM', 'BIA-BIA_LST', 'BIA-BIA_SMM',\n",
    "                'BIA-BIA_TBW', 'PAQ_A-PAQ_A_Total',\n",
    "                'PAQ_C-PAQ_C_Total', 'SDS-SDS_Total_Raw',\n",
    "                'SDS-SDS_Total_T',\n",
    "                'PreInt_EduHx-computerinternet_hoursday', 'Feat_0', 'Feat_1', 'Feat_2', 'Feat_3', 'Feat_4', 'Feat_5',\n",
    "        'Feat_7','Feat_9', 'Feat_10', 'Feat_11', 'Feat_14', 'Feat_15',\n",
    "       'Feat_16', 'Feat_17', 'Feat_18', 'Feat_19']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T10:32:57.696891Z",
     "iopub.status.busy": "2024-12-20T10:32:57.696570Z",
     "iopub.status.idle": "2024-12-20T10:32:57.701801Z",
     "shell.execute_reply": "2024-12-20T10:32:57.700922Z",
     "shell.execute_reply.started": "2024-12-20T10:32:57.696863Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Model parameters for LightGBM\n",
    "Params = {\n",
    "    'learning_rate': 0.046,\n",
    "    'max_depth': 12,\n",
    "    'num_leaves': 478,\n",
    "    'min_data_in_leaf': 13,\n",
    "    'feature_fraction': 0.893,\n",
    "    'bagging_fraction': 0.784,\n",
    "    'bagging_freq': 4,\n",
    "    'lambda_l1': 10,  # Increased from 6.59\n",
    "    'lambda_l2': 0.01,  # Increased from 2.68e-06\n",
    "    'device': 'cpu'\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "# XGBoost parameters\n",
    "XGB_Params = {\n",
    "    'learning_rate': 0.05,\n",
    "    'max_depth': 6,\n",
    "    'n_estimators': 200,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'reg_alpha': 1,  # Increased from 0.1\n",
    "    'reg_lambda': 5,  # Increased from 1\n",
    "    'random_state': SEED,\n",
    "    'tree_method': 'gpu_hist',\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "CatBoost_Params = {\n",
    "    'learning_rate': 0.05,\n",
    "    'depth': 6,\n",
    "    'iterations': 200,\n",
    "    'random_seed': SEED,\n",
    "    'verbose': 0,\n",
    "    'l2_leaf_reg': 10,  # Increase this value\n",
    "    'task_type': 'GPU'\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T10:33:00.763133Z",
     "iopub.status.busy": "2024-12-20T10:33:00.762865Z",
     "iopub.status.idle": "2024-12-20T10:33:00.767734Z",
     "shell.execute_reply": "2024-12-20T10:33:00.766885Z",
     "shell.execute_reply.started": "2024-12-20T10:33:00.763113Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create model instances\n",
    "Light = LGBMRegressor(**Params, random_state=SEED, verbose=-1, n_estimators=300)\n",
    "XGB_Model = XGBRegressor(**XGB_Params)\n",
    "CatBoost_Model = CatBoostRegressor(**CatBoost_Params)\n",
    "# TabNet_Model = TabNetWrapper(**TabNet_Params) \n",
    "voting_model = VotingRegressor(estimators=[\n",
    "    ('lightgbm', Light),\n",
    "    ('xgboost', XGB_Model),\n",
    "    ('catboost', CatBoost_Model),\n",
    "    # ('tabnet', TabNet_Model)\n",
    "],weights=[4.0,4.0,5.0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T10:40:22.921823Z",
     "iopub.status.busy": "2024-12-20T10:40:22.921494Z",
     "iopub.status.idle": "2024-12-20T10:40:22.951349Z",
     "shell.execute_reply": "2024-12-20T10:40:22.950693Z",
     "shell.execute_reply.started": "2024-12-20T10:40:22.921795Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_sub2 = pd.merge(train, train_sleep, how=\"left\", on='id')\n",
    "test_sub2 = pd.merge(test, test_sleep, how=\"left\", on='id')\n",
    "\n",
    "# imputer = KNNImputer(n_neighbors=5)\n",
    "# numeric_cols = train.select_dtypes(include=['float64', 'int64']).columns\n",
    "# imputed_data = imputer.fit_transform(train_sub2[numeric_cols])\n",
    "# train_imputed = pd.DataFrame(imputed_data, columns=numeric_cols)\n",
    "# train_imputed['sii'] = train_imputed['sii'].round().astype(int)\n",
    "\n",
    "# for col in train_sub2.columns:\n",
    "#     if col not in numeric_cols:\n",
    "#         train_imputed[col] = train_sub2[col]\n",
    "        \n",
    "# train_sub2 = train_imputed\n",
    "\n",
    "train_sub2 = feature_engineering(train_sub2)\n",
    "train_sub2 = train_sub2.dropna(subset='sii', ignore_index=True)\n",
    "test_sub2 = feature_engineering(test_sub2)\n",
    "\n",
    "train_sub2 = train_sub2.drop('id', axis=1)\n",
    "test_sub2  = test_sub2.drop('id', axis=1)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T10:40:23.410663Z",
     "iopub.status.busy": "2024-12-20T10:40:23.410427Z",
     "iopub.status.idle": "2024-12-20T10:40:23.422932Z",
     "shell.execute_reply": "2024-12-20T10:40:23.422149Z",
     "shell.execute_reply.started": "2024-12-20T10:40:23.410641Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "features_sub2 = noseason_features + sleep_cols\n",
    "\n",
    "# train_sub2 = pd.merge(train, train_ts, how=\"left\", on='id')\n",
    "# test_sub2 = pd.merge(test, test_ts, how=\"left\", on='id')\n",
    "\n",
    "train_sub2 = train_sub2.dropna(subset='sii')\n",
    "if np.any(np.isinf(train_sub2)):\n",
    "    train_sub2 = train_sub2.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "X_sub2 = train_sub2[features_sub2]\n",
    "y_sub2 = train_sub2['sii']\n",
    "test_sub2 = test_sub2[features_sub2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-20T10:46:11.165768Z",
     "iopub.status.busy": "2024-12-20T10:46:11.165454Z",
     "iopub.status.idle": "2024-12-20T10:46:28.069631Z",
     "shell.execute_reply": "2024-12-20T10:46:28.068725Z",
     "shell.execute_reply.started": "2024-12-20T10:46:11.165740Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Folds: 100%|██████████| 5/5 [00:16<00:00,  3.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Train QWK --> 0.7458\n",
      "Mean Validation QWK ---> 0.4002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPTIMIZED THRESHOLDS [0.5250443  1.02169925 2.83555884]\n",
      "----> || Optimized QWK SCORE :: \u001b[36m\u001b[1m 0.450\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "submission2, _, _, _, _= TrainML_sub2(voting_model, X_sub2, y_sub2, test_sub2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission 6: MAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-20T10:31:38.213446Z",
     "iopub.status.idle": "2024-12-20T10:31:38.213767Z",
     "shell.execute_reply": "2024-12-20T10:31:38.213618Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_ts = load_time_series(\"../input/child-mind-institute-problematic-internet-use/series_train.parquet\")\n",
    "test_ts = load_time_series(\"../input/child-mind-institute-problematic-internet-use/series_test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-20T10:31:38.214679Z",
     "iopub.status.idle": "2024-12-20T10:31:38.215011Z",
     "shell.execute_reply": "2024-12-20T10:31:38.214903Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/child-mind-institute-problematic-internet-use/train.csv')\n",
    "test = pd.read_csv('../input/child-mind-institute-problematic-internet-use/test.csv')\n",
    "sample = pd.read_csv('../input/child-mind-institute-problematic-internet-use/sample_submission.csv')\n",
    "\n",
    "\n",
    "featuresCols = ['Basic_Demos-Enroll_Season', 'Basic_Demos-Age', 'Basic_Demos-Sex',\n",
    "                'CGAS-Season', 'CGAS-CGAS_Score', 'Physical-Season', 'Physical-BMI',\n",
    "                'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference',\n",
    "                'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP',\n",
    "                'Fitness_Endurance-Season', 'Fitness_Endurance-Max_Stage',\n",
    "                'Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec',\n",
    "                'FGC-Season', 'FGC-FGC_CU', 'FGC-FGC_CU_Zone', 'FGC-FGC_GSND',\n",
    "                'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone', 'FGC-FGC_PU',\n",
    "                'FGC-FGC_PU_Zone', 'FGC-FGC_SRL', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR',\n",
    "                'FGC-FGC_SRR_Zone', 'FGC-FGC_TL', 'FGC-FGC_TL_Zone', 'BIA-Season',\n",
    "                'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC', 'BIA-BIA_BMI',\n",
    "                'BIA-BIA_BMR', 'BIA-BIA_DEE', 'BIA-BIA_ECW', 'BIA-BIA_FFM',\n",
    "                'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_Fat', 'BIA-BIA_Frame_num',\n",
    "                'BIA-BIA_ICW', 'BIA-BIA_LDM', 'BIA-BIA_LST', 'BIA-BIA_SMM',\n",
    "                'BIA-BIA_TBW', 'PAQ_A-Season', 'PAQ_A-PAQ_A_Total', 'PAQ_C-Season',\n",
    "                'PAQ_C-PAQ_C_Total', 'SDS-Season', 'SDS-SDS_Total_Raw',\n",
    "                'SDS-SDS_Total_T', 'PreInt_EduHx-Season',\n",
    "                'PreInt_EduHx-computerinternet_hoursday', 'sii',\n",
    "               \n",
    "                 'PCIAT-Season', 'PCIAT-PCIAT_01', 'PCIAT-PCIAT_02', 'PCIAT-PCIAT_03', 'PCIAT-PCIAT_04',\n",
    "                'PCIAT-PCIAT_05', 'PCIAT-PCIAT_06', 'PCIAT-PCIAT_07', 'PCIAT-PCIAT_08',\n",
    "                'PCIAT-PCIAT_09', 'PCIAT-PCIAT_10', 'PCIAT-PCIAT_11', 'PCIAT-PCIAT_12',\n",
    "                'PCIAT-PCIAT_13', 'PCIAT-PCIAT_14', 'PCIAT-PCIAT_15', 'PCIAT-PCIAT_16',\n",
    "                'PCIAT-PCIAT_17', 'PCIAT-PCIAT_18', 'PCIAT-PCIAT_19', 'PCIAT-PCIAT_20', 'PCIAT-PCIAT_Total',\n",
    "]\n",
    "\n",
    "cat_c = ['Basic_Demos-Enroll_Season', 'CGAS-Season', 'Physical-Season', \n",
    "          'Fitness_Endurance-Season', 'FGC-Season', 'BIA-Season', \n",
    "          'PAQ_A-Season', 'PAQ_C-Season', 'SDS-Season', 'PreInt_EduHx-Season',\n",
    "        'PCIAT-Season',\n",
    "        ]\n",
    "\n",
    "\n",
    "time_series_cols = train_ts.columns.tolist()\n",
    "time_series_cols.remove(\"id\")\n",
    "\n",
    "train = pd.merge(train, train_ts, how=\"left\", on='id')\n",
    "test = pd.merge(test, test_ts, how=\"left\", on='id')\n",
    "\n",
    "train = train.drop('id', axis=1)\n",
    "test = test.drop('id', axis=1)\n",
    "\n",
    "featuresCols += time_series_cols\n",
    "\n",
    "train = train[featuresCols]\n",
    "# train = train.dropna(subset='sii')\n",
    "\n",
    "def update(df):\n",
    "    global cat_c\n",
    "    for c in cat_c: \n",
    "        if c not in df.columns: continue\n",
    "        df[c] = df[c].fillna('Missing')\n",
    "        df[c] = df[c].astype('category')\n",
    "    return df\n",
    "\n",
    "train = update(train)\n",
    "test = update(test)\n",
    "\n",
    "def create_mapping(column, dataset):\n",
    "    unique_values = dataset[column].unique()\n",
    "    return {value: idx for idx, value in enumerate(unique_values)}\n",
    "\n",
    "for col in cat_c:\n",
    "    if col in train.columns:\n",
    "        if 'Season' in col:\n",
    "            mapping = {\n",
    "                'Missing': float('nan'),\n",
    "                'Spring': 0.,\n",
    "                'Summer': 1.,\n",
    "                'Fall': 2.,\n",
    "                'Winter': 3.,\n",
    "            }\n",
    "        else:\n",
    "            mapping = create_mapping(col, train)\n",
    "            print(f'{col}: {mapping}')\n",
    "        train[col] = train[col].replace(mapping)\n",
    "    if col in test.columns:\n",
    "        if 'Season' in col:\n",
    "            mappingTe = {\n",
    "                'Missing': float('nan'),\n",
    "                'Spring': 0.,\n",
    "                'Summer': 1.,\n",
    "                'Fall': 2.,\n",
    "                'Winter': 3.,\n",
    "            }\n",
    "        else:\n",
    "            mappingTe = create_mapping(col, test)\n",
    "            print(f'{col}: {mappingTe}')\n",
    "            \n",
    "        test[col] = test[col].replace(mappingTe)\n",
    "\n",
    "def quadratic_weighted_kappa(y_true, y_pred):\n",
    "    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n",
    "\n",
    "def threshold_Rounder(oof_non_rounded, thresholds):\n",
    "    return np.where(oof_non_rounded < thresholds[0], 0,\n",
    "                    np.where(oof_non_rounded < thresholds[1], 1,\n",
    "                             np.where(oof_non_rounded < thresholds[2], 2, 3)))\n",
    "\n",
    "def evaluate_predictions(thresholds, y_true, oof_non_rounded):\n",
    "    rounded_p = threshold_Rounder(oof_non_rounded, thresholds)\n",
    "    return -quadratic_weighted_kappa(y_true, rounded_p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define PyTorch MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-20T10:31:38.216010Z",
     "iopub.status.idle": "2024-12-20T10:31:38.216399Z",
     "shell.execute_reply": "2024-12-20T10:31:38.216216Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# current implementation: only support numerical values\n",
    "\n",
    "from functools import partial\n",
    "from tkinter import E\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from timm.models.vision_transformer import Block\n",
    "\n",
    "# current implementation: only support numerical values\n",
    "import numpy as np\n",
    "import torch, os\n",
    "from torch import nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import math\n",
    "import argparse\n",
    "import random\n",
    "\n",
    "class MaskEmbed(nn.Module):\n",
    "    \"\"\" record to mask embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, rec_len=25, embed_dim=64, norm_layer=None):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.rec_len = rec_len\n",
    "        self.proj = nn.Conv1d(1, embed_dim, kernel_size=1, stride=1)\n",
    "        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, _, L = x.shape\n",
    "        # assert(L == self.rec_len, f\"Input data width ({L}) doesn't match model ({self.rec_len}).\")\n",
    "        x = self.proj(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ActiveEmbed(nn.Module):\n",
    "    \"\"\" record to mask embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, rec_len=25, embed_dim=64, norm_layer=None):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.rec_len = rec_len\n",
    "        self.proj = nn.Conv1d(1, embed_dim, kernel_size=1, stride=1)\n",
    "        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, _, L = x.shape\n",
    "        # assert(L == self.rec_len, f\"Input data width ({L}) doesn't match model ({self.rec_len}).\")\n",
    "        x = self.proj(x)\n",
    "        x = torch.sin(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        #   x = torch.cat((torch.sin(x), torch.cos(x + math.pi/2)), -1)\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "def get_1d_sincos_pos_embed(embed_dim, pos, cls_token=False):\n",
    "    \"\"\"\n",
    "    embed_dim: output dimension for each position\n",
    "    pos: a list of positions to be encoded: size (M,)\n",
    "    out: (M, D)\n",
    "    \"\"\"\n",
    "\n",
    "    assert embed_dim % 2 == 0\n",
    "    omega = np.arange(embed_dim // 2, dtype=np.float32)\n",
    "    omega /= embed_dim / 2.\n",
    "    omega = 1. / 10000**omega  # (D/2,)\n",
    "\n",
    "    pos = np.arange(pos)  # (M,)\n",
    "    out = np.einsum('m,d->md', pos, omega)  # (M, D/2), outer product\n",
    "\n",
    "    emb_sin = np.sin(out) # (M, D/2)\n",
    "    emb_cos = np.cos(out) # (M, D/2)\n",
    "\n",
    "    pos_embed = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)\n",
    "\n",
    "    if cls_token:\n",
    "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
    "\n",
    "    return pos_embed\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch, lr, min_lr, max_epochs, warmup_epochs):\n",
    "    \"\"\"Decay the learning rate with half-cycle cosine after warmup\"\"\"\n",
    "    if epoch < warmup_epochs:\n",
    "        tmp_lr = lr * epoch / warmup_epochs \n",
    "    else:\n",
    "        tmp_lr = min_lr + (lr - min_lr) * 0.5 * \\\n",
    "            (1. + math.cos(math.pi * (epoch - warmup_epochs) / (max_epochs - warmup_epochs)))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        if \"lr_scale\" in param_group:\n",
    "            param_group[\"lr\"] = tmp_lr * param_group[\"lr_scale\"]\n",
    "        else:\n",
    "            param_group[\"lr\"] = tmp_lr\n",
    "    return tmp_lr\n",
    "\n",
    "\n",
    "def get_grad_norm_(parameters, norm_type: float = 2.0) -> torch.Tensor:\n",
    "    if isinstance(parameters, torch.Tensor):\n",
    "        parameters = [parameters]\n",
    "    parameters = [p for p in parameters if p.grad is not None]\n",
    "    norm_type = float(norm_type)\n",
    "    if len(parameters) == 0:\n",
    "        return torch.tensor(0.)\n",
    "    device = parameters[0].grad.device\n",
    "    if norm_type == np.inf:\n",
    "        total_norm = max(p.grad.detach().abs().max().to(device) for p in parameters)\n",
    "    else:\n",
    "        total_norm = torch.norm(torch.stack([torch.norm(p.grad.detach(), norm_type).to(device) for p in parameters]), norm_type)\n",
    "    return total_norm\n",
    "\n",
    "\n",
    "class NativeScaler:\n",
    "\n",
    "    state_dict_key = \"amp_scaler\"\n",
    "    def __init__(self):\n",
    "        self._scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    def __call__(self, loss, optimizer, clip_grad=None, parameters=None, create_graph=False, update_grad=True):\n",
    "        self._scaler.scale(loss).backward(create_graph=create_graph)\n",
    "        if update_grad:\n",
    "            if clip_grad is not None:\n",
    "                assert parameters is not None\n",
    "                self._scaler.unscale_(optimizer)  # unscale the gradients of optimizer's assigned params in-place\n",
    "                norm = torch.nn.utils.clip_grad_norm_(parameters, clip_grad)\n",
    "            else:\n",
    "                self._scaler.unscale_(optimizer)\n",
    "                norm = get_grad_norm_(parameters)\n",
    "            self._scaler.step(optimizer)\n",
    "            self._scaler.update()\n",
    "        else:\n",
    "            norm = None\n",
    "        return norm\n",
    "\n",
    "    def state_dict(self):\n",
    "        return self._scaler.state_dict()\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        self._scaler.load_state_dict(state_dict)\n",
    "\n",
    "\n",
    "\n",
    "class MAEDataset(Dataset):\n",
    "\n",
    "    def __init__(self, X, M):        \n",
    "         self.X = X\n",
    "         self.M = M\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        return self.X[idx], self.M[idx]\n",
    "\n",
    "\n",
    "\n",
    "def get_dataset(dataset : str, path : str):\n",
    "\n",
    "    if dataset in ['climate', 'compression', 'wine', 'yacht', 'spam', 'letter', 'credit', 'raisin', 'bike', 'obesity', 'airfoil', 'blood', 'yeast', 'health', 'review', 'travel']:\n",
    "        df = pd.read_csv(os.path.join(path, 'data', dataset + '.csv'))\n",
    "        last_col = df.columns[-1]\n",
    "        y = df[last_col]\n",
    "        X = df.drop(columns=[last_col])\n",
    "    elif dataset == 'california':\n",
    "        from sklearn.datasets import fetch_california_housing\n",
    "        X, y = fetch_california_housing(as_frame=True, return_X_y=True)\n",
    "    elif dataset == 'diabetes':\n",
    "        from sklearn.datasets import load_diabetes\n",
    "        X, y = load_diabetes(as_frame=True, return_X_y=True)\n",
    "    elif dataset == 'iris':\n",
    "        # only for testing\n",
    "        from sklearn.datasets import load_iris\n",
    "        X, y = load_iris(as_frame=True, return_X_y=True)\n",
    "    \n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "eps = 1e-6\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.nn import TransformerEncoderLayer\n",
    "from transformers.models.bert.modeling_bert import BertPooler\n",
    "\n",
    "class MaskedAutoencoder(nn.Module):\n",
    "    def __init__(self, rec_len=25, embed_dim=64, depth=4, num_heads=4,\n",
    "                 decoder_embed_dim=64, decoder_depth=2, decoder_num_heads=4,\n",
    "                 mlp_ratio=4., cls_mlp_dim=64, norm_layer=nn.LayerNorm, norm_field_loss=False,\n",
    "                 encode_func='linear', dropout=0.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.rec_len = rec_len\n",
    "        self.embed_dim = embed_dim\n",
    "        self.norm_field_loss = norm_field_loss\n",
    "        \n",
    "        # Encoder\n",
    "        if encode_func == 'active':\n",
    "            self.mask_embed = ActiveEmbed(rec_len, embed_dim, norm_layer)\n",
    "        else:\n",
    "            self.mask_embed = MaskEmbed(rec_len, embed_dim, norm_layer)\n",
    "        \n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, rec_len + 1, embed_dim), requires_grad=False)\n",
    "        \n",
    "        encoder_layer = TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=int(embed_dim * mlp_ratio),\n",
    "                        dropout=dropout, batch_first=True)\n",
    "        self.blocks = nn.TransformerEncoder(encoder_layer, depth)\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "        \n",
    "        \n",
    "        self.enc_pooler = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(embed_dim, 1),\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.Linear((rec_len + 1) * 1, cls_mlp_dim), nn.ReLU(),\n",
    "            ),\n",
    "        ])\n",
    "        self.enc_lbl_pred = nn.Sequential(\n",
    "            nn.Linear(cls_mlp_dim, cls_mlp_dim), nn.ReLU(),\n",
    "            # nn.Linear(cls_mlp_dim, cls_mlp_dim), nn.ReLU(),\n",
    "            # nn.Dropout(dropout),\n",
    "            nn.Linear(cls_mlp_dim, cls_mlp_dim), nn.Sigmoid(),\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder_embed = nn.Linear(embed_dim, decoder_embed_dim, bias=True)\n",
    "        self.mask_token = nn.Parameter(torch.zeros(1, 1, decoder_embed_dim))\n",
    "        self.decoder_pos_embed = nn.Parameter(torch.zeros(1, rec_len + 1, decoder_embed_dim), requires_grad=False)\n",
    "        \n",
    "        decoder_layer = TransformerEncoderLayer(d_model=decoder_embed_dim, nhead=decoder_num_heads, dim_feedforward=int(decoder_embed_dim * mlp_ratio),\n",
    "                        dropout=dropout, batch_first=True)\n",
    "        self.decoder_blocks = nn.TransformerEncoder(decoder_layer, decoder_depth)\n",
    "        self.decoder_norm = norm_layer(decoder_embed_dim)\n",
    "        self.decoder_pred = nn.Linear(decoder_embed_dim, 1, bias=True)\n",
    "        \n",
    "        \n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        pos_embed = get_1d_sincos_pos_embed(self.pos_embed.shape[-1], self.rec_len, cls_token=True)\n",
    "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
    "        \n",
    "        decoder_pos_embed = get_1d_sincos_pos_embed(self.decoder_pos_embed.shape[-1], self.rec_len, cls_token=True)\n",
    "        self.decoder_pos_embed.data.copy_(torch.from_numpy(decoder_pos_embed).float().unsqueeze(0))\n",
    "        \n",
    "        torch.nn.init.xavier_uniform_(self.mask_embed.proj.weight.view([self.mask_embed.proj.weight.shape[0], -1]))\n",
    "        torch.nn.init.normal_(self.cls_token, std=.02)\n",
    "        torch.nn.init.normal_(self.mask_token, std=.02)\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def random_masking(self, x, m, mask_ratio, training=None):\n",
    "        N, L, D = x.shape\n",
    "        if training is None:\n",
    "            training = self.training\n",
    "        if training:\n",
    "            len_keep = int(L * (1 - mask_ratio))\n",
    "            noise = torch.rand(N, L, device=x.device)\n",
    "            noise[m < 1e-6] = 1\n",
    "            ids_shuffle = torch.argsort(noise, dim=1)\n",
    "            ids_restore = torch.argsort(ids_shuffle, dim=1)\n",
    "            ids_keep = ids_shuffle[:, :len_keep]\n",
    "            mask = torch.ones([N, L], device=x.device)\n",
    "            mask[:, :len_keep] = 0\n",
    "            mask = torch.gather(mask, dim=1, index=ids_restore)\n",
    "            mask = torch.logical_or(mask, ~m.bool())\n",
    "            nask = ~mask\n",
    "            return mask, nask\n",
    "        else:\n",
    "            mask = ~m.bool()\n",
    "            nask = m.bool()\n",
    "            return mask, nask\n",
    "\n",
    "    def forward_encoder(self, x, m, mask_ratio=0.5, training=None):\n",
    "        x = self.mask_embed(x)\n",
    "        x = x + self.pos_embed[:, 1:, :]\n",
    "        mask, nask = self.random_masking(x, m, mask_ratio, training)\n",
    "        x = x * (~mask.unsqueeze(-1)).float()\n",
    "        cls_token = self.cls_token + self.pos_embed[:, :1, :]\n",
    "        cls_tokens = cls_token.expand(x.shape[0], -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        attn_mask = torch.cat((torch.zeros(x.shape[0], 1, device=x.device), mask), dim=1)\n",
    "        x = self.blocks(src=x, src_key_padding_mask=attn_mask.bool())\n",
    "        x = self.norm(x)\n",
    "        return x, mask, nask\n",
    "\n",
    "    def forward_decoder(self, x, mask):\n",
    "        x = self.decoder_embed(x)\n",
    "        x = x + self.decoder_pos_embed\n",
    "        mask_with_cls = torch.cat((torch.zeros(x.shape[0], 1, device=x.device), mask), dim=1)\n",
    "        x = self.blocks(src=x, src_key_padding_mask=mask_with_cls.bool())\n",
    "        \n",
    "        x = self.decoder_norm(x)\n",
    "        x = self.decoder_pred(x)\n",
    "        x = x[:, 1:, :].sigmoid()\n",
    "        return x\n",
    "\n",
    "    def forward_loss(self, data, pred, m, mask, nask):\n",
    "        target = data.squeeze(dim=1)\n",
    "        # if self.norm_field_loss:\n",
    "        #     mean = target.mean(dim=-1, keepdim=True)\n",
    "        #     var = target.var(dim=-1, keepdim=True)\n",
    "        #     target = (target - mean) / (var + 1e-6) ** 0.5\n",
    "        rec_mask = mask * m\n",
    "        loss = (pred.squeeze(dim=2) - target) ** 2\n",
    "        loss = (loss * rec_mask).sum() / (rec_mask.sum() + 1e-6) + (loss * nask).sum() / (nask.sum() + 1e-6)\n",
    "        return loss\n",
    "\n",
    "    def forward(self, data, m):\n",
    "        x, _, _ = self.forward_encoder(data, m, 0.0, False)\n",
    "        B = x.shape[0]\n",
    "        h = self.enc_pooler[1](self.enc_pooler[0](x).reshape(B, -1))\n",
    "        enc_pred = self.enc_lbl_pred(h)[:, 0]\n",
    "        return enc_pred\n",
    "\n",
    "    def forward_selfsl(self, data, m, mask_ratio=0.5, training=None):\n",
    "        x, mask, nask = self.forward_encoder(data, m, mask_ratio, training)\n",
    "        pred = self.forward_decoder(x, mask)\n",
    "        loss = self.forward_loss(data, pred, m, mask, nask)\n",
    "        return loss, (loss.item(), )\n",
    "\n",
    "    def forward_sl(self, data, m, lbl_cols):\n",
    "        num_lbls = len(lbl_cols)\n",
    "        lbl_mask = m[:, lbl_cols]\n",
    "        ft_mask = m.clone()\n",
    "        ft_mask[:, lbl_cols] = 0\n",
    "        ft = data.clone()\n",
    "        ft[:, :, lbl_cols] = 0\n",
    "        x, _, _ = self.forward_encoder(ft, ft_mask, 0.0, False)\n",
    "        B = x.shape[0]\n",
    "        h = self.enc_pooler[1](self.enc_pooler[0](x).reshape(B, -1))\n",
    "        enc_pred = self.enc_lbl_pred(h)[:, :num_lbls]\n",
    "\n",
    "        tgt = data[:, 0, lbl_cols]\n",
    "        enc_loss = (((enc_pred - tgt) ** 2) * lbl_mask).sum() / (lbl_mask.sum() + 1e-6)\n",
    "        \n",
    "        loss = enc_loss\n",
    "        \n",
    "        return loss, (enc_loss.item(), )\n",
    "      \n",
    "    def forward_semisl(self, data, m, lbl_cols, ema_model=None, hard=False):\n",
    "        num_lbls = len(lbl_cols)\n",
    "        lbl_mask = m[:, lbl_cols]\n",
    "        nlbl_mask = 1 - m[:, lbl_cols]\n",
    "        ft_mask = m.clone()\n",
    "        ft_mask[:, lbl_cols] = 0\n",
    "        ft = data.clone()\n",
    "        ft[:, :, lbl_cols] = 0\n",
    "        \n",
    "        noise = torch.randn_like(ft)\n",
    "        noise_norm = torch.norm(noise, p=2, dim=-1, keepdim=True)\n",
    "        noise = noise / (noise_norm + 1e-8)\n",
    "        noise = noise * 0.0\n",
    "        if not hard: noise *= 0\n",
    "        \n",
    "        x, _, _ = self.forward_encoder(torch.clamp(ft + noise, min=0.0, max=1.0), ft_mask, 0.0, False)\n",
    "        B = x.shape[0]\n",
    "        h = self.enc_pooler[1](self.enc_pooler[0](x).reshape(B, -1))\n",
    "        enc_pred = self.enc_lbl_pred(h)[:, :num_lbls]\n",
    "        \n",
    "        if ema_model is None: raise NotImplementedError()\n",
    "        # with torch.no_grad():\n",
    "        #     x_tgt, _, _ = ema_model.forward_encoder(ft, ft_mask, 0.0, False)\n",
    "            \n",
    "        #     B = x.shape[0]\n",
    "        #     h_tgt = ema_model.enc_pooler[1](ema_model.enc_pooler[0](x_tgt).reshape(B, -1))\n",
    "        #     semisl_tgt = ema_model.enc_lbl_pred(h_tgt)[:, :num_lbls].detach()\n",
    "        #     if hard:\n",
    "        #         semisl_tgt[:, 0] = (semisl_tgt[:, 0] * 3.).round() / 3.\n",
    "        #         semisl_weight = 1.0\n",
    "        #     else:\n",
    "        #         semisl_weight = 0.1\n",
    "        \n",
    "        \n",
    "        # semisl_loss = (((enc_pred - semisl_tgt) ** 2) * nlbl_mask).sum() / (nlbl_mask.sum() + 1e-6)\n",
    "\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            if hard:\n",
    "                x_tgt, _, _ = ema_model.forward_encoder(ft, ft_mask, 0.0, False)\n",
    "                \n",
    "                B = x_tgt.shape[0]\n",
    "                h_tgt = ema_model.enc_pooler[1](ema_model.enc_pooler[0](x_tgt).reshape(B, -1))\n",
    "                semisl_tgt = ema_model.enc_lbl_pred(h_tgt)[:, :num_lbls].detach()\n",
    "                semisl_tgt[:, 0] = (semisl_tgt[:, 0] * 3.).round() / 3.\n",
    "                semisl_weight = 1.0\n",
    "                semisl_loss = (\n",
    "                    0.5 * (((enc_pred - semisl_tgt) ** 2) * nlbl_mask).sum() / (nlbl_mask.sum() + 1e-6) # all labels\n",
    "                    + 0.5 * (((enc_pred[:, 0] - semisl_tgt[:, 0]) ** 2) * nlbl_mask[:, 0]).sum() / (nlbl_mask[:, 0].sum() + 1e-6) # sii only\n",
    "                )\n",
    "            else:\n",
    "                semisl_weight = 0.0\n",
    "                semisl_loss = torch.tensor([0.0]).to(x.device)\n",
    "                \n",
    "\n",
    "        \n",
    "\n",
    "        sl_tgt = data[:, 0, lbl_cols]\n",
    "        sl_loss = (((enc_pred - sl_tgt) ** 2) * lbl_mask).sum() / (lbl_mask.sum() + 1e-6)\n",
    "        \n",
    "        loss = 1.0 * sl_loss + semisl_weight * semisl_loss\n",
    "        return loss, (sl_loss.item(), semisl_loss.item(),)\n",
    "      \n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define sklearn-like api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-20T10:31:38.217374Z",
     "iopub.status.idle": "2024-12-20T10:31:38.217744Z",
     "shell.execute_reply": "2024-12-20T10:31:38.217577Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# stdlib\n",
    "from typing import Any, List, Tuple, Union\n",
    "\n",
    "# third party\n",
    "import numpy as np\n",
    "import math, sys, argparse\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from functools import partial\n",
    "import time, os, json\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "import sys\n",
    "import timm.optim.optim_factory as optim_factory\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "\n",
    "\n",
    "def quadratic_weighted_kappa(y_true, y_pred):\n",
    "    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n",
    "\n",
    "\n",
    "\n",
    "eps = 1e-8\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from argparse import Namespace\n",
    "remasker_args = Namespace(\n",
    "    batch_size=64,\n",
    "    max_epochs= 600,\n",
    "    accum_iter=1,\n",
    "    mask_ratio=0.5,\n",
    "    embed_dim=32,\n",
    "    depth=6,\n",
    "    decoder_depth=4,\n",
    "    num_heads=4,\n",
    "    mlp_ratio=4.0,\n",
    "    encode_func='linear',\n",
    "    norm_field_loss=False,\n",
    "    weight_decay=0.05,\n",
    "    lr=None, blr=0.001,\n",
    "    min_lr=1e-05,\n",
    "    warmup_epochs=40,\n",
    "    device='cuda', seed=SEED, overwrite=True, pin_mem=True\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "def set_dropout_p(m, p):\n",
    "    if isinstance(m, nn.Dropout):\n",
    "        m.p = p\n",
    "\n",
    "def update_ema_variables(model, ema_model, alpha, global_step, max_global_step):\n",
    "    # Use the true average until the exponential average is more correct\n",
    "    def f(alpha, t, T):\n",
    "        A = 1\n",
    "        B = alpha * T / (T - 1)\n",
    "        return (B * (1 - A / (t + 1)))\n",
    "    current_alpha = f(alpha, global_step, max_global_step)\n",
    "    for ema_param, param in zip(ema_model.parameters(), model.parameters()):\n",
    "        ema_param.data.mul_(current_alpha).add_(1 - current_alpha, param.data)\n",
    "\n",
    "\n",
    "class ReMasker:\n",
    "\n",
    "    def __init__(self, args=remasker_args):\n",
    "\n",
    "        self.batch_size = args.batch_size\n",
    "        self.accum_iter = args.accum_iter\n",
    "        self.min_lr = args.min_lr\n",
    "        self.norm_field_loss = args.norm_field_loss\n",
    "        # self.weight_decay = args.weight_decay\n",
    "        self.lr = args.lr\n",
    "        self.blr = args.blr\n",
    "        self.warmup_epochs = max(1, args.max_epochs // 10)\n",
    "        self.ema_decay = args.ema_decay\n",
    "        self.model = None\n",
    "        self.norm_parameters = None\n",
    "\n",
    "        self.embed_dim = args.embed_dim\n",
    "        self.depth = args.depth\n",
    "        self.decoder_depth = args.decoder_depth\n",
    "        self.num_heads = args.num_heads\n",
    "        self.mlp_ratio = args.mlp_ratio\n",
    "        self.cls_mlp_dim = args.cls_mlp_dim\n",
    "        self.max_epochs = args.max_epochs\n",
    "        self.mask_ratio = args.mask_ratio\n",
    "        self.encode_func = args.encode_func\n",
    "        self.dropout = args.dropout\n",
    "\n",
    "\n",
    "    def fit(self, X_raw: pd.DataFrame, X_val=None, lbl_cols=None, model=None):\n",
    "        global dbg_var\n",
    "        X = X_raw.clone()\n",
    "\n",
    "        # Parameters\n",
    "        no = len(X)\n",
    "        dim = len(X[0, :])\n",
    "\n",
    "        X = X.cpu()\n",
    "\n",
    "        min_val = np.zeros(dim)\n",
    "        max_val = np.zeros(dim)\n",
    "\n",
    "        for i in range(dim):\n",
    "            min_val[i] = np.nanmin(X[:, i])\n",
    "            max_val[i] = np.nanmax(X[:, i])\n",
    "            X[:, i] = (X[:, i] - min_val[i]) / (max_val[i] - min_val[i] + eps)\n",
    "\n",
    "        self.norm_parameters = {\"min\": min_val, \"max\": max_val}\n",
    "\n",
    "        # Set missing\n",
    "        M = 1 - (1 * (np.isnan(X)))\n",
    "        M = M.float().to(device)\n",
    "\n",
    "        X = torch.nan_to_num(X)\n",
    "        X = X.to(device)\n",
    "\n",
    "        if model is None:\n",
    "            self.model = MaskedAutoencoder(\n",
    "                rec_len=dim,\n",
    "                embed_dim=self.embed_dim,\n",
    "                depth=self.depth,\n",
    "                num_heads=self.num_heads,\n",
    "                decoder_embed_dim=self.embed_dim,\n",
    "                decoder_depth=self.decoder_depth,\n",
    "                decoder_num_heads=self.num_heads,\n",
    "                mlp_ratio=self.mlp_ratio,\n",
    "                cls_mlp_dim=self.cls_mlp_dim,\n",
    "                norm_layer=partial(nn.LayerNorm, eps=eps),\n",
    "                norm_field_loss=self.norm_field_loss,\n",
    "                encode_func=self.encode_func,\n",
    "                dropout=self.dropout,\n",
    "            )\n",
    "        else:\n",
    "            self.model = copy.deepcopy(model)\n",
    "            for param in self.model.blocks.layers[:].parameters():\n",
    "                param.detach_()\n",
    "        self.ema_model = copy.deepcopy(self.model)\n",
    "        self.ema_model.apply(lambda m: set_dropout_p(m, p=0.0))\n",
    "        for param in self.ema_model.parameters():\n",
    "            param.detach_()\n",
    "        \n",
    "\n",
    "        self.model.to(device)\n",
    "        self.ema_model.to(device).eval()\n",
    "\n",
    "        # set optimizers\n",
    "        # param_groups = optim_factory.add_weight_decay(model, args.weight_decay)\n",
    "        eff_batch_size = self.batch_size * self.accum_iter\n",
    "        if self.lr is None:  # only base_lr is specified\n",
    "            self.lr = self.blr * eff_batch_size / 64\n",
    "        # param_groups = optim_factory.add_weight_decay(self.model, self.weight_decay)\n",
    "        # optimizer = torch.optim.AdamW(param_groups, lr=self.lr, betas=(0.9, 0.95))\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.lr, betas=(0.9, 0.95))\n",
    "        loss_scaler = NativeScaler()\n",
    "\n",
    "        dataset = MAEDataset(X, M)\n",
    "        dataloader = DataLoader(\n",
    "            dataset, sampler=RandomSampler(dataset),\n",
    "            batch_size=self.batch_size,\n",
    "        )\n",
    "\n",
    "\n",
    "        best_loss = 1e9\n",
    "        best_model = copy.deepcopy(self.model)\n",
    "        for epoch in range(self.max_epochs):\n",
    "            self.model.train()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            total_loss = 0\n",
    "            lbl_loss = 0.\n",
    "            \n",
    "            \n",
    "            import time\n",
    "            dbgt1 = 0\n",
    "            dbgt2 = 0\n",
    "            dbgt3 = 0\n",
    "\n",
    "            iter = 0\n",
    "            for iter, (samples, masks) in enumerate(dataloader):\n",
    "\n",
    "                # we use a per iteration (instead of per epoch) lr scheduler\n",
    "                if iter % self.accum_iter == 0:\n",
    "                    adjust_learning_rate(optimizer, iter / len(dataloader) + epoch, self.lr, self.min_lr,\n",
    "                                         self.max_epochs, self.warmup_epochs)\n",
    "\n",
    "                samples = samples.unsqueeze(dim=1)\n",
    "                samples = samples.to(device, non_blocking=True)\n",
    "                masks = masks.to(device, non_blocking=True)\n",
    "\n",
    "                # print(samples, masks)\n",
    "\n",
    "                # with torch.cuda.amp.autocast():\n",
    "\n",
    "                if lbl_cols is not None:\n",
    "                    input_samples = samples.clone()\n",
    "                    input_masks = masks.clone()\n",
    "                    \n",
    "                    \n",
    "                    # selfsl_loss, dbg_selfsl_loss = self.model.forward_selfsl(samples, masks, mask_ratio=self.mask_ratio)\n",
    "                    \n",
    "                    # sl_loss, dbg_sl_loss = self.model.forward_sl(input_samples, input_masks, lbl_cols)\n",
    "                    \n",
    "                    \n",
    "                    hard = True if (epoch >= (self.max_epochs // 2)) else False\n",
    "                    semisl_loss, dbg_semisl_loss = self.model.forward_semisl(input_samples, input_masks, lbl_cols, ema_model=self.ema_model, hard=hard)\n",
    "                    loss = 1.0 * semisl_loss\n",
    "                        \n",
    "                else:\n",
    "\n",
    "                    selfsl_loss, dbg_selfsl_loss = self.model.forward_selfsl(samples, masks, mask_ratio=self.mask_ratio)\n",
    "                    loss = selfsl_loss\n",
    "                \n",
    "                loss_value = loss.item()\n",
    "                total_loss += loss_value\n",
    "                if not math.isfinite(loss_value):\n",
    "                    print(\"Loss is {}, stopping training\".format(loss_value))\n",
    "                    dbg_var = (samples, masks)\n",
    "                    sys.exit(1)\n",
    "\n",
    "                loss /= self.accum_iter\n",
    "                loss_scaler(loss, optimizer, parameters=self.model.parameters(),\n",
    "                            update_grad=(iter + 1) % self.accum_iter == 0)\n",
    "\n",
    "                if (iter + 1) % self.accum_iter == 0:\n",
    "                    optimizer.zero_grad()\n",
    "                    \n",
    "                \n",
    "            update_ema_variables(self.model, self.ema_model, self.ema_decay, epoch, self.max_epochs)\n",
    "            # print(dbgt1)\n",
    "            # print(dbgt2)\n",
    "            # print(dbgt3)\n",
    "            total_loss = (total_loss / (iter + 1))\n",
    "            self.model.eval()\n",
    "            if X_val is not None:\n",
    "                val_loss = self.evaluate(X_val, lbl_cols)\n",
    "            else:\n",
    "                val_loss = total_loss\n",
    "            if val_loss <= best_loss:\n",
    "                best_loss = val_loss\n",
    "                best_model = copy.deepcopy(self.model)\n",
    "            if (epoch + 1) % max(1, self.max_epochs // 10) == 0 or epoch == 0:\n",
    "                lbl_loss = lbl_loss / (iter + 1)\n",
    "                \n",
    "                if lbl_cols is not None:\n",
    "                    print(\"Epoch: %d, train;val;best qwk: %.4f;%.4f;%.4f, loss: %.4f, val_loss: %.4f\" % \n",
    "                        (epoch+1, -self.evaluate(X_raw, lbl_cols), -val_loss, -best_loss, total_loss, val_loss)\n",
    "                    )\n",
    "                else:\n",
    "                    print(\"Epoch: %d, loss: %.4f\" % \n",
    "                        (epoch+1, -best_loss)\n",
    "                    )\n",
    "                    \n",
    "                \n",
    "\n",
    "        self.model = best_model\n",
    "        print(f'Loaded best model with loss={best_loss:.4f}')\n",
    "        # torch.save(self.model.state_dict(), self.path)\n",
    "        return self\n",
    "      \n",
    "      \n",
    "      \n",
    "    def evaluate(self, X_raw: torch.Tensor, lbl_cols):\n",
    "        keep_indices = torch.where(~X_raw[:, lbl_cols].isnan())[0]\n",
    "        X_raw = X_raw[keep_indices]\n",
    "        gt = X_raw[:, lbl_cols[0]].cpu().numpy().round(0).astype(int)\n",
    "        X_raw[:, lbl_cols] = float('nan')\n",
    "        yp = self.predict(X_raw, lbl_cols)\n",
    "        yp = yp.cpu().numpy().round(0).astype(int)\n",
    "        return -quadratic_weighted_kappa(gt, yp)\n",
    "      \n",
    "      \n",
    "    def predict(self, X_raw: torch.Tensor, lbl_idx, bs=None):\n",
    "        X_raw = torch.tensor(X_raw, dtype=torch.float32)\n",
    "        \n",
    "        # Normalize the input data\n",
    "        min_val = self.norm_parameters[\"min\"]\n",
    "        max_val = self.norm_parameters[\"max\"]\n",
    "        X = X_raw.clone()\n",
    "        for i in range(X.shape[1]):\n",
    "            X[:, i] = (X[:, i] - min_val[i]) / (max_val[i] - min_val[i] + eps)\n",
    "        \n",
    "        M = (1 - (1 * torch.isnan(X))).float().to(device)\n",
    "        \n",
    "        X = torch.nan_to_num(X)\n",
    "        X = X.to(device)\n",
    "        \n",
    "        if bs == None: bs = self.batch_size\n",
    "        # Prepare DataLoader\n",
    "        dataset = MAEDataset(X, M)\n",
    "        dataloader = DataLoader(dataset, batch_size=bs, shuffle=False)\n",
    "        \n",
    "        # Ensure model is in evaluation mode\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Tensor to hold predictions\n",
    "        predictions = torch.zeros(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_samples, batch_masks in dataloader:\n",
    "                # Prepare input for the model\n",
    "                batch_samples = batch_samples.unsqueeze(dim=1).to(device)\n",
    "                batch_masks = batch_masks.to(device)\n",
    "                \n",
    "                # Forward pass with training=False\n",
    "                pred = self.model.forward(batch_samples, batch_masks)\n",
    "                \n",
    "                pred = pred.reshape(-1)\n",
    "                \n",
    "                predictions = torch.cat((predictions, pred), 0)\n",
    "        \n",
    "        return predictions * 3.\n",
    "\n",
    "    \n",
    "    def fit_transform(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Imputes the provided dataset using the GAIN strategy.\n",
    "        Args:\n",
    "            X: np.ndarray\n",
    "                A dataset with missing values.\n",
    "        Returns:\n",
    "            Xhat: The imputed dataset.\n",
    "        \"\"\"\n",
    "        X = torch.tensor(X.values, dtype=torch.float32)\n",
    "        return self.fit(X).transform(X).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load training tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-20T10:31:38.218675Z",
     "iopub.status.idle": "2024-12-20T10:31:38.219085Z",
     "shell.execute_reply": "2024-12-20T10:31:38.218904Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "full_df = pd.concat([train,test])\n",
    "X_raw = torch.tensor(full_df.to_numpy()).float()\n",
    "print(X_raw.shape)\n",
    "\n",
    "def random_extend(arr, k):\n",
    "    indices = np.concatenate([np.random.permutation(len(arr)) for _ in range(10)])[:k]\n",
    "    return arr[indices]\n",
    "\n",
    "random_extend(X_raw, 5000).shape\n",
    "\n",
    "for c in full_df.columns:\n",
    "    if c not in test.columns:\n",
    "        test[c] = float('nan')\n",
    "\n",
    "test = test[full_df.columns]\n",
    "\n",
    "X_tensor_test = torch.tensor(test.to_numpy()).float()\n",
    "\n",
    "\n",
    "def get_model_size(model):\n",
    "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    model_size = sum([np.prod(p.size()) for p in model_parameters])\n",
    "    return \"{}K\".format(round(model_size / 1e1) / 1e2)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-20T10:31:38.219876Z",
     "iopub.status.idle": "2024-12-20T10:31:38.220247Z",
     "shell.execute_reply": "2024-12-20T10:31:38.220081Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.base import clone\n",
    "\n",
    "def quadratic_weighted_kappa(y_true, y_pred):\n",
    "    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n",
    "\n",
    "def threshold_Rounder(oof_non_rounded, thresholds):\n",
    "    return np.where(oof_non_rounded < thresholds[0], 0,\n",
    "                    np.where(oof_non_rounded < thresholds[1], 1,\n",
    "                             np.where(oof_non_rounded < thresholds[2], 2, 3)))\n",
    "\n",
    "def evaluate_predictions(thresholds, y_true, oof_non_rounded):\n",
    "    rounded_p = threshold_Rounder(oof_non_rounded, thresholds)\n",
    "    return -quadratic_weighted_kappa(y_true, rounded_p)\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "def random_extend(arr, k):\n",
    "    indices = np.concatenate([np.random.permutation(len(arr)) for _ in range(10)])[:k]\n",
    "    return arr[indices]\n",
    "\n",
    "\n",
    "num_folds = 5\n",
    "\n",
    "def PerformImpute(imputer_args):\n",
    "    global X_raw, X_tensor_test, num_folds\n",
    "    train_S = []\n",
    "    test_S = []\n",
    "    \n",
    "    KF = KFold(n_splits=num_folds, shuffle=True, random_state=SEED)\n",
    "\n",
    "    oof_non_rounded = []\n",
    "    oof_rounded = []\n",
    "    oof_gt = []\n",
    "    test_preds = np.zeros((len(X_tensor_test), num_folds))\n",
    "    \n",
    "    \n",
    "    lbl_cols = [full_df.columns.get_loc(c) for c in full_df.columns if 'PCIAT' in c or 'sii' in c]\n",
    "    lbl_idx = lbl_cols[0]\n",
    "    \n",
    "    lbled_indices = torch.where(~X_raw[:, lbl_idx].isnan())[0]\n",
    "    \n",
    "    \n",
    "    \n",
    "    X_raw_no_lbl = X_raw.clone()\n",
    "    X_raw_no_lbl[:, lbl_cols] = float('nan')\n",
    "    \n",
    "    pretrain_args = copy.deepcopy(imputer_args)\n",
    "    pretrain_args.max_epochs = pretrain_args.pretrain_epochs\n",
    "    import time\n",
    "    pretrain_tick = time.time()\n",
    "    imputer_pretrain = ReMasker(pretrain_args)\n",
    "    imputer_pretrain.fit(X_raw_no_lbl, None, None, None)\n",
    "    ellapsed_time = time.time() - pretrain_tick\n",
    "    print(f\"Pretrained in {ellapsed_time:.4f}s.\")\n",
    "    \n",
    "    pretrain_model = imputer_pretrain.model\n",
    "\n",
    "    \n",
    "\n",
    "    pbar = tqdm(KF.split(lbled_indices), desc=\"Training Folds\", total=n_splits)    \n",
    "\n",
    "    for fold, (train_idx_idx, test_idx_idx) in enumerate(pbar):\n",
    "        train_idx = lbled_indices[train_idx_idx]\n",
    "        test_idx = lbled_indices[test_idx_idx]\n",
    "\n",
    "        X_train = X_raw.clone()\n",
    "        \n",
    "        \n",
    "        \n",
    "        X_train[test_idx.unsqueeze(1), lbl_cols] = float('nan')\n",
    "        \n",
    "        X_val = X_raw[test_idx].clone()\n",
    "\n",
    "        \n",
    "\n",
    "        X_train = random_extend(X_train, 9000)\n",
    "        # X_val = random_extend(X_val, 2000)\n",
    "        \n",
    "    \n",
    "        train_nonna_indices = torch.where(~X_train[:, lbl_idx].isnan())[0]\n",
    "        val_nonna_indices = torch.where(~X_val[:, lbl_idx].isnan())[0]\n",
    "        if len(train_nonna_indices) == 0 or len(val_nonna_indices)==0: continue\n",
    "    \n",
    "        imputer = ReMasker(imputer_args)\n",
    "        imputer.fit(X_train, X_val, lbl_cols, pretrain_model)\n",
    "\n",
    "        y_train_ = X_train[train_nonna_indices, lbl_idx].numpy().astype(int)\n",
    "        y_val_ = X_val[val_nonna_indices, lbl_idx].numpy().astype(int)\n",
    "\n",
    "        X_train[:, lbl_cols] = float('nan')\n",
    "        X_val[:, lbl_cols] = float('nan')\n",
    "        \n",
    "        y_train_pred = imputer.predict(X_train[train_nonna_indices], lbl_cols).cpu().detach().numpy()\n",
    "        y_val_pred = imputer.predict(X_val[val_nonna_indices], lbl_cols).cpu().detach().numpy()\n",
    "        y_test_pred = imputer.predict(X_tensor_test, lbl_cols).cpu().detach().numpy()\n",
    "\n",
    "        # model = clone(model_init)\n",
    "\n",
    "        # model.fit(X_train_, y_train_)\n",
    "\n",
    "        oof_non_rounded += [y_val_pred]\n",
    "        y_val_pred_rounded = y_val_pred.round(0).astype(int)\n",
    "        oof_rounded += [y_val_pred_rounded]\n",
    "        oof_gt += [y_val_]\n",
    "        \n",
    "        train_kappa = quadratic_weighted_kappa(y_train_, y_train_pred.round(0).astype(int))\n",
    "        val_kappa = quadratic_weighted_kappa(y_val_, y_val_pred.round(0).astype(int))\n",
    "\n",
    "        train_S.append(train_kappa)\n",
    "        test_S.append(val_kappa)\n",
    "        \n",
    "        \n",
    "        test_preds[:, fold] = y_test_pred\n",
    "\n",
    "        pbar.set_description_str(\n",
    "          \"Fold %d, Train MSE: %.4f, Val MSE: %.4f, Train QWK: %.4f, Val QWK: %.4f\" % (\n",
    "              fold + 1,\n",
    "              ((y_train_pred - y_train_) ** 2 / 9.).mean(),\n",
    "              ((y_val_pred - y_val_) ** 2 / 9.).mean(),\n",
    "              train_kappa,\n",
    "              val_kappa\n",
    "          )\n",
    "        )\n",
    "    \n",
    "    \n",
    "    print(f\"Mean Train QWK --> {np.mean(train_S):.4f}\")\n",
    "    print(f\"Mean Validation QWK ---> {np.mean(test_S):.4f}\")\n",
    "\n",
    "    oof_non_rounded = np.concatenate(oof_non_rounded)\n",
    "    oof_gt = np.concatenate(oof_gt)\n",
    "    KappaOPtimizer = minimize(evaluate_predictions,\n",
    "                              x0=[0.5, 1.5, 2.5], args=(oof_gt, oof_non_rounded), \n",
    "                              method='Nelder-Mead')\n",
    "    assert KappaOPtimizer.success, \"Optimization did not converge.\"\n",
    "    \n",
    "    oof_tuned = threshold_Rounder(oof_non_rounded, KappaOPtimizer.x)\n",
    "    tKappa = quadratic_weighted_kappa(oof_gt, oof_tuned)\n",
    "\n",
    "    print(f\"----> || Optimized QWK SCORE :: {Fore.CYAN}{Style.BRIGHT} {tKappa:.3f}{Style.RESET_ALL}\")\n",
    "\n",
    "\n",
    "    # return 0.5 * (np.mean(test_S) + tKappa)\n",
    "  \n",
    "  \n",
    "    tpm = test_preds.mean(axis=1)\n",
    "    tpTuned = threshold_Rounder(tpm, KappaOPtimizer.x)\n",
    "    \n",
    "    sample_sub_df = pd.read_csv('../input/child-mind-institute-problematic-internet-use/sample_submission.csv')\n",
    "    submission = pd.DataFrame({\n",
    "        'id': sample_sub_df['id'],\n",
    "        'sii': tpTuned\n",
    "    })\n",
    "\n",
    "    return submission\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-20T10:31:38.221559Z",
     "iopub.status.idle": "2024-12-20T10:31:38.221936Z",
     "shell.execute_reply": "2024-12-20T10:31:38.221765Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "imputer_args = Namespace(\n",
    "    batch_size=64,\n",
    "    max_epochs= 50,\n",
    "    pretrain_epochs=400,\n",
    "    accum_iter=1,\n",
    "    mask_ratio=0.75,\n",
    "    embed_dim=6,\n",
    "    depth=8,\n",
    "    decoder_depth=1,\n",
    "    num_heads=3,\n",
    "    mlp_ratio=21.5,\n",
    "    cls_mlp_dim=48,\n",
    "    dropout=0.5,\n",
    "    encode_func='linear',\n",
    "    norm_field_loss=False,\n",
    "    ema_decay=0.9,\n",
    "    weight_decay=0.05,\n",
    "    lr=None, blr=0.001,\n",
    "    min_lr=1e-05,\n",
    "    device='cuda', seed=SEED, overwrite=True, pin_mem=True\n",
    ")\n",
    "print(imputer_args)\n",
    "\n",
    "\n",
    "\n",
    "SEED = random.randint(1, int(2e9))\n",
    "np.random.seed(SEED)\n",
    "indices = np.random.permutation(len(X_raw))\n",
    "X_raw = X_raw[indices]\n",
    "submission6 = PerformImpute(imputer_args)\n",
    "submission6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final majority voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-20T10:31:38.222960Z",
     "iopub.status.idle": "2024-12-20T10:31:38.223293Z",
     "shell.execute_reply": "2024-12-20T10:31:38.223172Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sub1 = submission1\n",
    "sub2 = submission2\n",
    "sub3 = submission3\n",
    "sub4 = submission4\n",
    "sub5 = submission5\n",
    "sub6 = submission6\n",
    "\n",
    "sub1 = sub1.sort_values(by='id').reset_index(drop=True)\n",
    "sub2 = sub2.sort_values(by='id').reset_index(drop=True)\n",
    "sub3 = sub3.sort_values(by='id').reset_index(drop=True)\n",
    "sub4 = sub4.sort_values(by='id').reset_index(drop=True)\n",
    "sub5 = sub5.sort_values(by='id').reset_index(drop=True)\n",
    "sub6 = sub6.sort_values(by='id').reset_index(drop=True)\n",
    "\n",
    "\n",
    "combined = pd.DataFrame({\n",
    "    'id': sub1['id'],\n",
    "    'sii_1': sub1['sii'],\n",
    "    'sii_2': sub2['sii'],\n",
    "    'sii_3': sub3['sii'],\n",
    "    'sii_4': sub4['sii'],\n",
    "    'sii_5': sub5['sii'],\n",
    "    'sii_6': sub6['sii'],\n",
    "})\n",
    "\n",
    "def majority_vote(row):\n",
    "    return row.mode()[0]\n",
    "\n",
    "combined['final_sii'] = combined[['sii_1', 'sii_2', 'sii_3', 'sii_4', 'sii_5', 'sii_6']].apply(majority_vote, axis=1)\n",
    "\n",
    "final_submission = combined[['id', 'final_sii']].rename(columns={'final_sii': 'sii'})\n",
    "\n",
    "final_submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"Majority voting completed and saved to 'Final_Submission.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 9643020,
     "sourceId": 81933,
     "sourceType": "competition"
    },
    {
     "datasetId": 921302,
     "sourceId": 7453542,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5742470,
     "sourceId": 9448132,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5742473,
     "sourceId": 9448136,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6324642,
     "sourceId": 10229352,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6327724,
     "sourceId": 10233697,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30823,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
