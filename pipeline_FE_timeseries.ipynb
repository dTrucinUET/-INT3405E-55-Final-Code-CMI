{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":81933,"databundleVersionId":9643020,"sourceType":"competition"},{"sourceId":7453542,"sourceType":"datasetVersion","datasetId":921302},{"sourceId":9448132,"sourceType":"datasetVersion","datasetId":5742470},{"sourceId":9448136,"sourceType":"datasetVersion","datasetId":5742473},{"sourceId":10229352,"sourceType":"datasetVersion","datasetId":6324642},{"sourceId":10233697,"sourceType":"datasetVersion","datasetId":6327724},{"sourceId":10279281,"sourceType":"datasetVersion","datasetId":6360724}],"dockerImageVersionId":30823,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.base import clone\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom scipy.optimize import minimize\nfrom concurrent.futures import ThreadPoolExecutor\nfrom sklearn.preprocessing import StandardScaler\nimport torch.nn as nn\nimport torch.optim as optim\nimport polars as pl\nfrom colorama import Fore, Style\nfrom IPython.display import clear_output\nimport warnings\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import VotingRegressor\nfrom sklearn.impute import SimpleImputer\nimport os\nimport random\nimport torch\nimport numpy as np\nimport sys\nfrom glob import glob\nfrom tqdm import tqdm\nfrom pathlib import Path\nimport polars as pl\nimport datetime\nimport yaml\nimport gc\n\n\nwarnings.filterwarnings('ignore')\npd.options.display.max_columns = None","metadata":{"execution":{"iopub.status.busy":"2024-12-23T19:18:08.667258Z","iopub.execute_input":"2024-12-23T19:18:08.667609Z","iopub.status.idle":"2024-12-23T19:18:08.673874Z","shell.execute_reply.started":"2024-12-23T19:18:08.667583Z","shell.execute_reply":"2024-12-23T19:18:08.672987Z"},"trusted":true},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"# 1. Overview","metadata":{}},{"cell_type":"markdown","source":"In this notebook, we will introduce pipelines about feaute engineering with time series data. We will setup for 4 experiments\n\n1. Use statical for time serries (load parquet file and describle to extract feature like mean, std, max, mod,...)\n2. Use autoencoder to extract feature (input is statical)\n3. Implement sleep detection pipeline to ceate some new features\n4. Combine 2 and 3","metadata":{}},{"cell_type":"markdown","source":"# 2.Define common function","metadata":{}},{"cell_type":"code","source":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nSEED = 42\nn_splits = 5\nseed_everything(SEED)","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2024-12-23T19:18:08.701913Z","iopub.execute_input":"2024-12-23T19:18:08.702135Z","iopub.status.idle":"2024-12-23T19:18:08.709988Z","shell.execute_reply.started":"2024-12-23T19:18:08.702116Z","shell.execute_reply":"2024-12-23T19:18:08.709123Z"},"trusted":true},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# Handle non-numeric columns\ndef update(df):\n    global cat_c\n    for c in cat_c: \n        df[c] = df[c].fillna('Missing')\n        df[c] = df[c].astype('category')\n    return df\n\ndef create_mapping(column, dataset):\n    unique_values = dataset[column].unique()\n    return {value: idx for idx, value in enumerate(unique_values)}\n\n\n# Function to evaluate the predictions and optimize the thresholds\ndef quadratic_weighted_kappa(y_true, y_pred):\n    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n\ndef threshold_Rounder(oof_non_rounded, thresholds):\n    return np.where(oof_non_rounded < thresholds[0], 0,\n                    np.where(oof_non_rounded < thresholds[1], 1,\n                             np.where(oof_non_rounded < thresholds[2], 2, 3)))\n\ndef evaluate_predictions(thresholds, y_true, oof_non_rounded):\n    rounded_p = threshold_Rounder(oof_non_rounded, thresholds)\n    return -quadratic_weighted_kappa(y_true, rounded_p)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T19:18:08.733029Z","iopub.execute_input":"2024-12-23T19:18:08.733230Z","iopub.status.idle":"2024-12-23T19:18:08.739001Z","shell.execute_reply.started":"2024-12-23T19:18:08.733212Z","shell.execute_reply":"2024-12-23T19:18:08.738148Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"def TrainML(model_class, X, y, test_data):\n    SKF = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n    \n    train_S = []\n    test_S = []\n    \n    oof_non_rounded = np.zeros(len(y), dtype=float) \n    oof_rounded = np.zeros(len(y), dtype=int) \n    test_preds = np.zeros((len(test_data), n_splits))\n\n    for fold, (train_idx, test_idx) in enumerate(tqdm(SKF.split(X, y), desc=\"Training Folds\", total=n_splits)):\n        X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n        y_train, y_val = y.iloc[train_idx], y.iloc[test_idx]\n        model = clone(model_class)\n        model.fit(X_train, y_train)\n\n        y_train_pred = model.predict(X_train)\n        y_val_pred = model.predict(X_val)\n\n        oof_non_rounded[test_idx] = y_val_pred\n        y_val_pred_rounded = y_val_pred.round(0).astype(int)\n        oof_rounded[test_idx] = y_val_pred_rounded\n\n        train_kappa = quadratic_weighted_kappa(y_train, y_train_pred.round(0).astype(int))\n        val_kappa = quadratic_weighted_kappa(y_val, y_val_pred_rounded)\n\n        train_S.append(train_kappa)\n        test_S.append(val_kappa)\n        \n        test_preds[:, fold] = model.predict(test_data)\n        \n        print(f\"Fold {fold+1} - Train QWK: {train_kappa:.4f}, Validation QWK: {val_kappa:.4f}\")\n        clear_output(wait=True)\n    print(f\"Mean Train QWK --> {np.mean(train_S):.4f}\")\n    print(f\"Mean Validation QWK ---> {np.mean(test_S):.4f}\")\n\n    KappaOPtimizer = minimize(evaluate_predictions,\n                              x0=[0.5, 1.5, 2.5], args=(y, oof_non_rounded), \n                              method='Nelder-Mead')\n    assert KappaOPtimizer.success, \"Optimization did not converge.\"\n    print('OPTIMIZED THRESHOLDS', KappaOPtimizer.x)\n    oof_tuned = threshold_Rounder(oof_non_rounded, KappaOPtimizer.x)\n    tKappa = quadratic_weighted_kappa(y, oof_tuned)\n\n    print(f\"----> || Optimized QWK SCORE :: {Fore.CYAN}{Style.BRIGHT} {tKappa:.3f}{Style.RESET_ALL}\")\n\n    tpm = test_preds.mean(axis=1)\n    tpTuned = threshold_Rounder(tpm, KappaOPtimizer.x)\n    \n    submission = pd.DataFrame({\n        'id': sample['id'],\n        'sii': tpTuned\n    })\n    optimized_thresholds = KappaOPtimizer.x\n    return submission, oof_tuned, oof_non_rounded, y, optimized_thresholds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T19:18:08.756626Z","iopub.execute_input":"2024-12-23T19:18:08.756913Z","iopub.status.idle":"2024-12-23T19:18:08.765191Z","shell.execute_reply.started":"2024-12-23T19:18:08.756891Z","shell.execute_reply":"2024-12-23T19:18:08.764481Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"def process_file(filename, dirname):\n    df = pd.read_parquet(os.path.join(dirname, filename, 'part-0.parquet'))\n    df.drop('step', axis=1, inplace=True)\n    return df.describe().values.reshape(-1), filename.split('=')[1]\n\ndef load_time_series(dirname) -> pd.DataFrame:\n    ids = os.listdir(dirname)\n    with ThreadPoolExecutor() as executor:\n        results = list(tqdm(executor.map(lambda fname: process_file(fname, dirname), ids), total=len(ids)))\n    stats, indexes = zip(*results)\n    df = pd.DataFrame(stats, columns=[f\"stat_{i}\" for i in range(len(stats[0]))])\n    df['id'] = indexes\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T19:18:08.779599Z","iopub.execute_input":"2024-12-23T19:18:08.779829Z","iopub.status.idle":"2024-12-23T19:18:08.784815Z","shell.execute_reply.started":"2024-12-23T19:18:08.779810Z","shell.execute_reply":"2024-12-23T19:18:08.784087Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"# 3. Loading data","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/train.csv')\ntest = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/test.csv')\nsample = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/sample_submission.csv')\n\ntotal_features = list(test.columns)\ntotal_features.remove('id')\n\ncat_c = ['Basic_Demos-Enroll_Season', 'CGAS-Season', 'Physical-Season', \n          'Fitness_Endurance-Season', 'FGC-Season', 'BIA-Season', \n          'PAQ_A-Season', 'PAQ_C-Season', 'SDS-Season', 'PreInt_EduHx-Season']\n\ntrain_ts = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_train.parquet\")\ntest_ts = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_test.parquet\")\ntime_series_cols = train_ts.columns.tolist()\ntime_series_cols.remove(\"id\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T19:18:08.839444Z","iopub.execute_input":"2024-12-23T19:18:08.839732Z","iopub.status.idle":"2024-12-23T19:19:20.467961Z","shell.execute_reply.started":"2024-12-23T19:18:08.839701Z","shell.execute_reply":"2024-12-23T19:19:20.466950Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 996/996 [01:11<00:00, 13.96it/s]\n100%|██████████| 2/2 [00:00<00:00, 13.74it/s]\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"# 3. Define model","metadata":{}},{"cell_type":"code","source":"SVR_Best_Params = {\n    'C': 0.1,\n    'epsilon': 0.1,\n    'kernel': 'rbf',\n    'gamma': 'scale',\n}\n\nCatBoost_Best_Params = {\n    'learning_rate': 0.0021172579310639343,\n    'depth': 6,\n    'iterations': 130,\n    'random_seed': SEED,\n    'verbose': 0,\n    'l2_leaf_reg': 0.32557701990001503,\n    'task_type': 'GPU',  \n    'devices': '0'\n}\n\nXGB_Best_Params = {\n    'n_estimators': 700,\n    'max_depth': 4,\n    'learning_rate': 0.03325152156380898,\n    'subsample': 0.25295047248406266,\n    'colsample_bytree': 0.9760859719849787,\n    'gamma': 0.20085951790463402,\n    'min_child_weight': 11,\n    'eval_metric': 'rmse',\n    'objective': 'reg:squarederror',\n    'tree_method': 'gpu_hist',\n    'predictor': 'gpu_predictor',\n    'gpu_id': 0\n}\n\nLightGBM_Best_Params = {\n    'max_depth': 3,\n    'min_data_in_leaf': 40,\n    'num_leaves': 190,\n    'learning_rate': 0.05107368421432176,\n    'feature_fraction': 0.9918350138636185,\n    'bagging_fraction': 0.9331400899763774,\n    'bagging_freq': 1,\n    'lambda_l1': 9.49641646280519,\n    'lambda_l2': 2.446305429623661,\n    'min_gain_to_split': 0.05262124930522051,\n    'device_type': 'gpu',\n    'gpu_device_id': 0,\n    'verbosity': -1\n}\n\n\n\ncatboost_model = CatBoostRegressor(**CatBoost_Best_Params)\nxgb_model = XGBRegressor(**XGB_Best_Params)\nlightgbm_model = LGBMRegressor(**LightGBM_Best_Params)\nsvr_model = SVR(**SVR_Best_Params)\n\nfinal_voting_model = VotingRegressor(estimators=[\n    ('lightgbm', lightgbm_model),\n    ('xgboost', xgb_model),\n    ('catboost', catboost_model),\n], weights=[4.0, 4.0, 4.0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T19:19:20.469305Z","iopub.execute_input":"2024-12-23T19:19:20.469554Z","iopub.status.idle":"2024-12-23T19:19:20.477387Z","shell.execute_reply.started":"2024-12-23T19:19:20.469533Z","shell.execute_reply":"2024-12-23T19:19:20.476496Z"}},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":"# 3. Experiments","metadata":{}},{"cell_type":"markdown","source":"## 3.1. Statical","metadata":{}},{"cell_type":"code","source":"train_sub1 = pd.merge(train, train_ts, how=\"left\", on='id')\ntest_sub1 = pd.merge(test, test_ts, how=\"left\", on='id')\ntrain_season_cols = [col for col in train_sub1.columns if 'Season' in col]\ntest_season_cols = [col for col in test_sub1.columns if 'Season' in col]\n\ntrain_sub1 = train_sub1.drop(train_season_cols, axis=1) \ntest_sub1 = test_sub1.drop(test_season_cols, axis=1) \n\ntrain_sub1 = train_sub1.drop('id', axis=1)\ntest_sub1  = test_sub1.drop('id', axis=1)  \ntrain_sub1 = train_sub1.dropna(subset='sii')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T19:19:20.478775Z","iopub.execute_input":"2024-12-23T19:19:20.479005Z","iopub.status.idle":"2024-12-23T19:19:20.509471Z","shell.execute_reply.started":"2024-12-23T19:19:20.478985Z","shell.execute_reply":"2024-12-23T19:19:20.508578Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"feature_cols = test_sub1.columns\ny = train_sub1['sii']\nX = train_sub1[feature_cols]\ntest_sub1 = test_sub1[feature_cols]\n\nsubmission1, _, _, _, _= TrainML(xgb_model, X, y, test_sub1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T19:19:20.510939Z","iopub.execute_input":"2024-12-23T19:19:20.511291Z","iopub.status.idle":"2024-12-23T19:19:28.069930Z","shell.execute_reply.started":"2024-12-23T19:19:20.511253Z","shell.execute_reply":"2024-12-23T19:19:28.068968Z"}},"outputs":[{"name":"stderr","text":"Training Folds: 100%|██████████| 5/5 [00:07<00:00,  1.46s/it]","output_type":"stream"},{"name":"stdout","text":"Mean Train QWK --> 0.7707\nMean Validation QWK ---> 0.3498\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"OPTIMIZED THRESHOLDS [0.605951   0.96145833 2.68485966]\n----> || Optimized QWK SCORE :: \u001b[36m\u001b[1m 0.423\u001b[0m\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"submission11, _, _, _, _= TrainML(final_voting_model, X, y, test_sub1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T19:19:28.070887Z","iopub.execute_input":"2024-12-23T19:19:28.071155Z","iopub.status.idle":"2024-12-23T19:19:46.470710Z","shell.execute_reply.started":"2024-12-23T19:19:28.071133Z","shell.execute_reply":"2024-12-23T19:19:46.469732Z"}},"outputs":[{"name":"stderr","text":"Training Folds: 100%|██████████| 5/5 [00:18<00:00,  3.63s/it]","output_type":"stream"},{"name":"stdout","text":"Mean Train QWK --> 0.5531\nMean Validation QWK ---> 0.3434\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"OPTIMIZED THRESHOLDS [0.6245561  0.83645409 2.91189813]\n----> || Optimized QWK SCORE :: \u001b[36m\u001b[1m 0.455\u001b[0m\n","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"## 3.2. Autoencoder","metadata":{}},{"cell_type":"markdown","source":"### 3.2.1. Define function related","metadata":{}},{"cell_type":"code","source":"class AutoEncoder(nn.Module):\n    def __init__(self, input_dim, encoding_dim):\n        super(AutoEncoder, self).__init__()\n        self.encoder = nn.Sequential(\n            nn.Linear(input_dim, encoding_dim*3),\n            nn.ReLU(),\n            nn.Linear(encoding_dim*3, encoding_dim*2),\n            nn.ReLU(),\n            nn.Linear(encoding_dim*2, encoding_dim),\n            nn.ReLU()\n        )\n        self.decoder = nn.Sequential(\n            nn.Linear(encoding_dim, encoding_dim*2),\n            nn.ReLU(),\n            nn.Linear(encoding_dim*2, encoding_dim*3),\n            nn.ReLU(),\n            nn.Linear(encoding_dim*3, input_dim),\n            nn.Sigmoid()\n        )\n\n        \n    def forward(self, x):\n        encoded = self.encoder(x)\n        decoded = self.decoder(encoded)\n        return decoded\n\n\ndef perform_autoencoder(df, encoding_dim=50, epochs=50, batch_size=32):\n    scaler = StandardScaler()\n    df_scaled = scaler.fit_transform(df)\n    \n    data_tensor = torch.FloatTensor(df_scaled)\n    \n    input_dim = data_tensor.shape[1]\n    autoencoder = AutoEncoder(input_dim, encoding_dim)\n    \n    criterion = nn.MSELoss()\n    optimizer = optim.Adam(autoencoder.parameters())\n    \n    for epoch in range(epochs):\n        for i in range(0, len(data_tensor), batch_size):\n            batch = data_tensor[i : i + batch_size]\n            optimizer.zero_grad()\n            reconstructed = autoencoder(batch)\n            loss = criterion(reconstructed, batch)\n            loss.backward()\n            optimizer.step()\n            \n        if (epoch + 1) % 50 == 0:\n            print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}]')    \n    return autoencoder, scaler\n\ndef encode_data(autoencoder, scaler, df):\n    df_scaled = scaler.transform(df)\n    data_tensor = torch.FloatTensor(df_scaled)\n    with torch.no_grad():\n        encoded_data = autoencoder.encoder(data_tensor).numpy()\n\n    df_encoded = pd.DataFrame(encoded_data, columns=[f'Enc_{i + 1}' for i in range(encoded_data.shape[1])])\n    return df_encoded","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T19:19:46.471860Z","iopub.execute_input":"2024-12-23T19:19:46.472127Z","iopub.status.idle":"2024-12-23T19:19:46.481616Z","shell.execute_reply.started":"2024-12-23T19:19:46.472104Z","shell.execute_reply":"2024-12-23T19:19:46.480748Z"}},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":"### 3.2.2. Feature Extraction","metadata":{}},{"cell_type":"code","source":"df_train = train_ts.drop('id', axis=1)\ndf_test = test_ts.drop('id', axis=1)\nautoencoder, scaler = perform_autoencoder(df_train, encoding_dim=60, epochs=100, batch_size=32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T19:19:46.482763Z","iopub.execute_input":"2024-12-23T19:19:46.483126Z","iopub.status.idle":"2024-12-23T19:19:55.831566Z","shell.execute_reply.started":"2024-12-23T19:19:46.483061Z","shell.execute_reply":"2024-12-23T19:19:55.830507Z"}},"outputs":[{"name":"stdout","text":"Epoch [50/100], Loss: 1.4474]\nEpoch [100/100], Loss: 1.3982]\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"train_ts_encoded = encode_data(autoencoder, scaler, df_train)\ntest_ts_encoded = encode_data(autoencoder, scaler, df_test)\ntest_ts_encoded.reset_index(inplace=True, drop=True)\ntrain_ts_encoded[\"id\"]=train_ts[\"id\"]\ntest_ts_encoded['id']=test_ts[\"id\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T19:19:55.834635Z","iopub.execute_input":"2024-12-23T19:19:55.834942Z","iopub.status.idle":"2024-12-23T19:19:55.847962Z","shell.execute_reply.started":"2024-12-23T19:19:55.834916Z","shell.execute_reply":"2024-12-23T19:19:55.847237Z"}},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":"### 3.2.3. Training","metadata":{}},{"cell_type":"code","source":"train_sub2 = pd.merge(train, train_ts_encoded, how=\"left\", on='id')\ntest_sub2 = pd.merge(test, test_ts_encoded, how=\"left\", on='id')\ntrain_season_cols = [col for col in train_sub2.columns if 'Season' in col]\ntest_season_cols = [col for col in test_sub2.columns if 'Season' in col]\n\ntrain_sub2 = train_sub2.drop(train_season_cols, axis=1) \ntest_sub2 = test_sub2.drop(test_season_cols, axis=1) \n\ntrain_sub2 = train_sub2.drop('id', axis=1)\ntest_sub2  = test_sub2.drop('id', axis=1)  \ntrain_sub2 = train_sub2.dropna(subset='sii')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T19:19:55.849351Z","iopub.execute_input":"2024-12-23T19:19:55.849582Z","iopub.status.idle":"2024-12-23T19:19:55.868161Z","shell.execute_reply.started":"2024-12-23T19:19:55.849560Z","shell.execute_reply":"2024-12-23T19:19:55.867232Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"feature_cols = test_sub2.columns\ny = train_sub2['sii']\nX = train_sub2[feature_cols]\ntest_sub2 = test_sub2[feature_cols]\n\nsubmission2, _, _, _, _= TrainML(xgb_model, X, y, test_sub2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T19:20:13.350817Z","iopub.execute_input":"2024-12-23T19:20:13.351103Z","iopub.status.idle":"2024-12-23T19:20:20.336013Z","shell.execute_reply.started":"2024-12-23T19:20:13.351080Z","shell.execute_reply":"2024-12-23T19:20:20.335065Z"}},"outputs":[{"name":"stderr","text":"Training Folds: 100%|██████████| 5/5 [00:06<00:00,  1.35s/it]","output_type":"stream"},{"name":"stdout","text":"Mean Train QWK --> 0.7686\nMean Validation QWK ---> 0.3729\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"OPTIMIZED THRESHOLDS [0.59043153 1.05095802 2.69897292]\n----> || Optimized QWK SCORE :: \u001b[36m\u001b[1m 0.430\u001b[0m\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"submission22, _, _, _, _= TrainML(final_voting_model, X, y, test_sub2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T19:20:20.337422Z","iopub.execute_input":"2024-12-23T19:20:20.337831Z","iopub.status.idle":"2024-12-23T19:20:32.207002Z","shell.execute_reply.started":"2024-12-23T19:20:20.337804Z","shell.execute_reply":"2024-12-23T19:20:32.206059Z"}},"outputs":[{"name":"stderr","text":"Training Folds: 100%|██████████| 5/5 [00:11<00:00,  2.31s/it]","output_type":"stream"},{"name":"stdout","text":"Mean Train QWK --> 0.5480\nMean Validation QWK ---> 0.3494\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"OPTIMIZED THRESHOLDS [0.58171798 0.90530577 2.60575461]\n----> || Optimized QWK SCORE :: \u001b[36m\u001b[1m 0.460\u001b[0m\n","output_type":"stream"}],"execution_count":30},{"cell_type":"markdown","source":"## 3.3. Sleep Detection (External knowledge)","metadata":{}},{"cell_type":"markdown","source":"### 3.3.1. Define function related","metadata":{}},{"cell_type":"markdown","source":"First, prepare the features used in my sleep detection model. Please refer to the implementation by [@tatamikenn](https://www.kaggle.com/tatamikenn) [here](https://www.kaggle.com/code/tatamikenn/sleep-hdcza-a-pure-heuristic-approach-lb-0-447).\n\nThis pipeline processes accelerometer data for sleep detection, utilizing time-series datasets. It generates features to identify sleep episodes, static periods, and motion patterns, inspired by @tatamikenn's implementation.\n\n-  `transform`: this function processes input data to generate features for analysis. It breaks down the timestamp into components like year, month, day, hour, and weekday. It also groups data by night, adjusting the timestamp if necessary, and creates a unique `night_group` identifier for each night. Additionally, a cumulative step count (norm_step) is computed for each group to facilitate sequential analysis.\n\n- `transform_series`: this function enhances the transform function by adding a new feature: detecting clipped ENMO values. It flags instances where the enmo (motion metric) is zero, marking potential data quality issues.\n\n- `transform_events`: this function processes event data by adding a night column and pivoting the data. The events are rearranged by `series_id`, `group_id`, and night to simplify time-series analysis.\n\n- `add_feature`: this function generates advanced features for sleep detection, including:\n    - Difference Features: Computes the differences in anglez (angular motion) and enmo (motion magnitude).\n    - Rolling Median: Calculates rolling medians of anglez_diff and enmo_diff over a 5-minute window.\n    - Critical Threshold: Determines static periods by evaluating anglez_diff variability over a day.\n    - Static and Sleep Blocks: Flags periods with minimal motion (is_static) and identifies sleep blocks over 30-minute windows.\n    - Sleep Episodes: Detects continuous sleep episodes, identifies the longest one, and flags interruptions in sleep.\n\n-  `create_heuristic`: the main function processes raw data files by converting timestamps and applying transformations. It calls the transform_series function to prepare the data and the add_feature function to generate sleep-related features. Finally, it saves the processed data into .parquet files for further analysis.","metadata":{}},{"cell_type":"code","source":"MAX_FILE = 2000","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T19:19:55.890109Z","iopub.status.idle":"2024-12-23T19:19:55.890509Z","shell.execute_reply":"2024-12-23T19:19:55.890300Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def transform(df, night_offset=20):\n    return (\n        df.with_columns(\n            [\n                (pl.col(\"timestamp\").dt.year() - 2000).cast(pl.Int8).alias(\"year\"),\n                pl.col(\"timestamp\").dt.month().cast(pl.Int8).alias(\"month\"),\n                pl.col(\"timestamp\").dt.day().cast(pl.Int8).alias(\"day\"),\n                pl.col(\"timestamp\").dt.hour().cast(pl.Int8).alias(\"hour\"),\n                pl.col(\"timestamp\").dt.minute().cast(pl.Int8).alias(\"minute\"),\n                pl.col(\"timestamp\").dt.second().cast(pl.Int8).alias(\"second\"),\n                pl.col(\"timestamp\").dt.weekday().cast(pl.Int8).alias(\"weekday\"),\n            ]\n        )\n        .with_columns( \n            pl.when(pl.col(\"hour\") < night_offset)\n            .then(pl.col(\"timestamp\"))\n            .otherwise(pl.col(\"timestamp\") + pl.duration(days=1))\n            .dt.date()\n            .alias(\"night_group\"),\n        )\n        .with_columns(\n            [\n                (\n                    pl.col(\"series_id\") + pl.lit(\"_\") + pl.col(\"night_group\").cast(pl.Datetime).dt.strftime(\"%Y%m%d\")\n                ).alias(\"group_id\"),\n            ]\n        )\n        .with_columns(\n            [\n                pl.col(\"timestamp\").cum_count().over(\"group_id\").alias(\"norm_step\"),\n            ]\n        )\n        .drop([\"night_group\"])\n    )\n\n\ndef transform_series(df):\n    return transform(df).with_columns(\n        [\n            (pl.col(\"enmo\") == 0).alias(\"is_enmo_clipped\"),\n        ]\n    )\n\n\ndef transform_events(df):\n    return (\n        transform(df)\n        .with_columns(\n            [\n                pl.col(\"night\").cast(pl.UInt32).alias(\"night\"),\n            ]\n        )\n        .pivot([\"step\", \"timestamp\", \"tz_offset\"], [\"series_id\", \"group_id\", \"night\"], \"event\")\n    )\n\n\ndef add_feature(\n    df,\n    day_group_col=\"group_id\",\n    term1=(5 * 60) // 5,\n    term2=(30 * 60) // 5,\n    term3=(60 * 60) // 5,\n    min_threshold=0.005,\n    max_threshold=0.04,\n    center=True,\n):\n    return (\n        df.with_columns(\n            [\n                pl.col(\"anglez\").diff(1).abs().alias(\"anglez_diff\"),\n                pl.col(\"enmo\").diff(1).abs().alias(\"enmo_diff\"),\n            ]\n        )\n        .with_columns(\n            [\n                pl.col(\"anglez_diff\")\n                .rolling_median(term1, center=center)  # 5 min window\n                .alias(\"anglez_diff_median_5min\"),\n                pl.col(\"enmo_diff\")\n                .rolling_median(term1, center=center)  # 5 min window\n                .alias(\"enmo_diff_median_5min\"),\n            ]\n        )\n        .with_columns(\n            [\n                pl.col(\"anglez_diff_median_5min\")\n                .quantile(0.1)\n                .clip(min_threshold, max_threshold)\n                .over(day_group_col)\n                .alias(\"critical_threshold\")\n            ]\n        )\n        .with_columns([(pl.col(\"anglez_diff_median_5min\") < pl.col(\"critical_threshold\") * 15).alias(\"is_static\")])\n        .with_columns(\n            [\n                pl.col(\"is_static\").cast(pl.Int32).rolling_sum(term2, center=center).alias(\"is_static_sum_30min\"),\n            ]\n        )\n        .with_columns([(pl.col(\"is_static_sum_30min\") == ((30 * 60) // 5)).alias(\"tmp\")])\n        .with_columns(\n            [\n                pl.col(\"tmp\").shift(term2 // 2).alias(\"tmp_left\"),\n                pl.col(\"tmp\").shift(-(term2 // 2)).alias(\"tmp_right\"),\n            ]\n        )\n        .with_columns(\n            [\n                (pl.col(\"tmp_left\") | pl.col(\"tmp_right\")).alias(\"is_sleep_block\"),\n            ]\n        )\n        .drop([\"tmp\", \"tmp_left\", \"tmp_right\"])\n        .with_columns([pl.col(\"is_sleep_block\").not_().alias(\"is_gap\")])\n        .with_columns([pl.col(\"is_gap\").cast(pl.Int32).rolling_sum(term3, center=center).alias(\"gap_length\")])\n        .with_columns([(pl.col(\"gap_length\") == term3).alias(\"tmp\")])\n        .with_columns(\n            [\n                pl.col(\"tmp\").shift(term3 // 2).alias(\"tmp_left\"),\n                pl.col(\"tmp\").shift(-(term3 // 2)).alias(\"tmp_right\"),\n            ]\n        )\n        .with_columns(\n            [\n                (pl.col(\"tmp_left\") | pl.col(\"tmp_right\")).alias(\"is_large_gap\"),\n            ]\n        )\n        .drop([\"tmp\", \"tmp_left\", \"tmp_right\"])\n        .with_columns([pl.col(\"is_large_gap\").not_().alias(\"is_sleep_episode\")])\n        #\n        # extract longest sleep episode\n        #\n        .with_columns(\n            [\n                # extract false->true transition\n                (\n                    (\n                        pl.col(\"is_sleep_episode\")\n                        & pl.col(\"is_sleep_episode\").shift(1, fill_value=pl.lit(False)).not_()\n                    )\n                    .cum_sum()\n                    .over(\"group_id\")\n                ).alias(\"sleep_episode_id\")\n            ]\n        )\n        .with_columns(\n            [pl.col(\"is_sleep_episode\").sum().over([\"group_id\", \"sleep_episode_id\"]).alias(\"sleep_episode_length\")]\n        )\n        .with_columns([pl.col(\"sleep_episode_length\").max().over([\"group_id\"]).alias(\"max_sleep_episode_length\")])\n        .with_columns(\n            [\n                (\n                    pl.col(\"is_sleep_episode\") & (pl.col(\"sleep_episode_length\") == pl.col(\"max_sleep_episode_length\"))\n                ).alias(\"is_longest_sleep_episode\")\n            ]\n        )\n    )\n\n\nuse_columns = [\n    \"series_id\",\n    \"step\",\n    \"is_longest_sleep_episode\",\n    \"is_sleep_block\",\n    \"is_gap\",\n    \"is_large_gap\",\n    \"is_sleep_episode\",\n    \"is_static\",\n]\n\ndef create_heuristic(paths, train_or_test):\n    i = 0\n    for path in tqdm(paths):\n        i += 1\n        if (i == MAX_FILE):\n            break\n        sdf = pl.read_parquet(path)\n    \n        # dummy timestamp\n        sdf = sdf.with_columns((pl.col(\"time_of_day\") == 0).cast(pl.Int32).cum_sum().alias(\"day_offset\"))\n        sdf = sdf.with_columns(\n            (\n                datetime.datetime(2020, 1, 1)\n                + (pl.col(\"day_offset\") * 86400_000_000 + pl.col(\"time_of_day\") / 1000).cast(pl.Duration(\"us\"))\n            ).alias(\"timestamp\")\n        )\n    \n        sdf = sdf.with_columns(pl.lit(path.split(\"/\")[-2]).alias(\"series_id\"))\n        sdf = sdf.sort(\"step\")\n        sdf = transform_series(sdf)\n        sdf = add_feature(sdf)\n        sdf = sdf[use_columns].fill_null(False)\n    \n        sidf = path.split(\"/\")[-2]\n        save_path = f\"/kaggle/working/heuristic_features/{train_or_test}/{sidf}.parquet\"\n        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n        sdf.write_parquet(save_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T19:19:55.892112Z","iopub.status.idle":"2024-12-23T19:19:55.892524Z","shell.execute_reply":"2024-12-23T19:19:55.892413Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if True:\n    sys.path.append(\"/kaggle/input/cmi-2023-src\")\n    from consts import ANGLEZ_MEAN, ANGLEZ_STD, ENMO_MEAN, ENMO_STD\n    from torch_models.dataset import ZzzPatchDataset\n    from torch_models.models import ZzzConv1dGRUModel, ZzzTransformerGRUModel, ZzzWaveGRUModel\n\n    from utils.feature_contena import Features\n    from utils.lightning_utils import MyLightningDataModule, MyLightningModule\n    from utils.set_seed import seed_base_torch\n    from utils.torch_template import EnsembleModel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T19:19:55.893236Z","iopub.status.idle":"2024-12-23T19:19:55.893567Z","shell.execute_reply":"2024-12-23T19:19:55.893394Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def detection(paths=f\"/kaggle/input/child-mind-institute-problematic-internet-use/series_train.parquet/id=*/part-0.parquet\", train_or_test=\"train\"):\n    MODEL_NAME = \"patch_transformer_gru\"\n    \n    PACKAGE_DIR = Path(\"/kaggle/input/cmi-2023-src\")\n    CFG = yaml.safe_load(open(PACKAGE_DIR / \"config.yaml\", \"r\"))\n    BLOCK_SIZE = CFG[MODEL_NAME][\"execution\"][\"block_size\"]\n    \n    CFG[\"output_dir\"] = f\"/kaggle/input/cmi-2023-output/{CFG[MODEL_NAME]['execution']['best_exp_id']}\"\n    \n    seed_base_torch(CFG[\"env\"][\"seed\"])\n    \n    DEVICE = \"cuda\"\n    \n    files = glob(\n        paths\n    )\n    \n    features = Features()\n    features.add_num_features([\"anglez\", \"enmo\"])\n    features.add_num_features([\"anglez_diff\", \"enmo_diff\"])\n    features.add_num_features([\"same_count\"])\n    features.add_num_features([\"large_diff_count\"])\n    features.add_num_features([\"same_count_shift_plus\", \"same_count_shift_minus\"])\n    features.add_num_features([\"is_longest_sleep_episode\", \"is_sleep_block\"])\n    \n    # transformer + gru\n    model = ZzzTransformerGRUModel(\n        max_len=BLOCK_SIZE // CFG[MODEL_NAME][\"execution\"][\"patch_size\"],\n        input_numerical_size=len(features.all_features()) * CFG[MODEL_NAME][\"execution\"][\"patch_size\"],\n        **CFG[MODEL_NAME][\"params\"],\n    )\n    trn_models = [\n        MyLightningModule.load_from_checkpoint(\n            os.path.join(\"/kaggle/input/cmi-2023-output/exp_160\", f\"logs/best_model_fold{fold}.ckpt\"),\n            model=model,\n            map_location=torch.device(DEVICE),\n        ).to(DEVICE)\n        for fold in range(5 if len(files) > 100 else 1)\n    ]\n    \n    models = trn_models\n    model = EnsembleModel(models).to(DEVICE)\n    model.eval()\n    \n    all_oof_dfs = []\n    i = 0\n    for file in tqdm(files):\n        # load file\n        i += 1\n        if (i == MAX_FILE):\n            break\n        df = pd.read_parquet(file)\n        if len(df) < BLOCK_SIZE:\n            continue\n        time_of_days = df[\"time_of_day\"].values\n    \n        # same_count\n        DAY_STEPS = 12 * 60 * 24\n        n_days = int(len(df) // DAY_STEPS) + 1\n        df[\"same_count\"] = 0\n        for day in range(-n_days, n_days + 1):\n            if day == 0:\n                continue\n            df[\"_anglez_diff\"] = df[\"anglez\"].diff(DAY_STEPS * day)\n            df[\"_anglez_diff\"] = df[\"_anglez_diff\"].fillna(1)\n            df[\"same_count\"] += (df[\"_anglez_diff\"] == 0).astype(int)\n        df[\"same_count\"] = (df[\"same_count\"].clip(0, 5) - 2.5) / 2.5\n    \n        SHIFT_STEPS = 12 * 60 * 6  # 6h\n        df[\"same_count_shift_plus\"] = df[\"same_count\"].shift(SHIFT_STEPS).fillna(1.0).astype(np.float16)\n        df[\"same_count_shift_minus\"] = df[\"same_count\"].shift(-SHIFT_STEPS).fillna(1.0).astype(np.float16)\n    \n        # features\n        df[\"anglez_diffabs\"] = df[\"anglez\"].diff().abs().fillna(0)\n        df[\"large_diff\"] = (df[\"anglez_diffabs\"] > 5).astype(int)\n        df[\"large_diff_count\"] = df[\"large_diff\"].rolling(10, center=True).mean().fillna(0)\n        df[\"large_diff_count\"] = (df[\"large_diff_count\"] - 0.5) * 2\n    \n        # normalize\n        df[\"anglez\"] = (df[\"anglez\"] - ANGLEZ_MEAN) / ANGLEZ_STD\n        df[\"enmo\"] = (df[\"enmo\"] - ENMO_MEAN) / ENMO_STD\n        df[\"anglez_diff\"] = df[\"anglez\"].diff().fillna(0)\n        df[\"enmo_diff\"] = df[\"enmo\"].diff().fillna(0)\n    \n        # heuristic_features by @bilzard\n        sid = file.split(\"/\")[-2]\n        df[\"series_id\"] = sid\n        path = f\"/kaggle/working/heuristic_features/{train_or_test}/{sid}.parquet\"\n        hdf = pd.read_parquet(path)\n        df = pd.concat([df, hdf.drop(columns=[\"series_id\", \"step\"])], axis=1)\n        df[[\"is_longest_sleep_episode\", \"is_sleep_block\"]] = df[[\"is_longest_sleep_episode\", \"is_sleep_block\"]] * 2 - 1\n    \n        # split\n        dfs = []\n        df = df.sort_values(\"step\").reset_index(drop=True)\n        for start in range(0, len(df), BLOCK_SIZE // 8):\n            end = start + BLOCK_SIZE\n            if end > len(df):\n                end = len(df) - len(df) % CFG[MODEL_NAME][\"execution\"][\"patch_size\"]\n                start = end - BLOCK_SIZE\n                assert start >= 0\n            assert df.iloc[start][\"step\"] % CFG[MODEL_NAME][\"execution\"][\"patch_size\"] == 0\n            dfs.append(df.iloc[start:end])\n        gc.collect()\n    \n        # inference\n        train_dataset = ZzzPatchDataset(\n            dfs, mode=\"test\", features=features, patch_size=CFG[MODEL_NAME][\"execution\"][\"patch_size\"]\n        )\n        valid_dataset = ZzzPatchDataset(\n            dfs, mode=\"test\", features=features, patch_size=CFG[MODEL_NAME][\"execution\"][\"patch_size\"]\n        )\n        data_module = MyLightningDataModule(train_dataset, valid_dataset, batch_size=64)\n        preds = []\n        with torch.no_grad():\n            for X in data_module.val_dataloader():\n                pred = torch.sigmoid(model(X.to(\"cuda\"))).detach().cpu().numpy() * 10\n                preds.append(pred)\n    \n        oof_dfs = []\n        for pred, df in zip(np.vstack(preds), dfs):\n            df = df.iloc[\n                CFG[MODEL_NAME][\"execution\"][\"patch_size\"] // 2 : len(df) : CFG[MODEL_NAME][\"execution\"][\"patch_size\"]\n            ].reset_index(drop=True)\n            df[[\"wakeup_oof\", \"onset_oof\"]] = pred\n            oof_dfs.append(df[[\"series_id\", \"step\", \"wakeup_oof\", \"onset_oof\"]])\n    \n        oof_df = pd.concat(oof_dfs)\n        oof_df = oof_df.groupby([\"series_id\", \"step\"]).mean().reset_index().sort_values([\"series_id\", \"step\"])\n        oof_df = oof_df[[\"series_id\", \"step\", \"wakeup_oof\", \"onset_oof\"]]\n        oof_df[\"step\"] = oof_df[\"step\"].astype(int)\n    \n        del preds, oof_dfs\n        gc.collect()\n    \n        train = oof_df.reset_index(drop=True)\n        train[\"time_of_day\"] = time_of_days[\n            CFG[MODEL_NAME][\"execution\"][\"patch_size\"] // 2 :: CFG[MODEL_NAME][\"execution\"][\"patch_size\"]\n        ][: len(train)]\n        all_oof_dfs.append(train[[\"series_id\", \"step\", \"wakeup_oof\", \"onset_oof\", \"time_of_day\"]])\n        # del dfs, df\n        gc.collect()\n\n    # save\n    for df in tqdm(all_oof_dfs):\n        save_path = f\"/kaggle/working/features/sleep_detection/{train_or_test}/{df['series_id'].iloc[0]}.parquet\"\n        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n        df.to_parquet(save_path, index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T19:19:55.894430Z","iopub.status.idle":"2024-12-23T19:19:55.894807Z","shell.execute_reply":"2024-12-23T19:19:55.894652Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"time_of_day_max = 86400000000000","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T19:19:55.895694Z","iopub.status.idle":"2024-12-23T19:19:55.896033Z","shell.execute_reply":"2024-12-23T19:19:55.895891Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def feature_engineering(paths=\"/kaggle/working/features/sleep_detection/train/*.parquet\", data_paths=\"/kaggle/input/child-mind-institute-problematic-internet-use/series_train.parquet\", train_or_test=\"train\"):\n    features = []\n    debug_count = 0\n    all_files = sorted(glob(paths))\n    i = 0\n    for file in tqdm(all_files):\n        i += 1\n        if (i == MAX_FILE):\n            break\n        df = pl.read_parquet(file)\n        df = df.with_columns(pl.col(\"step\").cast(pl.UInt32)).drop(\"time_of_day\")\n        sid = df[\"series_id\"][0]\n    \n        sensor_df = pl.read_parquet(\n            f\"{data_paths}/{sid}/part-0.parquet\"\n        ).with_columns((pl.col(\"time_of_day\") == 0).cum_sum().alias(\"day\"))\n    \n        feature = {\n            \"id\": sid,\n            \"length\": df.shape[0],\n            \"day\": sensor_df[\"relative_date_PCIAT\"].max() - sensor_df[\"relative_date_PCIAT\"].min(),\n        }\n    \n        # skip if time step is not 5sec\n        diffs = sensor_df[\"time_of_day\"].diff().drop_nulls().unique()\n        if set(diffs) != set([-86395000000000, 5000000000]):\n            features.append(feature)\n            continue\n    \n        sensor_df = (\n            sensor_df.join(df, on=\"step\", how=\"left\")\n            .sort(\"step\")\n            .with_columns(\n                pl.col(\"onset_oof\").interpolate(),\n                pl.col(\"wakeup_oof\").interpolate(),\n            )\n        )\n    \n        # onset = 15:00~3:00, wakeup = 3:00~15:00\n        onset_start = time_of_day_max / 24 * 15  # 15:00\n        onset_end = time_of_day_max / 24 * 3  # 3:00\n        sensor_df = sensor_df.with_columns(\n            ((pl.col(\"time_of_day\") > onset_start) | (pl.col(\"time_of_day\") < onset_end)).alias(\"onset_duration\"),\n        ).with_columns(\n            pl.col(\"onset_duration\").cast(pl.Int32).diff().fill_null(0).abs().cum_sum().alias(\"onset_wakeup_duration\")\n        )\n    \n        # get sleep period\n        sleep_info = []\n        for _, df in sensor_df.group_by(\"onset_wakeup_duration\", maintain_order=True):\n            is_onset = df[\"onset_duration\"][0]\n            if is_onset:\n                max_idx = df[\"onset_oof\"].arg_max()\n                if max_idx is None:\n                    continue\n                max_score = df[\"onset_oof\"][max_idx]\n                step = df[\"step\"][max_idx]\n    \n                # date\n                start_time = df[\"time_of_day\"][0] / time_of_day_max * 24\n                if start_time >= 15:\n                    day = df[\"day\"][0]\n                    week_day = df[\"weekday\"][0]\n                else:\n                    day = df[\"day\"][0] - 1\n                    week_day = df[\"weekday\"][0] - 1\n                    if week_day == 0:\n                        week_day = 7\n            else:\n                max_idx = df[\"wakeup_oof\"].arg_max()\n                if max_idx is None:\n                    continue\n                max_score = df[\"wakeup_oof\"][max_idx]\n                step = df[\"step\"][max_idx]\n    \n                # date\n                start_time = df[\"time_of_day\"][0] / time_of_day_max * 24\n                day = df[\"day\"][0] - 1\n                week_day = df[\"weekday\"][0] - 1\n    \n            info = {\n                \"day\": day,\n                \"weekday\": week_day,\n                \"type\": \"onset\" if is_onset else \"wakeup\",\n                \"step\": step,\n                \"max_score\": max_score,\n                \"time\": df[\"time_of_day\"][max_idx] / time_of_day_max * 24,\n            }\n            sleep_info.append(info)\n        sleep_df = pl.DataFrame(sleep_info)\n    \n        # merge\n        sleep_df = (\n            sleep_df.filter(pl.col(\"type\") == \"onset\")\n            .drop(\"type\")\n            .rename(\n                {\n                    \"max_score\": \"onset_score\",\n                    \"step\": \"onset_step\",\n                    \"time\": \"onset_time\",\n                }\n            )\n            .join(\n                sleep_df.filter(pl.col(\"type\") == \"wakeup\")\n                .drop([\"type\", \"weekday\"])\n                .rename(\n                    {\n                        \"max_score\": \"wakeup_score\",\n                        \"step\": \"wakeup_step\",\n                        \"time\": \"wakeup_time\",\n                    }\n                ),\n                on=\"day\",\n            )\n        ).select(\n            [\"day\", \"weekday\", \"onset_time\", \"wakeup_time\", \"onset_step\", \"wakeup_step\", \"onset_score\", \"wakeup_score\"]\n        )\n    \n        # feature engineering\n        sleep_lengths = []  # wakeup - onset\n        sleep_enmo_mean = []  \n        sleep_enmo_std = []  \n        sleep_light_mean = []\n        sleep_light_std = [] \n        for i in range(len(sleep_df)):\n            # sleep period\n            start = sleep_df[\"onset_step\"][i]\n            end = sleep_df[\"wakeup_step\"][i]\n            if sleep_df[\"onset_score\"][i] < 1 or sleep_df[\"wakeup_score\"][i] < 1:\n                sleep_lengths.append(np.nan)\n                sleep_enmo_mean.append(np.nan)\n                sleep_enmo_std.append(np.nan)\n                sleep_light_mean.append(np.nan)\n                sleep_light_std.append(np.nan)\n                continue\n    \n            # sleep length\n            length = end - start\n            sleep_lengths.append(length * 5 / 60 / 60)  # hour\n    \n            # enmo\n            enmo_mean = sensor_df[\"enmo\"][start:end].mean()\n            enmo_std = sensor_df[\"enmo\"][start:end].std()\n            sleep_enmo_mean.append(enmo_mean)\n            sleep_enmo_std.append(enmo_std)\n    \n            # light\n            light_mean = sensor_df[\"light\"][start:end].mean()\n            light_std = sensor_df[\"light\"][start:end].std()\n            sleep_light_mean.append(light_mean)\n            sleep_light_std.append(light_std)\n            \n        sleep_df = sleep_df.with_columns(\n            pl.DataFrame(\n                {\n                    \"sleep_length\": sleep_lengths,\n                    \"sleep_enmo_mean\": sleep_enmo_mean,\n                    \"sleep_enmo_std\": sleep_enmo_std,\n                    \"sleep_light_mean\": sleep_light_mean,\n                    \"sleep_light_std\": sleep_light_std,\n                }\n            )\n        )\n        \n        # leave only high confidence periods\n        sleep_df = sleep_df.filter((pl.col(\"wakeup_score\") > 1) & (pl.col(\"onset_score\") > 1))\n        if debug_count < 3:\n            display(sleep_df.head())\n        debug_count += 1\n            \n    \n        # agg\n        feature.update(\n            {\n                \"sleep_measurement_count\": sleep_df.shape[0],\n                \"sleep_length_mean\": sleep_df[\"sleep_length\"].mean(),\n                \"sleep_length_std\": sleep_df[\"sleep_length\"].std(),\n                \"sleep_start_mean\": sleep_df[\"onset_time\"].mean(),\n                \"sleep_start_std\": sleep_df[\"onset_time\"].std(),\n                \"sleep_end_mean\": sleep_df[\"wakeup_time\"].mean(),\n                \"sleep_end_std\": sleep_df[\"wakeup_time\"].std(),\n                \"sleep_enmo_mean_mean\": sleep_df[\"sleep_enmo_mean\"].mean(),\n                \"sleep_enmo_mean_std\": sleep_df[\"sleep_enmo_mean\"].std(),\n                \"sleep_enmo_std_mean\": sleep_df[\"sleep_enmo_std\"].mean(),\n                \"sleep_enmo_std_std\": sleep_df[\"sleep_enmo_std\"].std(),\n                \"sleep_light_mean_mean\": sleep_df[\"sleep_light_mean\"].mean(),\n                \"sleep_light_mean_std\": sleep_df[\"sleep_light_mean\"].std(),\n                \"sleep_light_std_mean\": sleep_df[\"sleep_light_std\"].mean(),\n                \"sleep_light_std_std\": sleep_df[\"sleep_light_std\"].std(),\n            }\n        )\n        features.append(feature)\n    output_dir = f\"/kaggle/working/features/{train_or_test}\"\n    os.makedirs(output_dir, exist_ok=True)\n    feature_df = pl.DataFrame(features).with_columns(pl.col(\"id\").str.slice(3, 8))\n    feature_df.write_csv(f\"/kaggle/working/features/{train_or_test}/sleep_features.csv\")\n    print(feature_df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T19:19:55.897783Z","iopub.status.idle":"2024-12-23T19:19:55.898117Z","shell.execute_reply":"2024-12-23T19:19:55.897930Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 3.3.2. Loading data","metadata":{}},{"cell_type":"code","source":"create_heuristic(paths=glob(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_test.parquet/id=*/part-0.parquet\"), train_or_test=\"test\")\ndetection(paths=\"/kaggle/input/child-mind-institute-problematic-internet-use/series_test.parquet/id=*/part-0.parquet\", train_or_test=\"test\")\nfeature_engineering(paths=\"/kaggle/working/features/sleep_detection/test/*.parquet\", data_paths=\"/kaggle/input/child-mind-institute-problematic-internet-use/series_test.parquet\", train_or_test=\"test\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T19:19:55.899066Z","iopub.status.idle":"2024-12-23T19:19:55.899403Z","shell.execute_reply":"2024-12-23T19:19:55.899220Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_sleep = pd.read_csv(\"/kaggle/input/train-sleep/sleep_features.csv\")\ntest_sleep = pd.read_csv(\"/kaggle/working/features/test/sleep_features.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T19:19:55.900288Z","iopub.status.idle":"2024-12-23T19:19:55.900570Z","shell.execute_reply":"2024-12-23T19:19:55.900463Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sleep_cols = train_sleep.columns.tolist()\nsleep_cols.remove(\"id\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T19:19:55.901862Z","iopub.status.idle":"2024-12-23T19:19:55.902133Z","shell.execute_reply":"2024-12-23T19:19:55.902033Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"rm -rf /kaggle/working/features","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T19:19:55.902616Z","iopub.status.idle":"2024-12-23T19:19:55.902880Z","shell.execute_reply":"2024-12-23T19:19:55.902782Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"rm -rf /kaggle/working/heuristic_features","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T19:19:55.903817Z","iopub.status.idle":"2024-12-23T19:19:55.904222Z","shell.execute_reply":"2024-12-23T19:19:55.904100Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_sub3 = pd.merge(train, train_sleep, how=\"left\", on='id')\ntest_sub3 = pd.merge(test, test_sleep, how=\"left\", on='id')\ntrain_season_cols = [col for col in train_sub3.columns if 'Season' in col]\ntest_season_cols = [col for col in test_sub3.columns if 'Season' in col]\n\ntrain_sub3 = train_sub3.drop(train_season_cols, axis=1) \ntest_sub3 = test_sub3.drop(test_season_cols, axis=1) \n\ntrain_sub3 = train_sub3.drop('id', axis=1)\ntest_sub3  = test_sub3.drop('id', axis=1)  \ntrain_sub3 = train_sub3.dropna(subset='sii')","metadata":{"execution":{"iopub.status.busy":"2024-12-23T19:19:55.905087Z","iopub.status.idle":"2024-12-23T19:19:55.905460Z","shell.execute_reply":"2024-12-23T19:19:55.905338Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 3.3.3. Trainng","metadata":{}},{"cell_type":"code","source":"feature_cols = test_sub3.columns\ny = train_sub3['sii']\nX = train_sub3[feature_cols]\ntest_sub3 = test_sub3[feature_cols]\n\nsubmission3, _, _, _, _= TrainML(xgb_model, X, y, test_sub3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T19:19:55.906203Z","iopub.status.idle":"2024-12-23T19:19:55.906481Z","shell.execute_reply":"2024-12-23T19:19:55.906371Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission33, _, _, _, _= TrainML(final_voting_model, X, y, test_sub3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T19:19:55.907050Z","iopub.status.idle":"2024-12-23T19:19:55.907320Z","shell.execute_reply":"2024-12-23T19:19:55.907216Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3.4. Combine 3 and 4","metadata":{}},{"cell_type":"code","source":"train_sub4 = pd.merge(train, train_sleep, how=\"left\", on='id')\ntest_sub4 = pd.merge(test, test_sleep, how=\"left\", on='id')\ntrain_sub4 = pd.merge(train, train_ts_encoded, how=\"left\", on='id')\ntest_sub4 = pd.merge(test, test_ts_encoded, how=\"left\", on='id')\ntrain_season_cols = [col for col in train_sub4.columns if 'Season' in col]\ntest_season_cols = [col for col in test_sub4.columns if 'Season' in col]\n\ntrain_sub4 = train_sub4.drop(train_season_cols, axis=1) \ntest_sub4 = test_sub4.drop(test_season_cols, axis=1) \n\ntrain_sub4 = train_sub4.drop('id', axis=1)\ntest_sub4  = test_sub4.drop('id', axis=1)  \ntrain_sub4 = train_sub4.dropna(subset='sii')","metadata":{"execution":{"iopub.status.busy":"2024-12-23T19:19:55.908120Z","iopub.status.idle":"2024-12-23T19:19:55.908393Z","shell.execute_reply":"2024-12-23T19:19:55.908264Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"feature_cols = test_sub4.columns\ny = train_sub4['sii']\nX = train_sub4[feature_cols]\ntest_sub4 = test_sub4[feature_cols]\n\nsubmission4, _, _, _, _= TrainML(xgb_model, X, y, test_sub4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T19:19:55.909016Z","iopub.status.idle":"2024-12-23T19:19:55.909334Z","shell.execute_reply":"2024-12-23T19:19:55.909166Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission44, _, _, _, _= TrainML(xgb_model, X, y, test_sub44)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T19:19:55.909913Z","iopub.status.idle":"2024-12-23T19:19:55.910144Z","shell.execute_reply":"2024-12-23T19:19:55.910049Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"markdown","source":"Need select submission 1 - 4 to submit","metadata":{}},{"cell_type":"code","source":"final_submission = submission2\nfinal_submission.to_csv('submission.csv', index=False)\n\nprint(\"Submission saved to 'submission.csv'\")","metadata":{"execution":{"iopub.status.busy":"2024-12-23T19:19:55.910936Z","iopub.status.idle":"2024-12-23T19:19:55.911163Z","shell.execute_reply":"2024-12-23T19:19:55.911071Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"final_submission","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T19:19:55.911682Z","iopub.status.idle":"2024-12-23T19:19:55.911945Z","shell.execute_reply":"2024-12-23T19:19:55.911825Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}