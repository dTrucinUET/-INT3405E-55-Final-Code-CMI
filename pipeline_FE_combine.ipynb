{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":81933,"databundleVersionId":9643020,"sourceType":"competition"},{"sourceId":7453542,"sourceType":"datasetVersion","datasetId":921302},{"sourceId":9448132,"sourceType":"datasetVersion","datasetId":5742470},{"sourceId":9448136,"sourceType":"datasetVersion","datasetId":5742473},{"sourceId":10229352,"sourceType":"datasetVersion","datasetId":6324642},{"sourceId":10233697,"sourceType":"datasetVersion","datasetId":6327724}],"dockerImageVersionId":30823,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport re\nimport copy\nimport pickle\nfrom sklearn.base import clone\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom scipy.optimize import minimize\nfrom concurrent.futures import ThreadPoolExecutor\nfrom tqdm import tqdm\nimport polars as pl\nimport polars.selectors as cs\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import MaxNLocator, FormatStrFormatter, PercentFormatter\nimport seaborn as sns\n\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nfrom keras.models import Model\nfrom keras.layers import Input, Dense\nfrom keras.optimizers import Adam\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nfrom colorama import Fore, Style\nfrom IPython.display import clear_output\nimport warnings\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import VotingRegressor, RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.impute import SimpleImputer, KNNImputer\nfrom sklearn.pipeline import Pipeline\nimport plotly.express as px\n\nimport sys\nfrom glob import glob\nfrom pathlib import Path\nimport polars as pl\nimport datetime\nimport yaml\nimport gc\nimport random \nwarnings.filterwarnings('ignore')\npd.options.display.max_columns = None","metadata":{"execution":{"iopub.status.busy":"2024-12-24T02:27:10.029942Z","iopub.execute_input":"2024-12-24T02:27:10.030219Z","iopub.status.idle":"2024-12-24T02:27:29.393292Z","shell.execute_reply.started":"2024-12-24T02:27:10.030185Z","shell.execute_reply":"2024-12-24T02:27:29.392631Z"},"trusted":true},"outputs":[],"execution_count":1},{"cell_type":"code","source":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nSEED = 42\nn_splits = 5\nseed_everything(SEED)","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2024-12-24T02:27:29.394087Z","iopub.execute_input":"2024-12-24T02:27:29.394878Z","iopub.status.idle":"2024-12-24T02:27:29.404061Z","shell.execute_reply.started":"2024-12-24T02:27:29.394853Z","shell.execute_reply":"2024-12-24T02:27:29.403362Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# Define function","metadata":{}},{"cell_type":"markdown","source":"## Feature engineer for tabular data","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest, f_regression\n\ndef feature_engineering_v2(df, selector=None, imputer=None, fit=True):\n    df = df.loc[:, ~df.columns.duplicated()]\n    if fit: \n        y = df['sii']\n\n    numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns\n    season_cols = [col for col in df.columns if 'Season' in col]\n    pciat_cols = [col for col in df.columns if 'PCIAT' in col and 'Season' not in col]\n    remaining_numeric_cols = [col for col in numeric_cols if col not in pciat_cols and col not in ['sii']]\n    X = df[remaining_numeric_cols]\n    print(X.keys())\n    if np.any(np.isinf(X)):\n        X = X.replace([np.inf, -np.inf], np.nan)\n    if fit: \n        imputer = SimpleImputer()\n        imputed_data = imputer.fit_transform(X)\n        train_imputed = pd.DataFrame(imputed_data, columns=remaining_numeric_cols)\n        X = train_imputed\n    else:\n        X = imputer.transform(X)\n\n    if fit:\n        selector = SelectKBest(score_func=f_regression, k=30)\n        X_new = selector.fit_transform(X, y)\n        selected_features = X.columns[selector.get_support()]\n    else: \n        X_new = selector.transform(X)\n        selected_features = [col for col, selected in zip(remaining_numeric_cols, selector.get_support()) if selected]\n    df_selected = pd.DataFrame(X_new, columns=selected_features)\n    print(df_selected.keys())\n    return df_selected, selector, imputer\n\ndef feature_engineering_tabular(df):\n    season_cols = [col for col in df.columns if 'Season' in col]\n    df = df.drop(season_cols, axis=1) \n    df['BMI_Age'] = df['Physical-BMI'] * df['Basic_Demos-Age']\n    df['Internet_Hours_Age'] = df['PreInt_EduHx-computerinternet_hoursday'] * df['Basic_Demos-Age']\n    df['BMI_Internet_Hours'] = df['Physical-BMI'] * df['PreInt_EduHx-computerinternet_hoursday']\n    df['BFP_BMI'] = df['BIA-BIA_Fat'] / df['BIA-BIA_BMI']\n    df['FFMI_BFP'] = df['BIA-BIA_FFMI'] / df['BIA-BIA_Fat']\n    df['FMI_BFP'] = df['BIA-BIA_FMI'] / df['BIA-BIA_Fat']\n    df['LST_TBW'] = df['BIA-BIA_LST'] / df['BIA-BIA_TBW']\n    df['BFP_BMR'] = df['BIA-BIA_Fat'] * df['BIA-BIA_BMR']\n    df['BFP_DEE'] = df['BIA-BIA_Fat'] * df['BIA-BIA_DEE']\n    df['BMR_Weight'] = df['BIA-BIA_BMR'] / df['Physical-Weight']\n    df['DEE_Weight'] = df['BIA-BIA_DEE'] / df['Physical-Weight']\n    df['SMM_Height'] = df['BIA-BIA_SMM'] / df['Physical-Height']\n    df['Muscle_to_Fat'] = df['BIA-BIA_SMM'] / df['BIA-BIA_FMI']\n    df['Hydration_Status'] = df['BIA-BIA_TBW'] / df['Physical-Weight']\n    df['ICW_TBW'] = df['BIA-BIA_ICW'] / df['BIA-BIA_TBW']\n    df['BMI_PHR'] = df['Physical-BMI'] * df['Physical-HeartRate']\n    \n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T02:27:29.404836Z","iopub.execute_input":"2024-12-24T02:27:29.405049Z","iopub.status.idle":"2024-12-24T02:27:29.446153Z","shell.execute_reply.started":"2024-12-24T02:27:29.405028Z","shell.execute_reply":"2024-12-24T02:27:29.445311Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"## Feature engineer for time series data","metadata":{}},{"cell_type":"markdown","source":"### Method 1: Autoencoder ","metadata":{}},{"cell_type":"code","source":"class AutoEncoder(nn.Module):\n    def __init__(self, input_dim, encoding_dim):\n        super(AutoEncoder, self).__init__()\n        self.encoder = nn.Sequential(\n            nn.Linear(input_dim, encoding_dim*3),\n            nn.ReLU(),\n            nn.Linear(encoding_dim*3, encoding_dim*2),\n            nn.ReLU(),\n            nn.Linear(encoding_dim*2, encoding_dim),\n            nn.ReLU()\n        )\n        self.decoder = nn.Sequential(\n            nn.Linear(encoding_dim, encoding_dim*2),\n            nn.ReLU(),\n            nn.Linear(encoding_dim*2, encoding_dim*3),\n            nn.ReLU(),\n            nn.Linear(encoding_dim*3, input_dim),\n            nn.Sigmoid()\n        )\n\n        \n    def forward(self, x):\n        encoded = self.encoder(x)\n        decoded = self.decoder(encoded)\n        return decoded\n\n\ndef perform_autoencoder(df, encoding_dim=50, epochs=50, batch_size=32):\n    scaler = StandardScaler()\n    df_scaled = scaler.fit_transform(df)\n    \n    data_tensor = torch.FloatTensor(df_scaled)\n    \n    input_dim = data_tensor.shape[1]\n    autoencoder = AutoEncoder(input_dim, encoding_dim)\n    \n    criterion = nn.MSELoss()\n    optimizer = optim.Adam(autoencoder.parameters())\n    \n    for epoch in range(epochs):\n        for i in range(0, len(data_tensor), batch_size):\n            batch = data_tensor[i : i + batch_size]\n            optimizer.zero_grad()\n            reconstructed = autoencoder(batch)\n            loss = criterion(reconstructed, batch)\n            loss.backward()\n            optimizer.step()\n            \n        if (epoch + 1) % 50 == 0:\n            print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}]')    \n    return autoencoder, scaler\n\ndef encode_data(autoencoder, scaler, df):\n    df_scaled = scaler.transform(df)\n    data_tensor = torch.FloatTensor(df_scaled)\n    with torch.no_grad():\n        encoded_data = autoencoder.encoder(data_tensor).numpy()\n\n    df_encoded = pd.DataFrame(encoded_data, columns=[f'Enc_{i + 1}' for i in range(encoded_data.shape[1])])\n    return df_encoded","metadata":{"execution":{"iopub.status.busy":"2024-12-24T02:27:29.447712Z","iopub.execute_input":"2024-12-24T02:27:29.447942Z","iopub.status.idle":"2024-12-24T02:27:29.456143Z","shell.execute_reply.started":"2024-12-24T02:27:29.447921Z","shell.execute_reply":"2024-12-24T02:27:29.455311Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"### Method 2: External knowledge\n\nFirst, prepare the features used in my sleep detection model. Please refer to the implementation by [@tatamikenn](https://www.kaggle.com/tatamikenn) [here](https://www.kaggle.com/code/tatamikenn/sleep-hdcza-a-pure-heuristic-approach-lb-0-447).\n\nThis pipeline processes accelerometer data for sleep detection, utilizing time-series datasets. It generates features to identify sleep episodes, static periods, and motion patterns, inspired by @tatamikenn's implementation.\n\n-  `transform`: this function processes input data to generate features for analysis. It breaks down the timestamp into components like year, month, day, hour, and weekday. It also groups data by night, adjusting the timestamp if necessary, and creates a unique `night_group` identifier for each night. Additionally, a cumulative step count (norm_step) is computed for each group to facilitate sequential analysis.\n\n- `transform_series`: this function enhances the transform function by adding a new feature: detecting clipped ENMO values. It flags instances where the enmo (motion metric) is zero, marking potential data quality issues.\n\n- `transform_events`: this function processes event data by adding a night column and pivoting the data. The events are rearranged by `series_id`, `group_id`, and night to simplify time-series analysis.\n\n- `add_feature`: this function generates advanced features for sleep detection, including:\n    - Difference Features: Computes the differences in anglez (angular motion) and enmo (motion magnitude).\n    - Rolling Median: Calculates rolling medians of anglez_diff and enmo_diff over a 5-minute window.\n    - Critical Threshold: Determines static periods by evaluating anglez_diff variability over a day.\n    - Static and Sleep Blocks: Flags periods with minimal motion (is_static) and identifies sleep blocks over 30-minute windows.\n    - Sleep Episodes: Detects continuous sleep episodes, identifies the longest one, and flags interruptions in sleep.\n\n-  `create_heuristic`: the main function processes raw data files by converting timestamps and applying transformations. It calls the transform_series function to prepare the data and the add_feature function to generate sleep-related features. Finally, it saves the processed data into .parquet files for further analysis.","metadata":{}},{"cell_type":"code","source":"MAX_FILE = 2000","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T02:27:29.457369Z","iopub.execute_input":"2024-12-24T02:27:29.457651Z","iopub.status.idle":"2024-12-24T02:27:29.473381Z","shell.execute_reply.started":"2024-12-24T02:27:29.457630Z","shell.execute_reply":"2024-12-24T02:27:29.472529Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def transform(df, night_offset=20):\n    return (\n        df.with_columns(\n            [\n                (pl.col(\"timestamp\").dt.year() - 2000).cast(pl.Int8).alias(\"year\"),\n                pl.col(\"timestamp\").dt.month().cast(pl.Int8).alias(\"month\"),\n                pl.col(\"timestamp\").dt.day().cast(pl.Int8).alias(\"day\"),\n                pl.col(\"timestamp\").dt.hour().cast(pl.Int8).alias(\"hour\"),\n                pl.col(\"timestamp\").dt.minute().cast(pl.Int8).alias(\"minute\"),\n                pl.col(\"timestamp\").dt.second().cast(pl.Int8).alias(\"second\"),\n                pl.col(\"timestamp\").dt.weekday().cast(pl.Int8).alias(\"weekday\"),\n            ]\n        )\n        .with_columns( \n            pl.when(pl.col(\"hour\") < night_offset)\n            .then(pl.col(\"timestamp\"))\n            .otherwise(pl.col(\"timestamp\") + pl.duration(days=1))\n            .dt.date()\n            .alias(\"night_group\"),\n        )\n        .with_columns(\n            [\n                (\n                    pl.col(\"series_id\") + pl.lit(\"_\") + pl.col(\"night_group\").cast(pl.Datetime).dt.strftime(\"%Y%m%d\")\n                ).alias(\"group_id\"),\n            ]\n        )\n        .with_columns(\n            [\n                pl.col(\"timestamp\").cum_count().over(\"group_id\").alias(\"norm_step\"),\n            ]\n        )\n        .drop([\"night_group\"])\n    )\n\n\ndef transform_series(df):\n    return transform(df).with_columns(\n        [\n            (pl.col(\"enmo\") == 0).alias(\"is_enmo_clipped\"),\n        ]\n    )\n\n\ndef transform_events(df):\n    return (\n        transform(df)\n        .with_columns(\n            [\n                pl.col(\"night\").cast(pl.UInt32).alias(\"night\"),\n            ]\n        )\n        .pivot([\"step\", \"timestamp\", \"tz_offset\"], [\"series_id\", \"group_id\", \"night\"], \"event\")\n    )\n\n\ndef add_feature(\n    df,\n    day_group_col=\"group_id\",\n    term1=(5 * 60) // 5,\n    term2=(30 * 60) // 5,\n    term3=(60 * 60) // 5,\n    min_threshold=0.005,\n    max_threshold=0.04,\n    center=True,\n):\n    return (\n        df.with_columns(\n            [\n                pl.col(\"anglez\").diff(1).abs().alias(\"anglez_diff\"),\n                pl.col(\"enmo\").diff(1).abs().alias(\"enmo_diff\"),\n            ]\n        )\n        .with_columns(\n            [\n                pl.col(\"anglez_diff\")\n                .rolling_median(term1, center=center)  # 5 min window\n                .alias(\"anglez_diff_median_5min\"),\n                pl.col(\"enmo_diff\")\n                .rolling_median(term1, center=center)  # 5 min window\n                .alias(\"enmo_diff_median_5min\"),\n            ]\n        )\n        .with_columns(\n            [\n                pl.col(\"anglez_diff_median_5min\")\n                .quantile(0.1)\n                .clip(min_threshold, max_threshold)\n                .over(day_group_col)\n                .alias(\"critical_threshold\")\n            ]\n        )\n        .with_columns([(pl.col(\"anglez_diff_median_5min\") < pl.col(\"critical_threshold\") * 15).alias(\"is_static\")])\n        .with_columns(\n            [\n                pl.col(\"is_static\").cast(pl.Int32).rolling_sum(term2, center=center).alias(\"is_static_sum_30min\"),\n            ]\n        )\n        .with_columns([(pl.col(\"is_static_sum_30min\") == ((30 * 60) // 5)).alias(\"tmp\")])\n        .with_columns(\n            [\n                pl.col(\"tmp\").shift(term2 // 2).alias(\"tmp_left\"),\n                pl.col(\"tmp\").shift(-(term2 // 2)).alias(\"tmp_right\"),\n            ]\n        )\n        .with_columns(\n            [\n                (pl.col(\"tmp_left\") | pl.col(\"tmp_right\")).alias(\"is_sleep_block\"),\n            ]\n        )\n        .drop([\"tmp\", \"tmp_left\", \"tmp_right\"])\n        .with_columns([pl.col(\"is_sleep_block\").not_().alias(\"is_gap\")])\n        .with_columns([pl.col(\"is_gap\").cast(pl.Int32).rolling_sum(term3, center=center).alias(\"gap_length\")])\n        .with_columns([(pl.col(\"gap_length\") == term3).alias(\"tmp\")])\n        .with_columns(\n            [\n                pl.col(\"tmp\").shift(term3 // 2).alias(\"tmp_left\"),\n                pl.col(\"tmp\").shift(-(term3 // 2)).alias(\"tmp_right\"),\n            ]\n        )\n        .with_columns(\n            [\n                (pl.col(\"tmp_left\") | pl.col(\"tmp_right\")).alias(\"is_large_gap\"),\n            ]\n        )\n        .drop([\"tmp\", \"tmp_left\", \"tmp_right\"])\n        .with_columns([pl.col(\"is_large_gap\").not_().alias(\"is_sleep_episode\")])\n        #\n        # extract longest sleep episode\n        #\n        .with_columns(\n            [\n                # extract false->true transition\n                (\n                    (\n                        pl.col(\"is_sleep_episode\")\n                        & pl.col(\"is_sleep_episode\").shift(1, fill_value=pl.lit(False)).not_()\n                    )\n                    .cum_sum()\n                    .over(\"group_id\")\n                ).alias(\"sleep_episode_id\")\n            ]\n        )\n        .with_columns(\n            [pl.col(\"is_sleep_episode\").sum().over([\"group_id\", \"sleep_episode_id\"]).alias(\"sleep_episode_length\")]\n        )\n        .with_columns([pl.col(\"sleep_episode_length\").max().over([\"group_id\"]).alias(\"max_sleep_episode_length\")])\n        .with_columns(\n            [\n                (\n                    pl.col(\"is_sleep_episode\") & (pl.col(\"sleep_episode_length\") == pl.col(\"max_sleep_episode_length\"))\n                ).alias(\"is_longest_sleep_episode\")\n            ]\n        )\n    )\n\n\nuse_columns = [\n    \"series_id\",\n    \"step\",\n    \"is_longest_sleep_episode\",\n    \"is_sleep_block\",\n    \"is_gap\",\n    \"is_large_gap\",\n    \"is_sleep_episode\",\n    \"is_static\",\n]\n\ndef create_heuristic(paths, train_or_test):\n    i = 0\n    for path in tqdm(paths):\n        i += 1\n        if (i == MAX_FILE):\n            break\n        sdf = pl.read_parquet(path)\n    \n        # dummy timestamp\n        sdf = sdf.with_columns((pl.col(\"time_of_day\") == 0).cast(pl.Int32).cum_sum().alias(\"day_offset\"))\n        sdf = sdf.with_columns(\n            (\n                datetime.datetime(2020, 1, 1)\n                + (pl.col(\"day_offset\") * 86400_000_000 + pl.col(\"time_of_day\") / 1000).cast(pl.Duration(\"us\"))\n            ).alias(\"timestamp\")\n        )\n    \n        sdf = sdf.with_columns(pl.lit(path.split(\"/\")[-2]).alias(\"series_id\"))\n        sdf = sdf.sort(\"step\")\n        sdf = transform_series(sdf)\n        sdf = add_feature(sdf)\n        sdf = sdf[use_columns].fill_null(False)\n    \n        sidf = path.split(\"/\")[-2]\n        save_path = f\"/kaggle/working/heuristic_features/{train_or_test}/{sidf}.parquet\"\n        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n        sdf.write_parquet(save_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T02:27:29.474041Z","iopub.execute_input":"2024-12-24T02:27:29.474268Z","iopub.status.idle":"2024-12-24T02:27:29.493367Z","shell.execute_reply.started":"2024-12-24T02:27:29.474250Z","shell.execute_reply":"2024-12-24T02:27:29.492541Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"if True:\n    sys.path.append(\"/kaggle/input/cmi-2023-src\")\n    from consts import ANGLEZ_MEAN, ANGLEZ_STD, ENMO_MEAN, ENMO_STD\n    from torch_models.dataset import ZzzPatchDataset\n    from torch_models.models import ZzzConv1dGRUModel, ZzzTransformerGRUModel, ZzzWaveGRUModel\n\n    from utils.feature_contena import Features\n    from utils.lightning_utils import MyLightningDataModule, MyLightningModule\n    from utils.set_seed import seed_base_torch\n    from utils.torch_template import EnsembleModel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T02:27:29.494226Z","iopub.execute_input":"2024-12-24T02:27:29.494544Z","iopub.status.idle":"2024-12-24T02:27:32.641351Z","shell.execute_reply.started":"2024-12-24T02:27:29.494514Z","shell.execute_reply":"2024-12-24T02:27:32.640529Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def detection(paths=f\"/kaggle/input/child-mind-institute-problematic-internet-use/series_train.parquet/id=*/part-0.parquet\", train_or_test=\"train\"):\n    MODEL_NAME = \"patch_transformer_gru\"\n    \n    PACKAGE_DIR = Path(\"/kaggle/input/cmi-2023-src\")\n    CFG = yaml.safe_load(open(PACKAGE_DIR / \"config.yaml\", \"r\"))\n    BLOCK_SIZE = CFG[MODEL_NAME][\"execution\"][\"block_size\"]\n    \n    CFG[\"output_dir\"] = f\"/kaggle/input/cmi-2023-output/{CFG[MODEL_NAME]['execution']['best_exp_id']}\"\n    \n    seed_base_torch(CFG[\"env\"][\"seed\"])\n    \n    DEVICE = \"cuda\"\n    \n    files = glob(\n        paths\n    )\n    \n    features = Features()\n    features.add_num_features([\"anglez\", \"enmo\"])\n    features.add_num_features([\"anglez_diff\", \"enmo_diff\"])\n    features.add_num_features([\"same_count\"])\n    features.add_num_features([\"large_diff_count\"])\n    features.add_num_features([\"same_count_shift_plus\", \"same_count_shift_minus\"])\n    features.add_num_features([\"is_longest_sleep_episode\", \"is_sleep_block\"])\n    \n    # transformer + gru\n    model = ZzzTransformerGRUModel(\n        max_len=BLOCK_SIZE // CFG[MODEL_NAME][\"execution\"][\"patch_size\"],\n        input_numerical_size=len(features.all_features()) * CFG[MODEL_NAME][\"execution\"][\"patch_size\"],\n        **CFG[MODEL_NAME][\"params\"],\n    )\n    trn_models = [\n        MyLightningModule.load_from_checkpoint(\n            os.path.join(\"/kaggle/input/cmi-2023-output/exp_160\", f\"logs/best_model_fold{fold}.ckpt\"),\n            model=model,\n            map_location=torch.device(DEVICE),\n        ).to(DEVICE)\n        for fold in range(5 if len(files) > 100 else 1)\n    ]\n    \n    models = trn_models\n    model = EnsembleModel(models).to(DEVICE)\n    model.eval()\n    \n    all_oof_dfs = []\n    i = 0\n    for file in tqdm(files):\n        # load file\n        i += 1\n        if (i == MAX_FILE):\n            break\n        df = pd.read_parquet(file)\n        if len(df) < BLOCK_SIZE:\n            continue\n        time_of_days = df[\"time_of_day\"].values\n    \n        # same_count\n        DAY_STEPS = 12 * 60 * 24\n        n_days = int(len(df) // DAY_STEPS) + 1\n        df[\"same_count\"] = 0\n        for day in range(-n_days, n_days + 1):\n            if day == 0:\n                continue\n            df[\"_anglez_diff\"] = df[\"anglez\"].diff(DAY_STEPS * day)\n            df[\"_anglez_diff\"] = df[\"_anglez_diff\"].fillna(1)\n            df[\"same_count\"] += (df[\"_anglez_diff\"] == 0).astype(int)\n        df[\"same_count\"] = (df[\"same_count\"].clip(0, 5) - 2.5) / 2.5\n    \n        SHIFT_STEPS = 12 * 60 * 6  # 6h\n        df[\"same_count_shift_plus\"] = df[\"same_count\"].shift(SHIFT_STEPS).fillna(1.0).astype(np.float16)\n        df[\"same_count_shift_minus\"] = df[\"same_count\"].shift(-SHIFT_STEPS).fillna(1.0).astype(np.float16)\n    \n        # features\n        df[\"anglez_diffabs\"] = df[\"anglez\"].diff().abs().fillna(0)\n        df[\"large_diff\"] = (df[\"anglez_diffabs\"] > 5).astype(int)\n        df[\"large_diff_count\"] = df[\"large_diff\"].rolling(10, center=True).mean().fillna(0)\n        df[\"large_diff_count\"] = (df[\"large_diff_count\"] - 0.5) * 2\n    \n        # normalize\n        df[\"anglez\"] = (df[\"anglez\"] - ANGLEZ_MEAN) / ANGLEZ_STD\n        df[\"enmo\"] = (df[\"enmo\"] - ENMO_MEAN) / ENMO_STD\n        df[\"anglez_diff\"] = df[\"anglez\"].diff().fillna(0)\n        df[\"enmo_diff\"] = df[\"enmo\"].diff().fillna(0)\n    \n        # heuristic_features by @bilzard\n        sid = file.split(\"/\")[-2]\n        df[\"series_id\"] = sid\n        path = f\"/kaggle/working/heuristic_features/{train_or_test}/{sid}.parquet\"\n        hdf = pd.read_parquet(path)\n        df = pd.concat([df, hdf.drop(columns=[\"series_id\", \"step\"])], axis=1)\n        df[[\"is_longest_sleep_episode\", \"is_sleep_block\"]] = df[[\"is_longest_sleep_episode\", \"is_sleep_block\"]] * 2 - 1\n    \n        # split\n        dfs = []\n        df = df.sort_values(\"step\").reset_index(drop=True)\n        for start in range(0, len(df), BLOCK_SIZE // 8):\n            end = start + BLOCK_SIZE\n            if end > len(df):\n                end = len(df) - len(df) % CFG[MODEL_NAME][\"execution\"][\"patch_size\"]\n                start = end - BLOCK_SIZE\n                assert start >= 0\n            assert df.iloc[start][\"step\"] % CFG[MODEL_NAME][\"execution\"][\"patch_size\"] == 0\n            dfs.append(df.iloc[start:end])\n        gc.collect()\n    \n        # inference\n        train_dataset = ZzzPatchDataset(\n            dfs, mode=\"test\", features=features, patch_size=CFG[MODEL_NAME][\"execution\"][\"patch_size\"]\n        )\n        valid_dataset = ZzzPatchDataset(\n            dfs, mode=\"test\", features=features, patch_size=CFG[MODEL_NAME][\"execution\"][\"patch_size\"]\n        )\n        data_module = MyLightningDataModule(train_dataset, valid_dataset, batch_size=64)\n        preds = []\n        with torch.no_grad():\n            for X in data_module.val_dataloader():\n                pred = torch.sigmoid(model(X.to(\"cuda\"))).detach().cpu().numpy() * 10\n                preds.append(pred)\n    \n        oof_dfs = []\n        for pred, df in zip(np.vstack(preds), dfs):\n            df = df.iloc[\n                CFG[MODEL_NAME][\"execution\"][\"patch_size\"] // 2 : len(df) : CFG[MODEL_NAME][\"execution\"][\"patch_size\"]\n            ].reset_index(drop=True)\n            df[[\"wakeup_oof\", \"onset_oof\"]] = pred\n            oof_dfs.append(df[[\"series_id\", \"step\", \"wakeup_oof\", \"onset_oof\"]])\n    \n        oof_df = pd.concat(oof_dfs)\n        oof_df = oof_df.groupby([\"series_id\", \"step\"]).mean().reset_index().sort_values([\"series_id\", \"step\"])\n        oof_df = oof_df[[\"series_id\", \"step\", \"wakeup_oof\", \"onset_oof\"]]\n        oof_df[\"step\"] = oof_df[\"step\"].astype(int)\n    \n        del preds, oof_dfs\n        gc.collect()\n    \n        train = oof_df.reset_index(drop=True)\n        train[\"time_of_day\"] = time_of_days[\n            CFG[MODEL_NAME][\"execution\"][\"patch_size\"] // 2 :: CFG[MODEL_NAME][\"execution\"][\"patch_size\"]\n        ][: len(train)]\n        all_oof_dfs.append(train[[\"series_id\", \"step\", \"wakeup_oof\", \"onset_oof\", \"time_of_day\"]])\n        # del dfs, df\n        gc.collect()\n\n    # save\n    for df in tqdm(all_oof_dfs):\n        save_path = f\"/kaggle/working/features/sleep_detection/{train_or_test}/{df['series_id'].iloc[0]}.parquet\"\n        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n        df.to_parquet(save_path, index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T02:27:32.642246Z","iopub.execute_input":"2024-12-24T02:27:32.643039Z","iopub.status.idle":"2024-12-24T02:27:32.672289Z","shell.execute_reply.started":"2024-12-24T02:27:32.643007Z","shell.execute_reply":"2024-12-24T02:27:32.671119Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"time_of_day_max = 86400000000000","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T02:27:32.673122Z","iopub.execute_input":"2024-12-24T02:27:32.673418Z","iopub.status.idle":"2024-12-24T02:27:32.715745Z","shell.execute_reply.started":"2024-12-24T02:27:32.673391Z","shell.execute_reply":"2024-12-24T02:27:32.715021Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def feature_engineering_sleep_detect(paths=\"/kaggle/working/features/sleep_detection/train/*.parquet\", data_paths=\"/kaggle/input/child-mind-institute-problematic-internet-use/series_train.parquet\", train_or_test=\"train\"):\n    features = []\n    debug_count = 0\n    all_files = sorted(glob(paths))\n    i = 0\n    for file in tqdm(all_files):\n        i += 1\n        if (i == MAX_FILE):\n            break\n        df = pl.read_parquet(file)\n        df = df.with_columns(pl.col(\"step\").cast(pl.UInt32)).drop(\"time_of_day\")\n        sid = df[\"series_id\"][0]\n    \n        sensor_df = pl.read_parquet(\n            f\"{data_paths}/{sid}/part-0.parquet\"\n        ).with_columns((pl.col(\"time_of_day\") == 0).cum_sum().alias(\"day\"))\n    \n        feature = {\n            \"id\": sid,\n            \"length\": df.shape[0],\n            \"day\": sensor_df[\"relative_date_PCIAT\"].max() - sensor_df[\"relative_date_PCIAT\"].min(),\n        }\n    \n        # skip if time step is not 5sec\n        diffs = sensor_df[\"time_of_day\"].diff().drop_nulls().unique()\n        if set(diffs) != set([-86395000000000, 5000000000]):\n            features.append(feature)\n            continue\n    \n        sensor_df = (\n            sensor_df.join(df, on=\"step\", how=\"left\")\n            .sort(\"step\")\n            .with_columns(\n                pl.col(\"onset_oof\").interpolate(),\n                pl.col(\"wakeup_oof\").interpolate(),\n            )\n        )\n    \n        # onset = 15:00~3:00, wakeup = 3:00~15:00\n        onset_start = time_of_day_max / 24 * 15  # 15:00\n        onset_end = time_of_day_max / 24 * 3  # 3:00\n        sensor_df = sensor_df.with_columns(\n            ((pl.col(\"time_of_day\") > onset_start) | (pl.col(\"time_of_day\") < onset_end)).alias(\"onset_duration\"),\n        ).with_columns(\n            pl.col(\"onset_duration\").cast(pl.Int32).diff().fill_null(0).abs().cum_sum().alias(\"onset_wakeup_duration\")\n        )\n    \n        # get sleep period\n        sleep_info = []\n        for _, df in sensor_df.group_by(\"onset_wakeup_duration\", maintain_order=True):\n            is_onset = df[\"onset_duration\"][0]\n            if is_onset:\n                max_idx = df[\"onset_oof\"].arg_max()\n                if max_idx is None:\n                    continue\n                max_score = df[\"onset_oof\"][max_idx]\n                step = df[\"step\"][max_idx]\n    \n                # date\n                start_time = df[\"time_of_day\"][0] / time_of_day_max * 24\n                if start_time >= 15:\n                    day = df[\"day\"][0]\n                    week_day = df[\"weekday\"][0]\n                else:\n                    day = df[\"day\"][0] - 1\n                    week_day = df[\"weekday\"][0] - 1\n                    if week_day == 0:\n                        week_day = 7\n            else:\n                max_idx = df[\"wakeup_oof\"].arg_max()\n                if max_idx is None:\n                    continue\n                max_score = df[\"wakeup_oof\"][max_idx]\n                step = df[\"step\"][max_idx]\n    \n                # date\n                start_time = df[\"time_of_day\"][0] / time_of_day_max * 24\n                day = df[\"day\"][0] - 1\n                week_day = df[\"weekday\"][0] - 1\n    \n            info = {\n                \"day\": day,\n                \"weekday\": week_day,\n                \"type\": \"onset\" if is_onset else \"wakeup\",\n                \"step\": step,\n                \"max_score\": max_score,\n                \"time\": df[\"time_of_day\"][max_idx] / time_of_day_max * 24,\n            }\n            sleep_info.append(info)\n        sleep_df = pl.DataFrame(sleep_info)\n    \n        # merge\n        sleep_df = (\n            sleep_df.filter(pl.col(\"type\") == \"onset\")\n            .drop(\"type\")\n            .rename(\n                {\n                    \"max_score\": \"onset_score\",\n                    \"step\": \"onset_step\",\n                    \"time\": \"onset_time\",\n                }\n            )\n            .join(\n                sleep_df.filter(pl.col(\"type\") == \"wakeup\")\n                .drop([\"type\", \"weekday\"])\n                .rename(\n                    {\n                        \"max_score\": \"wakeup_score\",\n                        \"step\": \"wakeup_step\",\n                        \"time\": \"wakeup_time\",\n                    }\n                ),\n                on=\"day\",\n            )\n        ).select(\n            [\"day\", \"weekday\", \"onset_time\", \"wakeup_time\", \"onset_step\", \"wakeup_step\", \"onset_score\", \"wakeup_score\"]\n        )\n    \n        # feature engineering\n        sleep_lengths = []  # wakeup - onset\n        sleep_enmo_mean = []  \n        sleep_enmo_std = []  \n        sleep_light_mean = []\n        sleep_light_std = [] \n        for i in range(len(sleep_df)):\n            # sleep period\n            start = sleep_df[\"onset_step\"][i]\n            end = sleep_df[\"wakeup_step\"][i]\n            if sleep_df[\"onset_score\"][i] < 1 or sleep_df[\"wakeup_score\"][i] < 1:\n                sleep_lengths.append(np.nan)\n                sleep_enmo_mean.append(np.nan)\n                sleep_enmo_std.append(np.nan)\n                sleep_light_mean.append(np.nan)\n                sleep_light_std.append(np.nan)\n                continue\n    \n            # sleep length\n            length = end - start\n            sleep_lengths.append(length * 5 / 60 / 60)  # hour\n    \n            # enmo\n            enmo_mean = sensor_df[\"enmo\"][start:end].mean()\n            enmo_std = sensor_df[\"enmo\"][start:end].std()\n            sleep_enmo_mean.append(enmo_mean)\n            sleep_enmo_std.append(enmo_std)\n    \n            # light\n            light_mean = sensor_df[\"light\"][start:end].mean()\n            light_std = sensor_df[\"light\"][start:end].std()\n            sleep_light_mean.append(light_mean)\n            sleep_light_std.append(light_std)\n            \n        sleep_df = sleep_df.with_columns(\n            pl.DataFrame(\n                {\n                    \"sleep_length\": sleep_lengths,\n                    \"sleep_enmo_mean\": sleep_enmo_mean,\n                    \"sleep_enmo_std\": sleep_enmo_std,\n                    \"sleep_light_mean\": sleep_light_mean,\n                    \"sleep_light_std\": sleep_light_std,\n                }\n            )\n        )\n        \n        # leave only high confidence periods\n        sleep_df = sleep_df.filter((pl.col(\"wakeup_score\") > 1) & (pl.col(\"onset_score\") > 1))\n        if debug_count < 3:\n            display(sleep_df.head())\n        debug_count += 1\n            \n    \n        # agg\n        feature.update(\n            {\n                \"sleep_measurement_count\": sleep_df.shape[0],\n                \"sleep_length_mean\": sleep_df[\"sleep_length\"].mean(),\n                \"sleep_length_std\": sleep_df[\"sleep_length\"].std(),\n                \"sleep_start_mean\": sleep_df[\"onset_time\"].mean(),\n                \"sleep_start_std\": sleep_df[\"onset_time\"].std(),\n                \"sleep_end_mean\": sleep_df[\"wakeup_time\"].mean(),\n                \"sleep_end_std\": sleep_df[\"wakeup_time\"].std(),\n                \"sleep_enmo_mean_mean\": sleep_df[\"sleep_enmo_mean\"].mean(),\n                \"sleep_enmo_mean_std\": sleep_df[\"sleep_enmo_mean\"].std(),\n                \"sleep_enmo_std_mean\": sleep_df[\"sleep_enmo_std\"].mean(),\n                \"sleep_enmo_std_std\": sleep_df[\"sleep_enmo_std\"].std(),\n                \"sleep_light_mean_mean\": sleep_df[\"sleep_light_mean\"].mean(),\n                \"sleep_light_mean_std\": sleep_df[\"sleep_light_mean\"].std(),\n                \"sleep_light_std_mean\": sleep_df[\"sleep_light_std\"].mean(),\n                \"sleep_light_std_std\": sleep_df[\"sleep_light_std\"].std(),\n            }\n        )\n        features.append(feature)\n    output_dir = f\"/kaggle/working/features/{train_or_test}\"\n    os.makedirs(output_dir, exist_ok=True)\n    feature_df = pl.DataFrame(features).with_columns(pl.col(\"id\").str.slice(3, 8))\n    feature_df.write_csv(f\"/kaggle/working/features/{train_or_test}/sleep_features.csv\")\n    print(feature_df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T02:27:32.716649Z","iopub.execute_input":"2024-12-24T02:27:32.716937Z","iopub.status.idle":"2024-12-24T02:27:32.736986Z","shell.execute_reply.started":"2024-12-24T02:27:32.716909Z","shell.execute_reply":"2024-12-24T02:27:32.736048Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"create_heuristic(paths=glob(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_test.parquet/id=*/part-0.parquet\"), train_or_test=\"test\")\ndetection(paths=\"/kaggle/input/child-mind-institute-problematic-internet-use/series_test.parquet/id=*/part-0.parquet\", train_or_test=\"test\")\nfeature_engineering_sleep_detect(paths=\"/kaggle/working/features/sleep_detection/test/*.parquet\", data_paths=\"/kaggle/input/child-mind-institute-problematic-internet-use/series_test.parquet\", train_or_test=\"test\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T02:27:32.737826Z","iopub.execute_input":"2024-12-24T02:27:32.738087Z","iopub.status.idle":"2024-12-24T02:27:40.175489Z","shell.execute_reply.started":"2024-12-24T02:27:32.738063Z","shell.execute_reply":"2024-12-24T02:27:40.174493Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 2/2 [00:00<00:00,  2.47it/s]\n100%|██████████| 2/2 [00:05<00:00,  2.64s/it]\n100%|██████████| 2/2 [00:00<00:00, 65.63it/s]\n  0%|          | 0/2 [00:00<?, ?it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"shape: (5, 13)\n┌─────┬─────────┬────────────┬─────────────┬───┬────────────┬────────────┬────────────┬────────────┐\n│ day ┆ weekday ┆ onset_time ┆ wakeup_time ┆ … ┆ sleep_enmo ┆ sleep_enmo ┆ sleep_ligh ┆ sleep_ligh │\n│ --- ┆ ---     ┆ ---        ┆ ---         ┆   ┆ _mean      ┆ _std       ┆ t_mean     ┆ t_std      │\n│ i64 ┆ i64     ┆ f64        ┆ f64         ┆   ┆ ---        ┆ ---        ┆ ---        ┆ ---        │\n│     ┆         ┆            ┆             ┆   ┆ f64        ┆ f64        ┆ f64        ┆ f64        │\n╞═════╪═════════╪════════════╪═════════════╪═══╪════════════╪════════════╪════════════╪════════════╡\n│ 0   ┆ 2       ┆ 22.091667  ┆ 7.041667    ┆ … ┆ 0.003588   ┆ 0.008487   ┆ 2.06153    ┆ 0.52031    │\n│ 1   ┆ 3       ┆ 22.641667  ┆ 8.141667    ┆ … ┆ 0.002427   ┆ 0.006132   ┆ 2.714605   ┆ 1.259364   │\n│ 2   ┆ 4       ┆ 21.575     ┆ 7.558333    ┆ … ┆ 0.003959   ┆ 0.007149   ┆ 6.441472   ┆ 2.73119    │\n│ 3   ┆ 5       ┆ 23.141667  ┆ 8.241667    ┆ … ┆ 0.006016   ┆ 0.007928   ┆ 9.246452   ┆ 14.259801  │\n│ 4   ┆ 6       ┆ 22.925     ┆ 7.008333    ┆ … ┆ 0.009862   ┆ 0.012524   ┆ 0.511077   ┆ 0.28813    │\n└─────┴─────────┴────────────┴─────────────┴───┴────────────┴────────────┴────────────┴────────────┘","text/html":"<div><style>\n.dataframe > thead > tr,\n.dataframe > tbody > tr {\n  text-align: right;\n  white-space: pre-wrap;\n}\n</style>\n<small>shape: (5, 13)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>day</th><th>weekday</th><th>onset_time</th><th>wakeup_time</th><th>onset_step</th><th>wakeup_step</th><th>onset_score</th><th>wakeup_score</th><th>sleep_length</th><th>sleep_enmo_mean</th><th>sleep_enmo_std</th><th>sleep_light_mean</th><th>sleep_light_std</th></tr><tr><td>i64</td><td>i64</td><td>f64</td><td>f64</td><td>i64</td><td>i64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>0</td><td>2</td><td>22.091667</td><td>7.041667</td><td>7854</td><td>14298</td><td>5.939998</td><td>6.883044</td><td>8.95</td><td>0.003588</td><td>0.008487</td><td>2.06153</td><td>0.52031</td></tr><tr><td>1</td><td>3</td><td>22.641667</td><td>8.141667</td><td>25530</td><td>32370</td><td>7.395478</td><td>2.857819</td><td>9.5</td><td>0.002427</td><td>0.006132</td><td>2.714605</td><td>1.259364</td></tr><tr><td>2</td><td>4</td><td>21.575</td><td>7.558333</td><td>42042</td><td>49230</td><td>5.715631</td><td>6.638568</td><td>9.983333</td><td>0.003959</td><td>0.007149</td><td>6.441472</td><td>2.73119</td></tr><tr><td>3</td><td>5</td><td>23.141667</td><td>8.241667</td><td>60450</td><td>67002</td><td>8.010484</td><td>3.987538</td><td>9.1</td><td>0.006016</td><td>0.007928</td><td>9.246452</td><td>14.259801</td></tr><tr><td>4</td><td>6</td><td>22.925</td><td>7.008333</td><td>77574</td><td>83394</td><td>8.050978</td><td>2.848315</td><td>8.083333</td><td>0.009862</td><td>0.012524</td><td>0.511077</td><td>0.28813</td></tr></tbody></table></div>"},"metadata":{}},{"name":"stderr","text":"100%|██████████| 2/2 [00:00<00:00, 16.87it/s]","output_type":"stream"},{"name":"stdout","text":"shape: (2, 18)\n┌──────────┬────────┬──────┬─────────────┬───┬─────────────┬─────────────┬────────────┬────────────┐\n│ id       ┆ length ┆ day  ┆ sleep_measu ┆ … ┆ sleep_light ┆ sleep_light ┆ sleep_ligh ┆ sleep_ligh │\n│ ---      ┆ ---    ┆ ---  ┆ rement_coun ┆   ┆ _mean_mean  ┆ _mean_std   ┆ t_std_mean ┆ t_std_std  │\n│ str      ┆ i64    ┆ f64  ┆ t           ┆   ┆ ---         ┆ ---         ┆ ---        ┆ ---        │\n│          ┆        ┆      ┆ ---         ┆   ┆ f64         ┆ f64         ┆ f64        ┆ f64        │\n│          ┆        ┆      ┆ i64         ┆   ┆             ┆             ┆            ┆            │\n╞══════════╪════════╪══════╪═════════════╪═══╪═════════════╪═════════════╪════════════╪════════════╡\n│ 00115b9f ┆ 3610   ┆ 44.0 ┆ null        ┆ … ┆ null        ┆ null        ┆ null       ┆ null       │\n│ 001f3379 ┆ 33033  ┆ 23.0 ┆ 7           ┆ … ┆ 4.917133    ┆ 4.370878    ┆ 3.281733   ┆ 4.990464   │\n└──────────┴────────┴──────┴─────────────┴───┴─────────────┴─────────────┴────────────┴────────────┘\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"### Utils functions","metadata":{}},{"cell_type":"code","source":"# Handle non-numeric columns\ndef update(df):\n    global cat_c\n    for c in cat_c: \n        df[c] = df[c].fillna('Missing')\n        df[c] = df[c].astype('category')\n    return df\n\ndef create_mapping(column, dataset):\n    unique_values = dataset[column].unique()\n    return {value: idx for idx, value in enumerate(unique_values)}\n\n# Download timeseries data\ndef process_file(filename, dirname):\n    df = pd.read_parquet(os.path.join(dirname, filename, 'part-0.parquet'))\n    df.drop('step', axis=1, inplace=True)\n    return df.describe().values.reshape(-1), filename.split('=')[1]\n\ndef load_time_series(dirname) -> pd.DataFrame:\n    ids = os.listdir(dirname)\n    with ThreadPoolExecutor() as executor:\n        results = list(tqdm(executor.map(lambda fname: process_file(fname, dirname), ids), total=len(ids)))\n    stats, indexes = zip(*results)\n    df = pd.DataFrame(stats, columns=[f\"stat_{i}\" for i in range(len(stats[0]))])\n    df['id'] = indexes\n    return df\n\n# Function to evaluate the predictions and optimize the thresholds\ndef quadratic_weighted_kappa(y_true, y_pred):\n    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n\ndef threshold_Rounder(oof_non_rounded, thresholds):\n    return np.where(oof_non_rounded < thresholds[0], 0,\n                    np.where(oof_non_rounded < thresholds[1], 1,\n                             np.where(oof_non_rounded < thresholds[2], 2, 3)))\n\ndef evaluate_predictions(thresholds, y_true, oof_non_rounded):\n    rounded_p = threshold_Rounder(oof_non_rounded, thresholds)\n    return -quadratic_weighted_kappa(y_true, rounded_p)","metadata":{"execution":{"iopub.status.busy":"2024-12-24T02:27:40.178481Z","iopub.execute_input":"2024-12-24T02:27:40.178738Z","iopub.status.idle":"2024-12-24T02:27:40.186458Z","shell.execute_reply.started":"2024-12-24T02:27:40.178716Z","shell.execute_reply":"2024-12-24T02:27:40.185597Z"},"trusted":true},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"## Function to train the model with processed time series data","metadata":{}},{"cell_type":"code","source":"def TrainML(model_class, X, y, test_data):\n    SKF = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n    \n    train_S = []\n    test_S = []\n    \n    oof_non_rounded = np.zeros(len(y), dtype=float) \n    oof_rounded = np.zeros(len(y), dtype=int) \n    test_preds = np.zeros((len(test_data), n_splits))\n\n    for fold, (train_idx, test_idx) in enumerate(tqdm(SKF.split(X, y), desc=\"Training Folds\", total=n_splits)):\n        X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n        y_train, y_val = y.iloc[train_idx], y.iloc[test_idx]\n        \n        X_train, selector_tr, imputer_tr = feature_engineering_v2(X_train, fit=True)\n        X_val, _, _ = feature_engineering_v2(X_val, selector_tr, imputer_tr, fit=False)\n        # Train the model\n        model = clone(model_class)\n        model.fit(X_train, y_train)\n\n        y_train_pred = model.predict(X_train)\n        y_val_pred = model.predict(X_val)\n\n        oof_non_rounded[test_idx] = y_val_pred\n        y_val_pred_rounded = y_val_pred.round(0).astype(int)\n        oof_rounded[test_idx] = y_val_pred_rounded\n\n        train_kappa = quadratic_weighted_kappa(y_train, y_train_pred.round(0).astype(int))\n        val_kappa = quadratic_weighted_kappa(y_val, y_val_pred_rounded)\n\n        train_S.append(train_kappa)\n        test_S.append(val_kappa)\n        \n        # Feature engineering for test data\n        test_data_fe, _, _ = feature_engineering_v2(test_data, selector_tr, imputer_tr, fit=False)\n        test_preds[:, fold] = model.predict(test_data_fe)\n        \n        print(f\"Fold {fold+1} - Train QWK: {train_kappa:.4f}, Validation QWK: {val_kappa:.4f}\")\n        clear_output(wait=True)\n    \n    print(f\"Mean Train QWK --> {np.mean(train_S):.4f}\")\n    print(f\"Mean Validation QWK ---> {np.mean(test_S):.4f}\")\n\n    KappaOptimizer = minimize(evaluate_predictions,\n                              x0=[0.5, 1.5, 2.5], args=(y, oof_non_rounded), \n                              method='Nelder-Mead')\n    assert KappaOptimizer.success, \"Optimization did not converge.\"\n    print('OPTIMIZED THRESHOLDS', KappaOptimizer.x)\n    \n    oof_tuned = threshold_Rounder(oof_non_rounded, KappaOptimizer.x)\n    tKappa = quadratic_weighted_kappa(y, oof_tuned)\n\n    print(f\"----> || Optimized QWK SCORE :: {Fore.CYAN}{Style.BRIGHT} {tKappa:.3f}{Style.RESET_ALL}\")\n\n    tpm = test_preds.mean(axis=1)\n    tpTuned = threshold_Rounder(tpm, KappaOptimizer.x)\n    \n    submission = pd.DataFrame({\n        'id': sample['id'],\n        'sii': tpTuned\n    })\n    optimized_thresholds = KappaOptimizer.x\n    return (submission, tKappa, oof_tuned, oof_non_rounded, y, optimized_thresholds)","metadata":{"execution":{"iopub.status.busy":"2024-12-24T02:27:40.187724Z","iopub.execute_input":"2024-12-24T02:27:40.187982Z","iopub.status.idle":"2024-12-24T02:27:40.204773Z","shell.execute_reply.started":"2024-12-24T02:27:40.187955Z","shell.execute_reply":"2024-12-24T02:27:40.204138Z"},"trusted":true},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"# Define features","metadata":{}},{"cell_type":"markdown","source":"## Normal features","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/train.csv')\ntest = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/test.csv')\nsample = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/sample_submission.csv')\n\ntotal_features = list(test.columns)\ntotal_features.remove('id')\n\ncat_c = ['Basic_Demos-Enroll_Season', 'CGAS-Season', 'Physical-Season', \n          'Fitness_Endurance-Season', 'FGC-Season', 'BIA-Season', \n          'PAQ_A-Season', 'PAQ_C-Season', 'SDS-Season', 'PreInt_EduHx-Season']","metadata":{"execution":{"iopub.status.busy":"2024-12-24T02:27:40.205630Z","iopub.execute_input":"2024-12-24T02:27:40.205922Z","iopub.status.idle":"2024-12-24T02:27:40.273623Z","shell.execute_reply.started":"2024-12-24T02:27:40.205891Z","shell.execute_reply":"2024-12-24T02:27:40.272761Z"},"trusted":true},"outputs":[],"execution_count":14},{"cell_type":"code","source":"noseason_features = ['Basic_Demos-Age', 'Basic_Demos-Sex',\n                'CGAS-CGAS_Score', 'Physical-BMI',\n                'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference',\n                'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP',\n                'Fitness_Endurance-Max_Stage',\n                'Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec',\n                'FGC-FGC_CU', 'FGC-FGC_CU_Zone', 'FGC-FGC_GSND',\n                'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone', 'FGC-FGC_PU',\n                'FGC-FGC_PU_Zone', 'FGC-FGC_SRL', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR',\n                'FGC-FGC_SRR_Zone', 'FGC-FGC_TL', 'FGC-FGC_TL_Zone',\n                'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC', 'BIA-BIA_BMI',\n                'BIA-BIA_BMR', 'BIA-BIA_DEE', 'BIA-BIA_ECW', 'BIA-BIA_FFM',\n                'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_Fat', 'BIA-BIA_Frame_num',\n                'BIA-BIA_ICW', 'BIA-BIA_LDM', 'BIA-BIA_LST', 'BIA-BIA_SMM',\n                'BIA-BIA_TBW', 'PAQ_A-PAQ_A_Total',\n                'PAQ_C-PAQ_C_Total', 'SDS-SDS_Total_Raw',\n                'SDS-SDS_Total_T',\n                'PreInt_EduHx-computerinternet_hoursday', 'BMI_Age','Internet_Hours_Age','BMI_Internet_Hours',\n                'BFP_BMI', 'FFMI_BFP', 'FMI_BFP', 'LST_TBW', 'BFP_BMR', 'BFP_DEE', 'BMR_Weight', 'DEE_Weight',\n                'SMM_Height', 'Muscle_to_Fat', 'Hydration_Status', 'ICW_TBW','BMI_PHR']","metadata":{"execution":{"iopub.status.busy":"2024-12-24T02:27:40.274383Z","iopub.execute_input":"2024-12-24T02:27:40.274669Z","iopub.status.idle":"2024-12-24T02:27:40.279183Z","shell.execute_reply.started":"2024-12-24T02:27:40.274647Z","shell.execute_reply":"2024-12-24T02:27:40.278387Z"},"trusted":true},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"## Loading timeseries","metadata":{}},{"cell_type":"code","source":"train_ts = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_train.parquet\")\ntest_ts = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_test.parquet\")","metadata":{"execution":{"iopub.status.busy":"2024-12-24T02:27:40.280024Z","iopub.execute_input":"2024-12-24T02:27:40.280307Z","iopub.status.idle":"2024-12-24T02:28:51.075043Z","shell.execute_reply.started":"2024-12-24T02:27:40.280278Z","shell.execute_reply":"2024-12-24T02:28:51.074299Z"},"trusted":true},"outputs":[{"name":"stderr","text":"100%|██████████| 996/996 [01:10<00:00, 14.11it/s]\n100%|██████████| 2/2 [00:00<00:00, 13.27it/s]\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"df_train = train_ts.drop('id', axis=1)\ndf_test = test_ts.drop('id', axis=1)\nautoencoder, scaler = perform_autoencoder(df_train, encoding_dim=64, epochs=100, batch_size=32)","metadata":{"execution":{"iopub.status.busy":"2024-12-24T02:28:51.075978Z","iopub.execute_input":"2024-12-24T02:28:51.076208Z","iopub.status.idle":"2024-12-24T02:29:00.673945Z","shell.execute_reply.started":"2024-12-24T02:28:51.076187Z","shell.execute_reply":"2024-12-24T02:29:00.672972Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Epoch [50/100], Loss: 1.4762]\nEpoch [100/100], Loss: 1.4641]\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"train_ts_encoded = encode_data(autoencoder, scaler, df_train)\ntest_ts_encoded = encode_data(autoencoder, scaler, df_test)\ntest_ts_encoded.reset_index(inplace=True, drop=True)","metadata":{"execution":{"iopub.status.busy":"2024-12-24T02:29:00.674818Z","iopub.execute_input":"2024-12-24T02:29:00.675065Z","iopub.status.idle":"2024-12-24T02:29:00.688503Z","shell.execute_reply.started":"2024-12-24T02:29:00.675044Z","shell.execute_reply":"2024-12-24T02:29:00.687815Z"},"trusted":true},"outputs":[],"execution_count":18},{"cell_type":"code","source":"train_ts_encoded[\"id\"]=train_ts[\"id\"]\ntest_ts_encoded['id']=test_ts[\"id\"]","metadata":{"execution":{"iopub.status.busy":"2024-12-24T02:29:00.689221Z","iopub.execute_input":"2024-12-24T02:29:00.689438Z","iopub.status.idle":"2024-12-24T02:29:00.704370Z","shell.execute_reply.started":"2024-12-24T02:29:00.689419Z","shell.execute_reply":"2024-12-24T02:29:00.703634Z"},"trusted":true},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"## Additiontal timeseries features","metadata":{}},{"cell_type":"code","source":"time_series_cols = train_ts.columns.tolist()\ntime_series_cols.remove(\"id\")\ntime_encoded_cols = train_ts_encoded.columns.tolist()\ntime_encoded_cols.remove(\"id\")","metadata":{"execution":{"iopub.status.busy":"2024-12-24T02:29:00.705112Z","iopub.execute_input":"2024-12-24T02:29:00.705366Z","iopub.status.idle":"2024-12-24T02:29:00.719009Z","shell.execute_reply.started":"2024-12-24T02:29:00.705346Z","shell.execute_reply":"2024-12-24T02:29:00.718331Z"},"trusted":true},"outputs":[],"execution_count":20},{"cell_type":"code","source":"train_sleep = pd.read_csv(\"/kaggle/input/sleep-detection/sleep_features.csv\")\ntest_sleep = pd.read_csv(\"/kaggle/working/features/test/sleep_features.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T02:29:00.719750Z","iopub.execute_input":"2024-12-24T02:29:00.719992Z","iopub.status.idle":"2024-12-24T02:29:00.748667Z","shell.execute_reply.started":"2024-12-24T02:29:00.719961Z","shell.execute_reply":"2024-12-24T02:29:00.747888Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"sleep_cols = train_sleep.columns.tolist()\nsleep_cols.remove(\"id\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T02:29:00.749466Z","iopub.execute_input":"2024-12-24T02:29:00.749738Z","iopub.status.idle":"2024-12-24T02:29:00.753081Z","shell.execute_reply.started":"2024-12-24T02:29:00.749710Z","shell.execute_reply":"2024-12-24T02:29:00.752392Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"rm -rf /kaggle/working/features","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T02:29:00.753821Z","iopub.execute_input":"2024-12-24T02:29:00.754118Z","iopub.status.idle":"2024-12-24T02:29:00.931123Z","shell.execute_reply.started":"2024-12-24T02:29:00.754090Z","shell.execute_reply":"2024-12-24T02:29:00.930080Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"rm -rf /kaggle/working/heuristic_features","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T02:29:00.932107Z","iopub.execute_input":"2024-12-24T02:29:00.932371Z","iopub.status.idle":"2024-12-24T02:29:01.098597Z","shell.execute_reply.started":"2024-12-24T02:29:00.932348Z","shell.execute_reply":"2024-12-24T02:29:01.097323Z"}},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"train_sub1 = pd.merge(train, train_sleep, how=\"left\", on='id')\ntest_sub1 = pd.merge(test, test_sleep, how=\"left\", on='id')\ntrain_sub1 = pd.merge(train_sub1, train_ts_encoded, how=\"left\", on='id')\ntest_sub1 = pd.merge(test_sub1, test_ts_encoded, how=\"left\", on='id')\ntrain_sub1 = train_sub1.dropna(subset='sii')","metadata":{"execution":{"iopub.status.busy":"2024-12-24T02:29:01.099981Z","iopub.execute_input":"2024-12-24T02:29:01.100338Z","iopub.status.idle":"2024-12-24T02:29:01.136597Z","shell.execute_reply.started":"2024-12-24T02:29:01.100302Z","shell.execute_reply":"2024-12-24T02:29:01.135917Z"},"trusted":true},"outputs":[],"execution_count":25},{"cell_type":"code","source":"SVR_Best_Params = {\n    'C': 0.1,\n    'epsilon': 0.1,\n    'kernel': 'rbf',\n    'gamma': 'scale',\n}\n\nCatBoost_Best_Params = {\n    'learning_rate': 0.0021172579310639343,\n    'depth': 6,\n    'iterations': 130,\n    'random_seed': SEED,\n    'verbose': 0,\n    'l2_leaf_reg': 0.32557701990001503,\n    'task_type': 'GPU',  \n    'devices': '0'\n}\n\nXGB_Best_Params = {\n    'n_estimators': 700,\n    'max_depth': 4,\n    'learning_rate': 0.03325152156380898,\n    'subsample': 0.25295047248406266,\n    'colsample_bytree': 0.9760859719849787,\n    'gamma': 0.20085951790463402,\n    'min_child_weight': 11,\n    'eval_metric': 'rmse',\n    'objective': 'reg:squarederror',\n    'tree_method': 'gpu_hist',\n    'predictor': 'gpu_predictor',\n    'gpu_id': 0\n}\n\nLightGBM_Best_Params = {\n    'max_depth': 3,\n    'min_data_in_leaf': 40,\n    'num_leaves': 190,\n    'learning_rate': 0.05107368421432176,\n    'feature_fraction': 0.9918350138636185,\n    'bagging_fraction': 0.9331400899763774,\n    'bagging_freq': 1,\n    'lambda_l1': 9.49641646280519,\n    'lambda_l2': 2.446305429623661,\n    'min_gain_to_split': 0.05262124930522051,\n    'device_type': 'gpu',\n    'gpu_device_id': 0,\n    'verbosity': -1\n}\n\n\n\ncatboost_model = CatBoostRegressor(**CatBoost_Best_Params)\nxgb_model = XGBRegressor(**XGB_Best_Params)\nlightgbm_model = LGBMRegressor(**LightGBM_Best_Params)\nsvr_model = SVR(**SVR_Best_Params)\n\nfinal_voting_model = VotingRegressor(estimators=[\n    ('lightgbm', lightgbm_model),\n    ('xgboost', xgb_model),\n    ('catboost', catboost_model),\n], weights=[4.0, 4.0, 4.0])\n\nX = train_sub1\ny = train_sub1['sii']","metadata":{"execution":{"iopub.status.busy":"2024-12-24T02:29:01.137400Z","iopub.execute_input":"2024-12-24T02:29:01.137667Z","iopub.status.idle":"2024-12-24T02:29:01.148708Z","shell.execute_reply.started":"2024-12-24T02:29:01.137645Z","shell.execute_reply":"2024-12-24T02:29:01.147895Z"},"trusted":true},"outputs":[],"execution_count":26},{"cell_type":"code","source":"submission1, val_score_sub1, _, _, _, _ = TrainML(xgb_model, X, y, test_sub1)\n\nprint(\"Val score sub1 with best parameters:\", val_score_sub1)","metadata":{"execution":{"iopub.status.busy":"2024-12-24T02:30:10.964255Z","iopub.execute_input":"2024-12-24T02:30:10.964654Z","iopub.status.idle":"2024-12-24T02:30:16.501828Z","shell.execute_reply.started":"2024-12-24T02:30:10.964610Z","shell.execute_reply":"2024-12-24T02:30:16.501156Z"},"trusted":true},"outputs":[{"name":"stderr","text":"Training Folds: 100%|██████████| 5/5 [00:05<00:00,  1.05s/it]","output_type":"stream"},{"name":"stdout","text":"Mean Train QWK --> 0.6928\nMean Validation QWK ---> 0.3825\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"OPTIMIZED THRESHOLDS [0.53160476 1.09984099 2.68423158]\n----> || Optimized QWK SCORE :: \u001b[36m\u001b[1m 0.434\u001b[0m\nVal score sub1 with best parameters: 0.43433804502338047\n","output_type":"stream"}],"execution_count":30},{"cell_type":"markdown","source":"# Final","metadata":{}},{"cell_type":"code","source":"final_submission = submission1\nfinal_submission.to_csv('submission.csv', index=False)\n\nprint(\"Submission saved to 'submission.csv'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T02:29:15.324628Z","iopub.execute_input":"2024-12-24T02:29:15.324908Z","iopub.status.idle":"2024-12-24T02:29:15.333214Z","shell.execute_reply.started":"2024-12-24T02:29:15.324883Z","shell.execute_reply":"2024-12-24T02:29:15.332388Z"}},"outputs":[{"name":"stdout","text":"Submission saved to 'submission.csv'\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"final_submission","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-24T02:29:15.334122Z","iopub.execute_input":"2024-12-24T02:29:15.334349Z","iopub.status.idle":"2024-12-24T02:29:15.352667Z","shell.execute_reply.started":"2024-12-24T02:29:15.334328Z","shell.execute_reply":"2024-12-24T02:29:15.351661Z"}},"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"          id  sii\n0   00008ff9    1\n1   000fd460    0\n2   00105258    0\n3   00115b9f    0\n4   0016bb22    1\n5   001f3379    1\n6   0038ba98    1\n7   0068a485    0\n8   0069fbed    2\n9   0083e397    1\n10  0087dd65    1\n11  00abe655    0\n12  00ae59c9    1\n13  00af6387    1\n14  00bd4359    1\n15  00c0cd71    0\n16  00d56d4b    0\n17  00d9913d    0\n18  00e6167c    0\n19  00ebc35d    1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>sii</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>00008ff9</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000fd460</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00105258</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>00115b9f</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0016bb22</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>001f3379</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0038ba98</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0068a485</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0069fbed</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0083e397</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0087dd65</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>00abe655</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>00ae59c9</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>00af6387</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>00bd4359</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>00c0cd71</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>00d56d4b</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>00d9913d</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>00e6167c</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>00ebc35d</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":29}]}