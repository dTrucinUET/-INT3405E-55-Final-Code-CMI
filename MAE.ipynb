{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc89caa0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T14:18:38.105383Z",
     "iopub.status.busy": "2024-12-19T14:18:38.104694Z",
     "iopub.status.idle": "2024-12-19T14:18:46.565790Z",
     "shell.execute_reply": "2024-12-19T14:18:46.565049Z"
    },
    "papermill": {
     "duration": 8.471164,
     "end_time": "2024-12-19T14:18:46.567882",
     "exception": false,
     "start_time": "2024-12-19T14:18:38.096718",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from scipy.optimize import minimize\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "from colorama import Fore, Style\n",
    "from IPython.display import clear_output\n",
    "import warnings\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.ensemble import VotingRegressor, RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bb63e01",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T14:18:46.580145Z",
     "iopub.status.busy": "2024-12-19T14:18:46.579327Z",
     "iopub.status.idle": "2024-12-19T14:18:46.587191Z",
     "shell.execute_reply": "2024-12-19T14:18:46.586592Z"
    },
    "papermill": {
     "duration": 0.015501,
     "end_time": "2024-12-19T14:18:46.588960",
     "exception": false,
     "start_time": "2024-12-19T14:18:46.573459",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "SEED = 314159\n",
    "seed_everything(SEED)\n",
    "\n",
    "n_splits = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ef2c6ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T14:18:46.601287Z",
     "iopub.status.busy": "2024-12-19T14:18:46.601011Z",
     "iopub.status.idle": "2024-12-19T14:20:04.525540Z",
     "shell.execute_reply": "2024-12-19T14:20:04.524405Z"
    },
    "papermill": {
     "duration": 77.932114,
     "end_time": "2024-12-19T14:20:04.527356",
     "exception": false,
     "start_time": "2024-12-19T14:18:46.595242",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 996/996 [01:17<00:00, 12.83it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  8.95it/s]\n"
     ]
    }
   ],
   "source": [
    "def process_file(filename, dirname):\n",
    "    df = pd.read_parquet(os.path.join(dirname, filename, 'part-0.parquet'))\n",
    "    df.drop('step', axis=1, inplace=True)\n",
    "    return df.describe().values.reshape(-1), filename.split('=')[1]\n",
    "\n",
    "def load_time_series(dirname) -> pd.DataFrame:\n",
    "    ids = os.listdir(dirname)\n",
    "    \n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        results = list(tqdm(executor.map(lambda fname: process_file(fname, dirname), ids), total=len(ids)))\n",
    "    \n",
    "    stats, indexes = zip(*results)\n",
    "    \n",
    "    df = pd.DataFrame(stats, columns=[f\"stat_{i}\" for i in range(len(stats[0]))])\n",
    "    df['id'] = indexes\n",
    "    return df\n",
    "    \n",
    "train_ts = load_time_series(\"../input/child-mind-institute-problematic-internet-use/series_train.parquet\")\n",
    "test_ts = load_time_series(\"../input/child-mind-institute-problematic-internet-use/series_test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6314b191",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T14:20:04.572444Z",
     "iopub.status.busy": "2024-12-19T14:20:04.572109Z",
     "iopub.status.idle": "2024-12-19T14:20:04.734304Z",
     "shell.execute_reply": "2024-12-19T14:20:04.733477Z"
    },
    "papermill": {
     "duration": 0.187426,
     "end_time": "2024-12-19T14:20:04.736572",
     "exception": false,
     "start_time": "2024-12-19T14:20:04.549146",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/child-mind-institute-problematic-internet-use/train.csv')\n",
    "test = pd.read_csv('../input/child-mind-institute-problematic-internet-use/test.csv')\n",
    "sample = pd.read_csv('../input/child-mind-institute-problematic-internet-use/sample_submission.csv')\n",
    "\n",
    "\n",
    "featuresCols = ['Basic_Demos-Enroll_Season', 'Basic_Demos-Age', 'Basic_Demos-Sex',\n",
    "                'CGAS-Season', 'CGAS-CGAS_Score', 'Physical-Season', 'Physical-BMI',\n",
    "                'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference',\n",
    "                'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP',\n",
    "                'Fitness_Endurance-Season', 'Fitness_Endurance-Max_Stage',\n",
    "                'Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec',\n",
    "                'FGC-Season', 'FGC-FGC_CU', 'FGC-FGC_CU_Zone', 'FGC-FGC_GSND',\n",
    "                'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone', 'FGC-FGC_PU',\n",
    "                'FGC-FGC_PU_Zone', 'FGC-FGC_SRL', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR',\n",
    "                'FGC-FGC_SRR_Zone', 'FGC-FGC_TL', 'FGC-FGC_TL_Zone', 'BIA-Season',\n",
    "                'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC', 'BIA-BIA_BMI',\n",
    "                'BIA-BIA_BMR', 'BIA-BIA_DEE', 'BIA-BIA_ECW', 'BIA-BIA_FFM',\n",
    "                'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_Fat', 'BIA-BIA_Frame_num',\n",
    "                'BIA-BIA_ICW', 'BIA-BIA_LDM', 'BIA-BIA_LST', 'BIA-BIA_SMM',\n",
    "                'BIA-BIA_TBW', 'PAQ_A-Season', 'PAQ_A-PAQ_A_Total', 'PAQ_C-Season',\n",
    "                'PAQ_C-PAQ_C_Total', 'SDS-Season', 'SDS-SDS_Total_Raw',\n",
    "                'SDS-SDS_Total_T', 'PreInt_EduHx-Season',\n",
    "                'PreInt_EduHx-computerinternet_hoursday', 'sii',\n",
    "               \n",
    "                 'PCIAT-Season', 'PCIAT-PCIAT_01', 'PCIAT-PCIAT_02', 'PCIAT-PCIAT_03', 'PCIAT-PCIAT_04',\n",
    "                'PCIAT-PCIAT_05', 'PCIAT-PCIAT_06', 'PCIAT-PCIAT_07', 'PCIAT-PCIAT_08',\n",
    "                'PCIAT-PCIAT_09', 'PCIAT-PCIAT_10', 'PCIAT-PCIAT_11', 'PCIAT-PCIAT_12',\n",
    "                'PCIAT-PCIAT_13', 'PCIAT-PCIAT_14', 'PCIAT-PCIAT_15', 'PCIAT-PCIAT_16',\n",
    "                'PCIAT-PCIAT_17', 'PCIAT-PCIAT_18', 'PCIAT-PCIAT_19', 'PCIAT-PCIAT_20', 'PCIAT-PCIAT_Total',\n",
    "]\n",
    "\n",
    "cat_c = ['Basic_Demos-Enroll_Season', 'CGAS-Season', 'Physical-Season', \n",
    "          'Fitness_Endurance-Season', 'FGC-Season', 'BIA-Season', \n",
    "          'PAQ_A-Season', 'PAQ_C-Season', 'SDS-Season', 'PreInt_EduHx-Season',\n",
    "        'PCIAT-Season',\n",
    "        ]\n",
    "\n",
    "\n",
    "time_series_cols = train_ts.columns.tolist()\n",
    "time_series_cols.remove(\"id\")\n",
    "\n",
    "train = pd.merge(train, train_ts, how=\"left\", on='id')\n",
    "test = pd.merge(test, test_ts, how=\"left\", on='id')\n",
    "\n",
    "train = train.drop('id', axis=1)\n",
    "test = test.drop('id', axis=1)\n",
    "\n",
    "featuresCols += time_series_cols\n",
    "\n",
    "train = train[featuresCols]\n",
    "# train = train.dropna(subset='sii')\n",
    "\n",
    "def update(df):\n",
    "    global cat_c\n",
    "    for c in cat_c: \n",
    "        if c not in df.columns: continue\n",
    "        df[c] = df[c].fillna('Missing')\n",
    "        df[c] = df[c].astype('category')\n",
    "    return df\n",
    "\n",
    "train = update(train)\n",
    "test = update(test)\n",
    "\n",
    "def create_mapping(column, dataset):\n",
    "    unique_values = dataset[column].unique()\n",
    "    return {value: idx for idx, value in enumerate(unique_values)}\n",
    "\n",
    "for col in cat_c:\n",
    "    if col in train.columns:\n",
    "        if 'Season' in col:\n",
    "            mapping = {\n",
    "                'Missing': float('nan'),\n",
    "                'Spring': 0.,\n",
    "                'Summer': 1.,\n",
    "                'Fall': 2.,\n",
    "                'Winter': 3.,\n",
    "            }\n",
    "        else:\n",
    "            mapping = create_mapping(col, train)\n",
    "            print(f'{col}: {mapping}')\n",
    "        train[col] = train[col].replace(mapping)\n",
    "    if col in test.columns:\n",
    "        if 'Season' in col:\n",
    "            mappingTe = {\n",
    "                'Missing': float('nan'),\n",
    "                'Spring': 0.,\n",
    "                'Summer': 1.,\n",
    "                'Fall': 2.,\n",
    "                'Winter': 3.,\n",
    "            }\n",
    "        else:\n",
    "            mappingTe = create_mapping(col, test)\n",
    "            print(f'{col}: {mappingTe}')\n",
    "            \n",
    "        test[col] = test[col].replace(mappingTe)\n",
    "\n",
    "def quadratic_weighted_kappa(y_true, y_pred):\n",
    "    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n",
    "\n",
    "def threshold_Rounder(oof_non_rounded, thresholds):\n",
    "    return np.where(oof_non_rounded < thresholds[0], 0,\n",
    "                    np.where(oof_non_rounded < thresholds[1], 1,\n",
    "                             np.where(oof_non_rounded < thresholds[2], 2, 3)))\n",
    "\n",
    "def evaluate_predictions(thresholds, y_true, oof_non_rounded):\n",
    "    rounded_p = threshold_Rounder(oof_non_rounded, thresholds)\n",
    "    return -quadratic_weighted_kappa(y_true, rounded_p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba4cc55",
   "metadata": {
    "papermill": {
     "duration": 0.020688,
     "end_time": "2024-12-19T14:20:04.779568",
     "exception": false,
     "start_time": "2024-12-19T14:20:04.758880",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17175e0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T14:20:04.826261Z",
     "iopub.status.busy": "2024-12-19T14:20:04.825878Z",
     "iopub.status.idle": "2024-12-19T14:20:19.493067Z",
     "shell.execute_reply": "2024-12-19T14:20:19.492277Z"
    },
    "papermill": {
     "duration": 14.69383,
     "end_time": "2024-12-19T14:20:19.495220",
     "exception": false,
     "start_time": "2024-12-19T14:20:04.801390",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# current implementation: only support numerical values\n",
    "\n",
    "from functools import partial\n",
    "from tkinter import E\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from timm.models.vision_transformer import Block\n",
    "\n",
    "# current implementation: only support numerical values\n",
    "import numpy as np\n",
    "import torch, os\n",
    "from torch import nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import math\n",
    "import argparse\n",
    "import random\n",
    "\n",
    "class MaskEmbed(nn.Module):\n",
    "    \"\"\" record to mask embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, rec_len=25, embed_dim=64, norm_layer=None):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.rec_len = rec_len\n",
    "        self.proj = nn.Conv1d(1, embed_dim, kernel_size=1, stride=1)\n",
    "        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, _, L = x.shape\n",
    "        # assert(L == self.rec_len, f\"Input data width ({L}) doesn't match model ({self.rec_len}).\")\n",
    "        x = self.proj(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ActiveEmbed(nn.Module):\n",
    "    \"\"\" record to mask embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, rec_len=25, embed_dim=64, norm_layer=None):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.rec_len = rec_len\n",
    "        self.proj = nn.Conv1d(1, embed_dim, kernel_size=1, stride=1)\n",
    "        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, _, L = x.shape\n",
    "        # assert(L == self.rec_len, f\"Input data width ({L}) doesn't match model ({self.rec_len}).\")\n",
    "        x = self.proj(x)\n",
    "        x = torch.sin(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        #   x = torch.cat((torch.sin(x), torch.cos(x + math.pi/2)), -1)\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "def get_1d_sincos_pos_embed(embed_dim, pos, cls_token=False):\n",
    "    \"\"\"\n",
    "    embed_dim: output dimension for each position\n",
    "    pos: a list of positions to be encoded: size (M,)\n",
    "    out: (M, D)\n",
    "    \"\"\"\n",
    "\n",
    "    assert embed_dim % 2 == 0\n",
    "    omega = np.arange(embed_dim // 2, dtype=np.float32)\n",
    "    omega /= embed_dim / 2.\n",
    "    omega = 1. / 10000**omega  # (D/2,)\n",
    "\n",
    "    pos = np.arange(pos)  # (M,)\n",
    "    out = np.einsum('m,d->md', pos, omega)  # (M, D/2), outer product\n",
    "\n",
    "    emb_sin = np.sin(out) # (M, D/2)\n",
    "    emb_cos = np.cos(out) # (M, D/2)\n",
    "\n",
    "    pos_embed = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)\n",
    "\n",
    "    if cls_token:\n",
    "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
    "\n",
    "    return pos_embed\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch, lr, min_lr, max_epochs, warmup_epochs):\n",
    "    \"\"\"Decay the learning rate with half-cycle cosine after warmup\"\"\"\n",
    "    if epoch < warmup_epochs:\n",
    "        tmp_lr = lr * epoch / warmup_epochs \n",
    "    else:\n",
    "        tmp_lr = min_lr + (lr - min_lr) * 0.5 * \\\n",
    "            (1. + math.cos(math.pi * (epoch - warmup_epochs) / (max_epochs - warmup_epochs)))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        if \"lr_scale\" in param_group:\n",
    "            param_group[\"lr\"] = tmp_lr * param_group[\"lr_scale\"]\n",
    "        else:\n",
    "            param_group[\"lr\"] = tmp_lr\n",
    "    return tmp_lr\n",
    "\n",
    "\n",
    "def get_grad_norm_(parameters, norm_type: float = 2.0) -> torch.Tensor:\n",
    "    if isinstance(parameters, torch.Tensor):\n",
    "        parameters = [parameters]\n",
    "    parameters = [p for p in parameters if p.grad is not None]\n",
    "    norm_type = float(norm_type)\n",
    "    if len(parameters) == 0:\n",
    "        return torch.tensor(0.)\n",
    "    device = parameters[0].grad.device\n",
    "    if norm_type == np.inf:\n",
    "        total_norm = max(p.grad.detach().abs().max().to(device) for p in parameters)\n",
    "    else:\n",
    "        total_norm = torch.norm(torch.stack([torch.norm(p.grad.detach(), norm_type).to(device) for p in parameters]), norm_type)\n",
    "    return total_norm\n",
    "\n",
    "\n",
    "class NativeScaler:\n",
    "\n",
    "    state_dict_key = \"amp_scaler\"\n",
    "    def __init__(self):\n",
    "        self._scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    def __call__(self, loss, optimizer, clip_grad=None, parameters=None, create_graph=False, update_grad=True):\n",
    "        self._scaler.scale(loss).backward(create_graph=create_graph)\n",
    "        if update_grad:\n",
    "            if clip_grad is not None:\n",
    "                assert parameters is not None\n",
    "                self._scaler.unscale_(optimizer)  # unscale the gradients of optimizer's assigned params in-place\n",
    "                norm = torch.nn.utils.clip_grad_norm_(parameters, clip_grad)\n",
    "            else:\n",
    "                self._scaler.unscale_(optimizer)\n",
    "                norm = get_grad_norm_(parameters)\n",
    "            self._scaler.step(optimizer)\n",
    "            self._scaler.update()\n",
    "        else:\n",
    "            norm = None\n",
    "        return norm\n",
    "\n",
    "    def state_dict(self):\n",
    "        return self._scaler.state_dict()\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        self._scaler.load_state_dict(state_dict)\n",
    "\n",
    "\n",
    "\n",
    "class MAEDataset(Dataset):\n",
    "\n",
    "    def __init__(self, X, M):        \n",
    "         self.X = X\n",
    "         self.M = M\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        return self.X[idx], self.M[idx]\n",
    "\n",
    "\n",
    "\n",
    "def get_dataset(dataset : str, path : str):\n",
    "\n",
    "    if dataset in ['climate', 'compression', 'wine', 'yacht', 'spam', 'letter', 'credit', 'raisin', 'bike', 'obesity', 'airfoil', 'blood', 'yeast', 'health', 'review', 'travel']:\n",
    "        df = pd.read_csv(os.path.join(path, 'data', dataset + '.csv'))\n",
    "        last_col = df.columns[-1]\n",
    "        y = df[last_col]\n",
    "        X = df.drop(columns=[last_col])\n",
    "    elif dataset == 'california':\n",
    "        from sklearn.datasets import fetch_california_housing\n",
    "        X, y = fetch_california_housing(as_frame=True, return_X_y=True)\n",
    "    elif dataset == 'diabetes':\n",
    "        from sklearn.datasets import load_diabetes\n",
    "        X, y = load_diabetes(as_frame=True, return_X_y=True)\n",
    "    elif dataset == 'iris':\n",
    "        # only for testing\n",
    "        from sklearn.datasets import load_iris\n",
    "        X, y = load_iris(as_frame=True, return_X_y=True)\n",
    "    \n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "eps = 1e-6\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.nn import TransformerEncoderLayer\n",
    "from transformers.models.bert.modeling_bert import BertPooler\n",
    "\n",
    "class MaskedAutoencoder(nn.Module):\n",
    "    def __init__(self, rec_len=25, embed_dim=64, depth=4, num_heads=4,\n",
    "                 decoder_embed_dim=64, decoder_depth=2, decoder_num_heads=4,\n",
    "                 mlp_ratio=4., cls_mlp_dim=64, norm_layer=nn.LayerNorm, norm_field_loss=False,\n",
    "                 encode_func='linear', dropout=0.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.rec_len = rec_len\n",
    "        self.embed_dim = embed_dim\n",
    "        self.norm_field_loss = norm_field_loss\n",
    "        \n",
    "        # Encoder\n",
    "        if encode_func == 'active':\n",
    "            self.mask_embed = ActiveEmbed(rec_len, embed_dim, norm_layer)\n",
    "        else:\n",
    "            self.mask_embed = MaskEmbed(rec_len, embed_dim, norm_layer)\n",
    "        \n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, rec_len + 1, embed_dim), requires_grad=False)\n",
    "        \n",
    "        encoder_layer = TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=int(embed_dim * mlp_ratio),\n",
    "                        dropout=dropout, batch_first=True)\n",
    "        self.blocks = nn.TransformerEncoder(encoder_layer, depth)\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "        \n",
    "        \n",
    "        self.enc_pooler = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(embed_dim, 1),\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.Linear((rec_len + 1) * 1, cls_mlp_dim), nn.ReLU(),\n",
    "            ),\n",
    "        ])\n",
    "        self.enc_lbl_pred = nn.Sequential(\n",
    "            nn.Linear(cls_mlp_dim, cls_mlp_dim), nn.ReLU(),\n",
    "            # nn.Linear(cls_mlp_dim, cls_mlp_dim), nn.ReLU(),\n",
    "            # nn.Dropout(dropout),\n",
    "            nn.Linear(cls_mlp_dim, cls_mlp_dim), nn.Sigmoid(),\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder_embed = nn.Linear(embed_dim, decoder_embed_dim, bias=True)\n",
    "        self.mask_token = nn.Parameter(torch.zeros(1, 1, decoder_embed_dim))\n",
    "        self.decoder_pos_embed = nn.Parameter(torch.zeros(1, rec_len + 1, decoder_embed_dim), requires_grad=False)\n",
    "        \n",
    "        decoder_layer = TransformerEncoderLayer(d_model=decoder_embed_dim, nhead=decoder_num_heads, dim_feedforward=int(decoder_embed_dim * mlp_ratio),\n",
    "                        dropout=dropout, batch_first=True)\n",
    "        self.decoder_blocks = nn.TransformerEncoder(decoder_layer, decoder_depth)\n",
    "        self.decoder_norm = norm_layer(decoder_embed_dim)\n",
    "        self.decoder_pred = nn.Linear(decoder_embed_dim, 1, bias=True)\n",
    "        \n",
    "        \n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        pos_embed = get_1d_sincos_pos_embed(self.pos_embed.shape[-1], self.rec_len, cls_token=True)\n",
    "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
    "        \n",
    "        decoder_pos_embed = get_1d_sincos_pos_embed(self.decoder_pos_embed.shape[-1], self.rec_len, cls_token=True)\n",
    "        self.decoder_pos_embed.data.copy_(torch.from_numpy(decoder_pos_embed).float().unsqueeze(0))\n",
    "        \n",
    "        torch.nn.init.xavier_uniform_(self.mask_embed.proj.weight.view([self.mask_embed.proj.weight.shape[0], -1]))\n",
    "        torch.nn.init.normal_(self.cls_token, std=.02)\n",
    "        torch.nn.init.normal_(self.mask_token, std=.02)\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def random_masking(self, x, m, mask_ratio, training=None):\n",
    "        N, L, D = x.shape\n",
    "        if training is None:\n",
    "            training = self.training\n",
    "        if training:\n",
    "            len_keep = int(L * (1 - mask_ratio))\n",
    "            noise = torch.rand(N, L, device=x.device)\n",
    "            noise[m < 1e-6] = 1\n",
    "            ids_shuffle = torch.argsort(noise, dim=1)\n",
    "            ids_restore = torch.argsort(ids_shuffle, dim=1)\n",
    "            ids_keep = ids_shuffle[:, :len_keep]\n",
    "            mask = torch.ones([N, L], device=x.device)\n",
    "            mask[:, :len_keep] = 0\n",
    "            mask = torch.gather(mask, dim=1, index=ids_restore)\n",
    "            mask = torch.logical_or(mask, ~m.bool())\n",
    "            nask = ~mask\n",
    "            return mask, nask\n",
    "        else:\n",
    "            mask = ~m.bool()\n",
    "            nask = m.bool()\n",
    "            return mask, nask\n",
    "\n",
    "    def forward_encoder(self, x, m, mask_ratio=0.5, training=None):\n",
    "        x = self.mask_embed(x)\n",
    "        x = x + self.pos_embed[:, 1:, :]\n",
    "        mask, nask = self.random_masking(x, m, mask_ratio, training)\n",
    "        x = x * (~mask.unsqueeze(-1)).float()\n",
    "        cls_token = self.cls_token + self.pos_embed[:, :1, :]\n",
    "        cls_tokens = cls_token.expand(x.shape[0], -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        attn_mask = torch.cat((torch.zeros(x.shape[0], 1, device=x.device), mask), dim=1)\n",
    "        x = self.blocks(src=x, src_key_padding_mask=attn_mask.bool())\n",
    "        x = self.norm(x)\n",
    "        return x, mask, nask\n",
    "\n",
    "    def forward_decoder(self, x, mask):\n",
    "        x = self.decoder_embed(x)\n",
    "        x = x + self.decoder_pos_embed\n",
    "        mask_with_cls = torch.cat((torch.zeros(x.shape[0], 1, device=x.device), mask), dim=1)\n",
    "        x = self.blocks(src=x, src_key_padding_mask=mask_with_cls.bool())\n",
    "        \n",
    "        x = self.decoder_norm(x)\n",
    "        x = self.decoder_pred(x)\n",
    "        x = x[:, 1:, :].sigmoid()\n",
    "        return x\n",
    "\n",
    "    def forward_loss(self, data, pred, m, mask, nask):\n",
    "        target = data.squeeze(dim=1)\n",
    "        # if self.norm_field_loss:\n",
    "        #     mean = target.mean(dim=-1, keepdim=True)\n",
    "        #     var = target.var(dim=-1, keepdim=True)\n",
    "        #     target = (target - mean) / (var + 1e-6) ** 0.5\n",
    "        rec_mask = mask * m\n",
    "        loss = (pred.squeeze(dim=2) - target) ** 2\n",
    "        loss = (loss * rec_mask).sum() / (rec_mask.sum() + 1e-6) + (loss * nask).sum() / (nask.sum() + 1e-6)\n",
    "        return loss\n",
    "\n",
    "    def forward(self, data, m):\n",
    "        x, _, _ = self.forward_encoder(data, m, 0.0, False)\n",
    "        B = x.shape[0]\n",
    "        h = self.enc_pooler[1](self.enc_pooler[0](x).reshape(B, -1))\n",
    "        enc_pred = self.enc_lbl_pred(h)[:, 0]\n",
    "        return enc_pred\n",
    "\n",
    "    def forward_selfsl(self, data, m, mask_ratio=0.5, training=None):\n",
    "        x, mask, nask = self.forward_encoder(data, m, mask_ratio, training)\n",
    "        pred = self.forward_decoder(x, mask)\n",
    "        loss = self.forward_loss(data, pred, m, mask, nask)\n",
    "        return loss, (loss.item(), )\n",
    "\n",
    "    def forward_sl(self, data, m, lbl_cols):\n",
    "        num_lbls = len(lbl_cols)\n",
    "        lbl_mask = m[:, lbl_cols]\n",
    "        ft_mask = m.clone()\n",
    "        ft_mask[:, lbl_cols] = 0\n",
    "        ft = data.clone()\n",
    "        ft[:, :, lbl_cols] = 0\n",
    "        x, _, _ = self.forward_encoder(ft, ft_mask, 0.0, False)\n",
    "        B = x.shape[0]\n",
    "        h = self.enc_pooler[1](self.enc_pooler[0](x).reshape(B, -1))\n",
    "        enc_pred = self.enc_lbl_pred(h)[:, :num_lbls]\n",
    "\n",
    "        tgt = data[:, 0, lbl_cols]\n",
    "        enc_loss = (((enc_pred - tgt) ** 2) * lbl_mask).sum() / (lbl_mask.sum() + 1e-6)\n",
    "        \n",
    "        loss = enc_loss\n",
    "        \n",
    "        return loss, (enc_loss.item(), )\n",
    "      \n",
    "    def forward_semisl(self, data, m, lbl_cols, ema_model=None, hard=False):\n",
    "        num_lbls = len(lbl_cols)\n",
    "        lbl_mask = m[:, lbl_cols]\n",
    "        nlbl_mask = 1 - m[:, lbl_cols]\n",
    "        ft_mask = m.clone()\n",
    "        ft_mask[:, lbl_cols] = 0\n",
    "        ft = data.clone()\n",
    "        ft[:, :, lbl_cols] = 0\n",
    "        \n",
    "        noise = torch.randn_like(ft)\n",
    "        noise_norm = torch.norm(noise, p=2, dim=-1, keepdim=True)\n",
    "        noise = noise / (noise_norm + 1e-8)\n",
    "        noise = noise * 0.0\n",
    "        if not hard: noise *= 0\n",
    "        \n",
    "        x, _, _ = self.forward_encoder(torch.clamp(ft + noise, min=0.0, max=1.0), ft_mask, 0.0, False)\n",
    "        B = x.shape[0]\n",
    "        h = self.enc_pooler[1](self.enc_pooler[0](x).reshape(B, -1))\n",
    "        enc_pred = self.enc_lbl_pred(h)[:, :num_lbls]\n",
    "        \n",
    "        if ema_model is None: raise NotImplementedError()\n",
    "        # with torch.no_grad():\n",
    "        #     x_tgt, _, _ = ema_model.forward_encoder(ft, ft_mask, 0.0, False)\n",
    "            \n",
    "        #     B = x.shape[0]\n",
    "        #     h_tgt = ema_model.enc_pooler[1](ema_model.enc_pooler[0](x_tgt).reshape(B, -1))\n",
    "        #     semisl_tgt = ema_model.enc_lbl_pred(h_tgt)[:, :num_lbls].detach()\n",
    "        #     if hard:\n",
    "        #         semisl_tgt[:, 0] = (semisl_tgt[:, 0] * 3.).round() / 3.\n",
    "        #         semisl_weight = 1.0\n",
    "        #     else:\n",
    "        #         semisl_weight = 0.1\n",
    "        \n",
    "        \n",
    "        # semisl_loss = (((enc_pred - semisl_tgt) ** 2) * nlbl_mask).sum() / (nlbl_mask.sum() + 1e-6)\n",
    "\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            if hard:\n",
    "                x_tgt, _, _ = ema_model.forward_encoder(ft, ft_mask, 0.0, False)\n",
    "                \n",
    "                B = x_tgt.shape[0]\n",
    "                h_tgt = ema_model.enc_pooler[1](ema_model.enc_pooler[0](x_tgt).reshape(B, -1))\n",
    "                semisl_tgt = ema_model.enc_lbl_pred(h_tgt)[:, :num_lbls].detach()\n",
    "                semisl_tgt[:, 0] = (semisl_tgt[:, 0] * 3.).round() / 3.\n",
    "                semisl_weight = 1.0\n",
    "                semisl_loss = (\n",
    "                    0.5 * (((enc_pred - semisl_tgt) ** 2) * nlbl_mask).sum() / (nlbl_mask.sum() + 1e-6) # all labels\n",
    "                    + 0.5 * (((enc_pred[:, 0] - semisl_tgt[:, 0]) ** 2) * nlbl_mask[:, 0]).sum() / (nlbl_mask[:, 0].sum() + 1e-6) # sii only\n",
    "                )\n",
    "            else:\n",
    "                semisl_weight = 0.0\n",
    "                semisl_loss = torch.tensor([0.0]).to(x.device)\n",
    "                \n",
    "\n",
    "        \n",
    "\n",
    "        sl_tgt = data[:, 0, lbl_cols]\n",
    "        sl_loss = (((enc_pred - sl_tgt) ** 2) * lbl_mask).sum() / (lbl_mask.sum() + 1e-6)\n",
    "        \n",
    "        loss = 1.0 * sl_loss + semisl_weight * semisl_loss\n",
    "        return loss, (sl_loss.item(), semisl_loss.item(),)\n",
    "      \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f362ee08",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T14:20:19.540259Z",
     "iopub.status.busy": "2024-12-19T14:20:19.539529Z",
     "iopub.status.idle": "2024-12-19T14:20:19.669916Z",
     "shell.execute_reply": "2024-12-19T14:20:19.669238Z"
    },
    "papermill": {
     "duration": 0.155827,
     "end_time": "2024-12-19T14:20:19.672036",
     "exception": false,
     "start_time": "2024-12-19T14:20:19.516209",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# stdlib\n",
    "from typing import Any, List, Tuple, Union\n",
    "\n",
    "# third party\n",
    "import numpy as np\n",
    "import math, sys, argparse\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from functools import partial\n",
    "import time, os, json\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "import sys\n",
    "import timm.optim.optim_factory as optim_factory\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "\n",
    "\n",
    "def quadratic_weighted_kappa(y_true, y_pred):\n",
    "    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n",
    "\n",
    "\n",
    "\n",
    "eps = 1e-8\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from argparse import Namespace\n",
    "remasker_args = Namespace(\n",
    "    batch_size=64,\n",
    "    max_epochs= 600,\n",
    "    accum_iter=1,\n",
    "    mask_ratio=0.5,\n",
    "    embed_dim=32,\n",
    "    depth=6,\n",
    "    decoder_depth=4,\n",
    "    num_heads=4,\n",
    "    mlp_ratio=4.0,\n",
    "    encode_func='linear',\n",
    "    norm_field_loss=False,\n",
    "    weight_decay=0.05,\n",
    "    lr=None, blr=0.001,\n",
    "    min_lr=1e-05,\n",
    "    warmup_epochs=40,\n",
    "    device='cuda', seed=SEED, overwrite=True, pin_mem=True\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "def set_dropout_p(m, p):\n",
    "    if isinstance(m, nn.Dropout):\n",
    "        m.p = p\n",
    "\n",
    "def update_ema_variables(model, ema_model, alpha, global_step, max_global_step):\n",
    "    # Use the true average until the exponential average is more correct\n",
    "    def f(alpha, t, T):\n",
    "        A = 1\n",
    "        B = alpha * T / (T - 1)\n",
    "        return (B * (1 - A / (t + 1)))\n",
    "    current_alpha = f(alpha, global_step, max_global_step)\n",
    "    for ema_param, param in zip(ema_model.parameters(), model.parameters()):\n",
    "        ema_param.data.mul_(current_alpha).add_(1 - current_alpha, param.data)\n",
    "\n",
    "\n",
    "class ReMasker:\n",
    "\n",
    "    def __init__(self, args=remasker_args):\n",
    "\n",
    "        self.batch_size = args.batch_size\n",
    "        self.accum_iter = args.accum_iter\n",
    "        self.min_lr = args.min_lr\n",
    "        self.norm_field_loss = args.norm_field_loss\n",
    "        # self.weight_decay = args.weight_decay\n",
    "        self.lr = args.lr\n",
    "        self.blr = args.blr\n",
    "        self.warmup_epochs = max(1, args.max_epochs // 10)\n",
    "        self.ema_decay = args.ema_decay\n",
    "        self.model = None\n",
    "        self.norm_parameters = None\n",
    "\n",
    "        self.embed_dim = args.embed_dim\n",
    "        self.depth = args.depth\n",
    "        self.decoder_depth = args.decoder_depth\n",
    "        self.num_heads = args.num_heads\n",
    "        self.mlp_ratio = args.mlp_ratio\n",
    "        self.cls_mlp_dim = args.cls_mlp_dim\n",
    "        self.max_epochs = args.max_epochs\n",
    "        self.mask_ratio = args.mask_ratio\n",
    "        self.encode_func = args.encode_func\n",
    "        self.dropout = args.dropout\n",
    "\n",
    "\n",
    "    def fit(self, X_raw: pd.DataFrame, X_val=None, lbl_cols=None, model=None):\n",
    "        global dbg_var\n",
    "        X = X_raw.clone()\n",
    "\n",
    "        # Parameters\n",
    "        no = len(X)\n",
    "        dim = len(X[0, :])\n",
    "\n",
    "        X = X.cpu()\n",
    "\n",
    "        min_val = np.zeros(dim)\n",
    "        max_val = np.zeros(dim)\n",
    "\n",
    "        for i in range(dim):\n",
    "            min_val[i] = np.nanmin(X[:, i])\n",
    "            max_val[i] = np.nanmax(X[:, i])\n",
    "            X[:, i] = (X[:, i] - min_val[i]) / (max_val[i] - min_val[i] + eps)\n",
    "\n",
    "        self.norm_parameters = {\"min\": min_val, \"max\": max_val}\n",
    "\n",
    "        # Set missing\n",
    "        M = 1 - (1 * (np.isnan(X)))\n",
    "        M = M.float().to(device)\n",
    "\n",
    "        X = torch.nan_to_num(X)\n",
    "        X = X.to(device)\n",
    "\n",
    "        if model is None:\n",
    "            self.model = MaskedAutoencoder(\n",
    "                rec_len=dim,\n",
    "                embed_dim=self.embed_dim,\n",
    "                depth=self.depth,\n",
    "                num_heads=self.num_heads,\n",
    "                decoder_embed_dim=self.embed_dim,\n",
    "                decoder_depth=self.decoder_depth,\n",
    "                decoder_num_heads=self.num_heads,\n",
    "                mlp_ratio=self.mlp_ratio,\n",
    "                cls_mlp_dim=self.cls_mlp_dim,\n",
    "                norm_layer=partial(nn.LayerNorm, eps=eps),\n",
    "                norm_field_loss=self.norm_field_loss,\n",
    "                encode_func=self.encode_func,\n",
    "                dropout=self.dropout,\n",
    "            )\n",
    "        else:\n",
    "            self.model = copy.deepcopy(model)\n",
    "            for param in self.model.blocks.layers[:].parameters():\n",
    "                param.detach_()\n",
    "        self.ema_model = copy.deepcopy(self.model)\n",
    "        self.ema_model.apply(lambda m: set_dropout_p(m, p=0.0))\n",
    "        for param in self.ema_model.parameters():\n",
    "            param.detach_()\n",
    "        \n",
    "\n",
    "        self.model.to(device)\n",
    "        self.ema_model.to(device).eval()\n",
    "\n",
    "        # set optimizers\n",
    "        # param_groups = optim_factory.add_weight_decay(model, args.weight_decay)\n",
    "        eff_batch_size = self.batch_size * self.accum_iter\n",
    "        if self.lr is None:  # only base_lr is specified\n",
    "            self.lr = self.blr * eff_batch_size / 64\n",
    "        # param_groups = optim_factory.add_weight_decay(self.model, self.weight_decay)\n",
    "        # optimizer = torch.optim.AdamW(param_groups, lr=self.lr, betas=(0.9, 0.95))\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.lr, betas=(0.9, 0.95))\n",
    "        loss_scaler = NativeScaler()\n",
    "\n",
    "        dataset = MAEDataset(X, M)\n",
    "        dataloader = DataLoader(\n",
    "            dataset, sampler=RandomSampler(dataset),\n",
    "            batch_size=self.batch_size,\n",
    "        )\n",
    "\n",
    "\n",
    "        best_loss = 1e9\n",
    "        best_model = copy.deepcopy(self.model)\n",
    "        for epoch in range(self.max_epochs):\n",
    "            self.model.train()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            total_loss = 0\n",
    "            lbl_loss = 0.\n",
    "            \n",
    "            \n",
    "            import time\n",
    "            dbgt1 = 0\n",
    "            dbgt2 = 0\n",
    "            dbgt3 = 0\n",
    "\n",
    "            iter = 0\n",
    "            for iter, (samples, masks) in enumerate(dataloader):\n",
    "\n",
    "                # we use a per iteration (instead of per epoch) lr scheduler\n",
    "                if iter % self.accum_iter == 0:\n",
    "                    adjust_learning_rate(optimizer, iter / len(dataloader) + epoch, self.lr, self.min_lr,\n",
    "                                         self.max_epochs, self.warmup_epochs)\n",
    "\n",
    "                samples = samples.unsqueeze(dim=1)\n",
    "                samples = samples.to(device, non_blocking=True)\n",
    "                masks = masks.to(device, non_blocking=True)\n",
    "\n",
    "                # print(samples, masks)\n",
    "\n",
    "                # with torch.cuda.amp.autocast():\n",
    "\n",
    "                if lbl_cols is not None:\n",
    "                    input_samples = samples.clone()\n",
    "                    input_masks = masks.clone()\n",
    "                    \n",
    "                    \n",
    "                    # selfsl_loss, dbg_selfsl_loss = self.model.forward_selfsl(samples, masks, mask_ratio=self.mask_ratio)\n",
    "                    \n",
    "                    # sl_loss, dbg_sl_loss = self.model.forward_sl(input_samples, input_masks, lbl_cols)\n",
    "                    \n",
    "                    \n",
    "                    hard = True if (epoch >= (self.max_epochs // 2)) else False\n",
    "                    semisl_loss, dbg_semisl_loss = self.model.forward_semisl(input_samples, input_masks, lbl_cols, ema_model=self.ema_model, hard=hard)\n",
    "                    loss = 1.0 * semisl_loss\n",
    "                        \n",
    "                else:\n",
    "\n",
    "                    selfsl_loss, dbg_selfsl_loss = self.model.forward_selfsl(samples, masks, mask_ratio=self.mask_ratio)\n",
    "                    loss = selfsl_loss\n",
    "                \n",
    "                loss_value = loss.item()\n",
    "                total_loss += loss_value\n",
    "                if not math.isfinite(loss_value):\n",
    "                    print(\"Loss is {}, stopping training\".format(loss_value))\n",
    "                    dbg_var = (samples, masks)\n",
    "                    sys.exit(1)\n",
    "\n",
    "                loss /= self.accum_iter\n",
    "                loss_scaler(loss, optimizer, parameters=self.model.parameters(),\n",
    "                            update_grad=(iter + 1) % self.accum_iter == 0)\n",
    "\n",
    "                if (iter + 1) % self.accum_iter == 0:\n",
    "                    optimizer.zero_grad()\n",
    "                    \n",
    "                \n",
    "            update_ema_variables(self.model, self.ema_model, self.ema_decay, epoch, self.max_epochs)\n",
    "            # print(dbgt1)\n",
    "            # print(dbgt2)\n",
    "            # print(dbgt3)\n",
    "            total_loss = (total_loss / (iter + 1))\n",
    "            self.model.eval()\n",
    "            if X_val is not None:\n",
    "                val_loss = self.evaluate(X_val, lbl_cols)\n",
    "            else:\n",
    "                val_loss = total_loss\n",
    "            if val_loss <= best_loss:\n",
    "                best_loss = val_loss\n",
    "                best_model = copy.deepcopy(self.model)\n",
    "            if (epoch + 1) % max(1, self.max_epochs // 10) == 0 or epoch == 0:\n",
    "                lbl_loss = lbl_loss / (iter + 1)\n",
    "                \n",
    "                if lbl_cols is not None:\n",
    "                    print(\"Epoch: %d, train;val;best qwk: %.4f;%.4f;%.4f, loss: %.4f, val_loss: %.4f\" % \n",
    "                        (epoch+1, -self.evaluate(X_raw, lbl_cols), -val_loss, -best_loss, total_loss, val_loss)\n",
    "                    )\n",
    "                else:\n",
    "                    print(\"Epoch: %d, loss: %.4f\" % \n",
    "                        (epoch+1, -best_loss)\n",
    "                    )\n",
    "                    \n",
    "                \n",
    "\n",
    "        self.model = best_model\n",
    "        print(f'Loaded best model with loss={best_loss:.4f}')\n",
    "        # torch.save(self.model.state_dict(), self.path)\n",
    "        return self\n",
    "      \n",
    "      \n",
    "      \n",
    "    def evaluate(self, X_raw: torch.Tensor, lbl_cols):\n",
    "        keep_indices = torch.where(~X_raw[:, lbl_cols].isnan())[0]\n",
    "        X_raw = X_raw[keep_indices]\n",
    "        gt = X_raw[:, lbl_cols[0]].cpu().numpy().round(0).astype(int)\n",
    "        X_raw[:, lbl_cols] = float('nan')\n",
    "        yp = self.predict(X_raw, lbl_cols)\n",
    "        yp = yp.cpu().numpy().round(0).astype(int)\n",
    "        return -quadratic_weighted_kappa(gt, yp)\n",
    "      \n",
    "      \n",
    "    def predict(self, X_raw: torch.Tensor, lbl_idx, bs=None):\n",
    "        X_raw = torch.tensor(X_raw, dtype=torch.float32)\n",
    "        \n",
    "        # Normalize the input data\n",
    "        min_val = self.norm_parameters[\"min\"]\n",
    "        max_val = self.norm_parameters[\"max\"]\n",
    "        X = X_raw.clone()\n",
    "        for i in range(X.shape[1]):\n",
    "            X[:, i] = (X[:, i] - min_val[i]) / (max_val[i] - min_val[i] + eps)\n",
    "        \n",
    "        M = (1 - (1 * torch.isnan(X))).float().to(device)\n",
    "        \n",
    "        X = torch.nan_to_num(X)\n",
    "        X = X.to(device)\n",
    "        \n",
    "        if bs == None: bs = self.batch_size\n",
    "        # Prepare DataLoader\n",
    "        dataset = MAEDataset(X, M)\n",
    "        dataloader = DataLoader(dataset, batch_size=bs, shuffle=False)\n",
    "        \n",
    "        # Ensure model is in evaluation mode\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Tensor to hold predictions\n",
    "        predictions = torch.zeros(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_samples, batch_masks in dataloader:\n",
    "                # Prepare input for the model\n",
    "                batch_samples = batch_samples.unsqueeze(dim=1).to(device)\n",
    "                batch_masks = batch_masks.to(device)\n",
    "                \n",
    "                # Forward pass with training=False\n",
    "                pred = self.model.forward(batch_samples, batch_masks)\n",
    "                \n",
    "                pred = pred.reshape(-1)\n",
    "                \n",
    "                predictions = torch.cat((predictions, pred), 0)\n",
    "        \n",
    "        return predictions * 3.\n",
    "\n",
    "\n",
    "    # def transform(self, X_raw: torch.Tensor):\n",
    "\n",
    "    #     X = X_raw.clone()\n",
    "\n",
    "    #     min_val = self.norm_parameters[\"min\"]\n",
    "    #     max_val = self.norm_parameters[\"max\"]\n",
    "\n",
    "    #     no, dim = X.shape\n",
    "    #     X = X.cpu()\n",
    "\n",
    "    #     # MinMaxScaler normalization\n",
    "    #     for i in range(dim):\n",
    "    #         X[:, i] = (X[:, i] - min_val[i]) / (max_val[i] - min_val[i] + eps)\n",
    "\n",
    "    #     # Set missing\n",
    "    #     M = 1 - (1 * (np.isnan(X)))\n",
    "    #     X = np.nan_to_num(X)\n",
    "\n",
    "    #     X = torch.from_numpy(X).to(device)\n",
    "    #     M = M.to(device)\n",
    "\n",
    "    #     self.model.eval()\n",
    "\n",
    "    #     bs = 64\n",
    "    #     # Imputed data\n",
    "    #     with torch.no_grad():\n",
    "    #         for i in range(0, no, bs):\n",
    "    #             sample = X[i:i+bs][:, None]\n",
    "    #             mask = M[i:i+bs]\n",
    "    #             _, pred, _, _ = self.model(sample, mask, 0.5, False)\n",
    "    #             pred = pred[:, :, 0]\n",
    "    #             if i == 0:\n",
    "    #                 imputed_data = pred\n",
    "    #             else:\n",
    "    #                 imputed_data = torch.cat((imputed_data, pred), 0)\n",
    "    #     print(imputed_data.shape)\n",
    "                    \n",
    "    #                 # Renormalize\n",
    "    #     for i in range(dim):\n",
    "    #         imputed_data[:, i] = imputed_data[:, i] * (max_val[i] - min_val[i] + eps) + min_val[i]\n",
    "\n",
    "\n",
    "    #     if np.all(np.isnan(imputed_data.detach().cpu().numpy())):\n",
    "    #         err = \"The imputed result contains nan. This is a bug. Please report it on the issue tracker.\"\n",
    "    #         raise RuntimeError(err)\n",
    "\n",
    "    #     M = M.cpu()\n",
    "    #     imputed_data = imputed_data.detach().cpu()\n",
    "    #     # print('imputed', imputed_data, M)\n",
    "    #     # print('imputed', M * np.nan_to_num(X_raw.cpu()) + (1 - M) * imputed_data)\n",
    "    #     return imputed_data\n",
    "    \n",
    "    \n",
    "    def fit_transform(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Imputes the provided dataset using the GAIN strategy.\n",
    "        Args:\n",
    "            X: np.ndarray\n",
    "                A dataset with missing values.\n",
    "        Returns:\n",
    "            Xhat: The imputed dataset.\n",
    "        \"\"\"\n",
    "        X = torch.tensor(X.values, dtype=torch.float32)\n",
    "        return self.fit(X).transform(X).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f046cd02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T14:20:19.716186Z",
     "iopub.status.busy": "2024-12-19T14:20:19.715797Z",
     "iopub.status.idle": "2024-12-19T14:20:19.730433Z",
     "shell.execute_reply": "2024-12-19T14:20:19.729638Z"
    },
    "papermill": {
     "duration": 0.038901,
     "end_time": "2024-12-19T14:20:19.732300",
     "exception": false,
     "start_time": "2024-12-19T14:20:19.693399",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "full_df = pd.concat([train,test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "275271cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T14:20:19.780565Z",
     "iopub.status.busy": "2024-12-19T14:20:19.780218Z",
     "iopub.status.idle": "2024-12-19T14:20:19.818207Z",
     "shell.execute_reply": "2024-12-19T14:20:19.817515Z"
    },
    "papermill": {
     "duration": 0.064939,
     "end_time": "2024-12-19T14:20:19.819940",
     "exception": false,
     "start_time": "2024-12-19T14:20:19.755001",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_raw = torch.tensor(full_df.to_numpy()).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6ee62a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T14:20:19.864598Z",
     "iopub.status.busy": "2024-12-19T14:20:19.863825Z",
     "iopub.status.idle": "2024-12-19T14:20:19.869416Z",
     "shell.execute_reply": "2024-12-19T14:20:19.868156Z"
    },
    "papermill": {
     "duration": 0.029528,
     "end_time": "2024-12-19T14:20:19.871278",
     "exception": false,
     "start_time": "2024-12-19T14:20:19.841750",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3980, 177])\n"
     ]
    }
   ],
   "source": [
    "print(X_raw.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "352dcba4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T14:20:19.916370Z",
     "iopub.status.busy": "2024-12-19T14:20:19.915688Z",
     "iopub.status.idle": "2024-12-19T14:20:19.938422Z",
     "shell.execute_reply": "2024-12-19T14:20:19.937657Z"
    },
    "papermill": {
     "duration": 0.046754,
     "end_time": "2024-12-19T14:20:19.940029",
     "exception": false,
     "start_time": "2024-12-19T14:20:19.893275",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5000, 177])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def random_extend(arr, k):\n",
    "    indices = np.concatenate([np.random.permutation(len(arr)) for _ in range(10)])[:k]\n",
    "    return arr[indices]\n",
    "\n",
    "random_extend(X_raw, 5000).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42581d48",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T14:20:19.984228Z",
     "iopub.status.busy": "2024-12-19T14:20:19.983588Z",
     "iopub.status.idle": "2024-12-19T14:20:19.999648Z",
     "shell.execute_reply": "2024-12-19T14:20:19.998925Z"
    },
    "papermill": {
     "duration": 0.039674,
     "end_time": "2024-12-19T14:20:20.001377",
     "exception": false,
     "start_time": "2024-12-19T14:20:19.961703",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "for c in full_df.columns:\n",
    "    if c not in test.columns:\n",
    "        test[c] = float('nan')\n",
    "\n",
    "test = test[full_df.columns]\n",
    "\n",
    "X_tensor_test = torch.tensor(test.to_numpy()).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "845ab059",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T14:20:20.045473Z",
     "iopub.status.busy": "2024-12-19T14:20:20.045111Z",
     "iopub.status.idle": "2024-12-19T14:20:20.051046Z",
     "shell.execute_reply": "2024-12-19T14:20:20.050278Z"
    },
    "papermill": {
     "duration": 0.030413,
     "end_time": "2024-12-19T14:20:20.052897",
     "exception": false,
     "start_time": "2024-12-19T14:20:20.022484",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df.columns.get_loc('sii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba27f792",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T14:20:20.098681Z",
     "iopub.status.busy": "2024-12-19T14:20:20.098375Z",
     "iopub.status.idle": "2024-12-19T14:20:20.103297Z",
     "shell.execute_reply": "2024-12-19T14:20:20.102463Z"
    },
    "papermill": {
     "duration": 0.03041,
     "end_time": "2024-12-19T14:20:20.105374",
     "exception": false,
     "start_time": "2024-12-19T14:20:20.074964",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_model_size(model):\n",
    "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    model_size = sum([np.prod(p.size()) for p in model_parameters])\n",
    "    return \"{}K\".format(round(model_size / 1e1) / 1e2)\n",
    "  \n",
    "# get_model_size(imputer.model.blocks.layers[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e37e079f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T14:20:20.157062Z",
     "iopub.status.busy": "2024-12-19T14:20:20.156789Z",
     "iopub.status.idle": "2024-12-19T14:20:20.175038Z",
     "shell.execute_reply": "2024-12-19T14:20:20.174289Z"
    },
    "papermill": {
     "duration": 0.043554,
     "end_time": "2024-12-19T14:20:20.176923",
     "exception": false,
     "start_time": "2024-12-19T14:20:20.133369",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.base import clone\n",
    "\n",
    "def quadratic_weighted_kappa(y_true, y_pred):\n",
    "    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n",
    "\n",
    "def threshold_Rounder(oof_non_rounded, thresholds):\n",
    "    return np.where(oof_non_rounded < thresholds[0], 0,\n",
    "                    np.where(oof_non_rounded < thresholds[1], 1,\n",
    "                             np.where(oof_non_rounded < thresholds[2], 2, 3)))\n",
    "\n",
    "def evaluate_predictions(thresholds, y_true, oof_non_rounded):\n",
    "    rounded_p = threshold_Rounder(oof_non_rounded, thresholds)\n",
    "    return -quadratic_weighted_kappa(y_true, rounded_p)\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "def random_extend(arr, k):\n",
    "    indices = np.concatenate([np.random.permutation(len(arr)) for _ in range(10)])[:k]\n",
    "    return arr[indices]\n",
    "\n",
    "\n",
    "num_folds = 5\n",
    "\n",
    "def PerformImpute(imputer_args):\n",
    "    global X_raw, X_tensor_test, num_folds\n",
    "    train_S = []\n",
    "    test_S = []\n",
    "    \n",
    "    KF = KFold(n_splits=num_folds, shuffle=True, random_state=SEED)\n",
    "\n",
    "    oof_non_rounded = []\n",
    "    oof_rounded = []\n",
    "    oof_gt = []\n",
    "    test_preds = np.zeros((len(X_tensor_test), num_folds))\n",
    "    \n",
    "    \n",
    "    lbl_cols = [full_df.columns.get_loc(c) for c in full_df.columns if 'PCIAT' in c or 'sii' in c]\n",
    "    lbl_idx = lbl_cols[0]\n",
    "    \n",
    "    lbled_indices = torch.where(~X_raw[:, lbl_idx].isnan())[0]\n",
    "    \n",
    "    \n",
    "    \n",
    "    X_raw_no_lbl = X_raw.clone()\n",
    "    X_raw_no_lbl[:, lbl_cols] = float('nan')\n",
    "    \n",
    "    pretrain_args = copy.deepcopy(imputer_args)\n",
    "    pretrain_args.max_epochs = pretrain_args.pretrain_epochs\n",
    "    import time\n",
    "    pretrain_tick = time.time()\n",
    "    imputer_pretrain = ReMasker(pretrain_args)\n",
    "    imputer_pretrain.fit(X_raw_no_lbl, None, None, None)\n",
    "    ellapsed_time = time.time() - pretrain_tick\n",
    "    print(f\"Pretrained in {ellapsed_time:.4f}s.\")\n",
    "    \n",
    "    pretrain_model = imputer_pretrain.model\n",
    "\n",
    "    \n",
    "\n",
    "    pbar = tqdm(KF.split(lbled_indices), desc=\"Training Folds\", total=n_splits)    \n",
    "\n",
    "    for fold, (train_idx_idx, test_idx_idx) in enumerate(pbar):\n",
    "        train_idx = lbled_indices[train_idx_idx]\n",
    "        test_idx = lbled_indices[test_idx_idx]\n",
    "\n",
    "        X_train = X_raw.clone()\n",
    "        \n",
    "        \n",
    "        \n",
    "        X_train[test_idx.unsqueeze(1), lbl_cols] = float('nan')\n",
    "        \n",
    "        X_val = X_raw[test_idx].clone()\n",
    "\n",
    "        \n",
    "\n",
    "        X_train = random_extend(X_train, 9000)\n",
    "        # X_val = random_extend(X_val, 2000)\n",
    "        \n",
    "    \n",
    "        train_nonna_indices = torch.where(~X_train[:, lbl_idx].isnan())[0]\n",
    "        val_nonna_indices = torch.where(~X_val[:, lbl_idx].isnan())[0]\n",
    "        if len(train_nonna_indices) == 0 or len(val_nonna_indices)==0: continue\n",
    "    \n",
    "        imputer = ReMasker(imputer_args)\n",
    "        imputer.fit(X_train, X_val, lbl_cols, pretrain_model)\n",
    "\n",
    "        y_train_ = X_train[train_nonna_indices, lbl_idx].numpy().astype(int)\n",
    "        y_val_ = X_val[val_nonna_indices, lbl_idx].numpy().astype(int)\n",
    "\n",
    "        X_train[:, lbl_cols] = float('nan')\n",
    "        X_val[:, lbl_cols] = float('nan')\n",
    "        \n",
    "        y_train_pred = imputer.predict(X_train[train_nonna_indices], lbl_cols).cpu().detach().numpy()\n",
    "        y_val_pred = imputer.predict(X_val[val_nonna_indices], lbl_cols).cpu().detach().numpy()\n",
    "        y_test_pred = imputer.predict(X_tensor_test, lbl_cols).cpu().detach().numpy()\n",
    "\n",
    "        # model = clone(model_init)\n",
    "\n",
    "        # model.fit(X_train_, y_train_)\n",
    "\n",
    "        oof_non_rounded += [y_val_pred]\n",
    "        y_val_pred_rounded = y_val_pred.round(0).astype(int)\n",
    "        oof_rounded += [y_val_pred_rounded]\n",
    "        oof_gt += [y_val_]\n",
    "        \n",
    "        train_kappa = quadratic_weighted_kappa(y_train_, y_train_pred.round(0).astype(int))\n",
    "        val_kappa = quadratic_weighted_kappa(y_val_, y_val_pred.round(0).astype(int))\n",
    "\n",
    "        train_S.append(train_kappa)\n",
    "        test_S.append(val_kappa)\n",
    "        \n",
    "        \n",
    "        test_preds[:, fold] = y_test_pred\n",
    "\n",
    "        pbar.set_description_str(\n",
    "          \"Fold %d, Train MSE: %.4f, Val MSE: %.4f, Train QWK: %.4f, Val QWK: %.4f\" % (\n",
    "              fold + 1,\n",
    "              ((y_train_pred - y_train_) ** 2 / 9.).mean(),\n",
    "              ((y_val_pred - y_val_) ** 2 / 9.).mean(),\n",
    "              train_kappa,\n",
    "              val_kappa\n",
    "          )\n",
    "        )\n",
    "    \n",
    "    \n",
    "    print(f\"Mean Train QWK --> {np.mean(train_S):.4f}\")\n",
    "    print(f\"Mean Validation QWK ---> {np.mean(test_S):.4f}\")\n",
    "\n",
    "    oof_non_rounded = np.concatenate(oof_non_rounded)\n",
    "    oof_gt = np.concatenate(oof_gt)\n",
    "    KappaOPtimizer = minimize(evaluate_predictions,\n",
    "                              x0=[0.5, 1.5, 2.5], args=(oof_gt, oof_non_rounded), \n",
    "                              method='Nelder-Mead')\n",
    "    assert KappaOPtimizer.success, \"Optimization did not converge.\"\n",
    "    \n",
    "    oof_tuned = threshold_Rounder(oof_non_rounded, KappaOPtimizer.x)\n",
    "    tKappa = quadratic_weighted_kappa(oof_gt, oof_tuned)\n",
    "\n",
    "    print(f\"----> || Optimized QWK SCORE :: {Fore.CYAN}{Style.BRIGHT} {tKappa:.3f}{Style.RESET_ALL}\")\n",
    "\n",
    "\n",
    "    # return 0.5 * (np.mean(test_S) + tKappa)\n",
    "  \n",
    "  \n",
    "    tpm = test_preds.mean(axis=1)\n",
    "    tpTuned = threshold_Rounder(tpm, KappaOPtimizer.x)\n",
    "    \n",
    "    sample_sub_df = pd.read_csv('../input/child-mind-institute-problematic-internet-use/sample_submission.csv')\n",
    "    submission = pd.DataFrame({\n",
    "        'id': sample_sub_df['id'],\n",
    "        'sii': tpTuned\n",
    "    })\n",
    "\n",
    "    return submission\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0050b85",
   "metadata": {
    "papermill": {
     "duration": 0.020602,
     "end_time": "2024-12-19T14:20:20.219274",
     "exception": false,
     "start_time": "2024-12-19T14:20:20.198672",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae7893a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T14:20:20.262475Z",
     "iopub.status.busy": "2024-12-19T14:20:20.262172Z",
     "iopub.status.idle": "2024-12-19T15:50:31.218769Z",
     "shell.execute_reply": "2024-12-19T15:50:31.217757Z"
    },
    "papermill": {
     "duration": 5410.980262,
     "end_time": "2024-12-19T15:50:31.220795",
     "exception": false,
     "start_time": "2024-12-19T14:20:20.240533",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=64, max_epochs=50, pretrain_epochs=400, accum_iter=1, mask_ratio=0.75, embed_dim=6, depth=8, decoder_depth=1, num_heads=3, mlp_ratio=21.5, cls_mlp_dim=48, dropout=0.5, encode_func='linear', norm_field_loss=False, ema_decay=0.9, weight_decay=0.05, lr=None, blr=0.001, min_lr=1e-05, device='cuda', seed=314159, overwrite=True, pin_mem=True)\n",
      "Epoch: 1, loss: -0.3139\n",
      "Epoch: 40, loss: -0.2113\n",
      "Epoch: 80, loss: -0.1535\n",
      "Epoch: 120, loss: -0.0766\n",
      "Epoch: 160, loss: -0.0644\n",
      "Epoch: 200, loss: -0.0611\n",
      "Epoch: 240, loss: -0.0595\n",
      "Epoch: 280, loss: -0.0582\n",
      "Epoch: 320, loss: -0.0573\n",
      "Epoch: 360, loss: -0.0564\n",
      "Epoch: 400, loss: -0.0564\n",
      "Loaded best model with loss=0.0564\n",
      "Pretrained in 1812.8070s.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Folds:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, train;val;best qwk: -0.0032;-0.0253;-0.0253, loss: 0.1604, val_loss: 0.0253\n",
      "Epoch: 5, train;val;best qwk: 0.3683;0.2932;0.2932, loss: 0.0653, val_loss: -0.2932\n",
      "Epoch: 10, train;val;best qwk: 0.4466;0.3903;0.3932, loss: 0.0587, val_loss: -0.3903\n",
      "Epoch: 15, train;val;best qwk: 0.5303;0.3865;0.4014, loss: 0.0547, val_loss: -0.3865\n",
      "Epoch: 20, train;val;best qwk: 0.5878;0.3622;0.4176, loss: 0.0505, val_loss: -0.3622\n",
      "Epoch: 25, train;val;best qwk: 0.6753;0.3740;0.4176, loss: 0.0472, val_loss: -0.3740\n",
      "Epoch: 30, train;val;best qwk: 0.7138;0.3826;0.4176, loss: 0.0527, val_loss: -0.3826\n",
      "Epoch: 35, train;val;best qwk: 0.7268;0.3623;0.4176, loss: 0.0504, val_loss: -0.3623\n",
      "Epoch: 40, train;val;best qwk: 0.7530;0.3616;0.4176, loss: 0.0489, val_loss: -0.3616\n",
      "Epoch: 45, train;val;best qwk: 0.7527;0.3593;0.4176, loss: 0.0476, val_loss: -0.3593\n",
      "Epoch: 50, train;val;best qwk: 0.7609;0.3735;0.4176, loss: 0.0475, val_loss: -0.3735\n",
      "Loaded best model with loss=-0.4176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 1, Train MSE: 0.0367, Val MSE: 0.0529, Train QWK: 0.5939, Val QWK: 0.4178:  20%|██        | 1/5 [12:00<48:02, 720.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, train;val;best qwk: -0.0087;-0.0256;-0.0256, loss: 0.1610, val_loss: 0.0256\n",
      "Epoch: 5, train;val;best qwk: 0.3700;0.2847;0.2847, loss: 0.0658, val_loss: -0.2847\n",
      "Epoch: 10, train;val;best qwk: 0.4536;0.3580;0.3808, loss: 0.0586, val_loss: -0.3580\n",
      "Epoch: 15, train;val;best qwk: 0.5272;0.3597;0.3808, loss: 0.0553, val_loss: -0.3597\n",
      "Epoch: 20, train;val;best qwk: 0.5545;0.3202;0.3912, loss: 0.0512, val_loss: -0.3202\n",
      "Epoch: 25, train;val;best qwk: 0.6333;0.3657;0.3912, loss: 0.0479, val_loss: -0.3657\n",
      "Epoch: 30, train;val;best qwk: 0.6754;0.3459;0.3912, loss: 0.0521, val_loss: -0.3459\n",
      "Epoch: 35, train;val;best qwk: 0.7196;0.3378;0.3912, loss: 0.0502, val_loss: -0.3378\n",
      "Epoch: 40, train;val;best qwk: 0.7362;0.3381;0.3912, loss: 0.0487, val_loss: -0.3381\n",
      "Epoch: 45, train;val;best qwk: 0.7459;0.3436;0.3912, loss: 0.0476, val_loss: -0.3436\n",
      "Epoch: 50, train;val;best qwk: 0.7450;0.3462;0.3912, loss: 0.0472, val_loss: -0.3462\n",
      "Loaded best model with loss=-0.3912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 2, Train MSE: 0.0377, Val MSE: 0.0536, Train QWK: 0.5940, Val QWK: 0.3917:  40%|████      | 2/5 [24:00<36:01, 720.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, train;val;best qwk: -0.0102;0.0121;0.0121, loss: 0.1625, val_loss: -0.0121\n",
      "Epoch: 5, train;val;best qwk: 0.3176;0.3025;0.3172, loss: 0.0664, val_loss: -0.3025\n",
      "Epoch: 10, train;val;best qwk: 0.4436;0.3860;0.4168, loss: 0.0597, val_loss: -0.3860\n",
      "Epoch: 15, train;val;best qwk: 0.5098;0.3902;0.4168, loss: 0.0551, val_loss: -0.3902\n",
      "Epoch: 20, train;val;best qwk: 0.5802;0.3651;0.4168, loss: 0.0512, val_loss: -0.3651\n",
      "Epoch: 25, train;val;best qwk: 0.6708;0.3130;0.4168, loss: 0.0479, val_loss: -0.3130\n",
      "Epoch: 30, train;val;best qwk: 0.7206;0.3279;0.4168, loss: 0.0529, val_loss: -0.3279\n",
      "Epoch: 35, train;val;best qwk: 0.7213;0.2910;0.4168, loss: 0.0507, val_loss: -0.2910\n",
      "Epoch: 40, train;val;best qwk: 0.7544;0.3054;0.4168, loss: 0.0491, val_loss: -0.3054\n",
      "Epoch: 45, train;val;best qwk: 0.7518;0.3053;0.4168, loss: 0.0476, val_loss: -0.3053\n",
      "Epoch: 50, train;val;best qwk: 0.7593;0.3038;0.4168, loss: 0.0476, val_loss: -0.3038\n",
      "Loaded best model with loss=-0.4168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 3, Train MSE: 0.0487, Val MSE: 0.0522, Train QWK: 0.4061, Val QWK: 0.4158:  60%|██████    | 3/5 [36:01<24:00, 720.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, train;val;best qwk: -0.0042;-0.0005;-0.0005, loss: 0.1630, val_loss: 0.0005\n",
      "Epoch: 5, train;val;best qwk: 0.3447;0.3485;0.3485, loss: 0.0657, val_loss: -0.3485\n",
      "Epoch: 10, train;val;best qwk: 0.4506;0.4314;0.4314, loss: 0.0595, val_loss: -0.4314\n",
      "Epoch: 15, train;val;best qwk: 0.5558;0.4536;0.4536, loss: 0.0554, val_loss: -0.4536\n",
      "Epoch: 20, train;val;best qwk: 0.5798;0.3858;0.4536, loss: 0.0521, val_loss: -0.3858\n",
      "Epoch: 25, train;val;best qwk: 0.6272;0.3811;0.4536, loss: 0.0486, val_loss: -0.3811\n",
      "Epoch: 30, train;val;best qwk: 0.6858;0.3802;0.4536, loss: 0.0529, val_loss: -0.3802\n",
      "Epoch: 35, train;val;best qwk: 0.7057;0.3764;0.4536, loss: 0.0509, val_loss: -0.3764\n",
      "Epoch: 40, train;val;best qwk: 0.7282;0.3769;0.4536, loss: 0.0495, val_loss: -0.3769\n",
      "Epoch: 45, train;val;best qwk: 0.7270;0.3685;0.4536, loss: 0.0488, val_loss: -0.3685\n",
      "Epoch: 50, train;val;best qwk: 0.7285;0.3722;0.4536, loss: 0.0482, val_loss: -0.3722\n",
      "Loaded best model with loss=-0.4536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 4, Train MSE: 0.0419, Val MSE: 0.0522, Train QWK: 0.5556, Val QWK: 0.4536:  80%|████████  | 4/5 [48:00<11:59, 719.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, train;val;best qwk: -0.0114;-0.0037;-0.0037, loss: 0.1604, val_loss: 0.0037\n",
      "Epoch: 5, train;val;best qwk: 0.3127;0.3558;0.3558, loss: 0.0657, val_loss: -0.3558\n",
      "Epoch: 10, train;val;best qwk: 0.4442;0.3569;0.3842, loss: 0.0589, val_loss: -0.3569\n",
      "Epoch: 15, train;val;best qwk: 0.5453;0.3871;0.4184, loss: 0.0543, val_loss: -0.3871\n",
      "Epoch: 20, train;val;best qwk: 0.6195;0.3714;0.4184, loss: 0.0506, val_loss: -0.3714\n",
      "Epoch: 25, train;val;best qwk: 0.6670;0.3813;0.4184, loss: 0.0478, val_loss: -0.3813\n",
      "Epoch: 30, train;val;best qwk: 0.7028;0.3859;0.4184, loss: 0.0536, val_loss: -0.3859\n",
      "Epoch: 35, train;val;best qwk: 0.7199;0.3821;0.4184, loss: 0.0510, val_loss: -0.3821\n",
      "Epoch: 40, train;val;best qwk: 0.7384;0.3759;0.4184, loss: 0.0493, val_loss: -0.3759\n",
      "Epoch: 45, train;val;best qwk: 0.7446;0.3697;0.4184, loss: 0.0477, val_loss: -0.3697\n",
      "Epoch: 50, train;val;best qwk: 0.7491;0.3726;0.4184, loss: 0.0475, val_loss: -0.3726\n",
      "Loaded best model with loss=-0.4184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 5, Train MSE: 0.0434, Val MSE: 0.0527, Train QWK: 0.5084, Val QWK: 0.4179: 100%|██████████| 5/5 [59:58<00:00, 719.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Train QWK --> 0.5316\n",
      "Mean Validation QWK ---> 0.4193\n",
      "----> || Optimized QWK SCORE :: \u001b[36m\u001b[1m 0.451\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sii</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00008ff9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000fd460</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00105258</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00115b9f</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0016bb22</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>001f3379</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0038ba98</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0068a485</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0069fbed</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0083e397</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0087dd65</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>00abe655</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>00ae59c9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>00af6387</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>00bd4359</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>00c0cd71</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>00d56d4b</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>00d9913d</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>00e6167c</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>00ebc35d</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  sii\n",
       "0   00008ff9    0\n",
       "1   000fd460    0\n",
       "2   00105258    1\n",
       "3   00115b9f    0\n",
       "4   0016bb22    1\n",
       "5   001f3379    0\n",
       "6   0038ba98    0\n",
       "7   0068a485    0\n",
       "8   0069fbed    1\n",
       "9   0083e397    1\n",
       "10  0087dd65    0\n",
       "11  00abe655    1\n",
       "12  00ae59c9    1\n",
       "13  00af6387    0\n",
       "14  00bd4359    1\n",
       "15  00c0cd71    0\n",
       "16  00d56d4b    0\n",
       "17  00d9913d    0\n",
       "18  00e6167c    0\n",
       "19  00ebc35d    1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputer_args = Namespace(\n",
    "    batch_size=64,\n",
    "    max_epochs= 50,\n",
    "    pretrain_epochs=400,\n",
    "    accum_iter=1,\n",
    "    mask_ratio=0.75,\n",
    "    embed_dim=6,\n",
    "    depth=8,\n",
    "    decoder_depth=1,\n",
    "    num_heads=3,\n",
    "    mlp_ratio=21.5,\n",
    "    cls_mlp_dim=48,\n",
    "    dropout=0.5,\n",
    "    encode_func='linear',\n",
    "    norm_field_loss=False,\n",
    "    ema_decay=0.9,\n",
    "    weight_decay=0.05,\n",
    "    lr=None, blr=0.001,\n",
    "    min_lr=1e-05,\n",
    "    device='cuda', seed=SEED, overwrite=True, pin_mem=True\n",
    ")\n",
    "print(imputer_args)\n",
    "\n",
    "\n",
    "\n",
    "SEED = random.randint(1, int(2e9))\n",
    "np.random.seed(SEED)\n",
    "indices = np.random.permutation(len(X_raw))\n",
    "X_raw = X_raw[indices]\n",
    "sub1 = PerformImpute(imputer_args)\n",
    "sub1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09bf3ecf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T15:50:31.273407Z",
     "iopub.status.busy": "2024-12-19T15:50:31.273054Z",
     "iopub.status.idle": "2024-12-19T15:51:45.893502Z",
     "shell.execute_reply": "2024-12-19T15:51:45.892615Z"
    },
    "papermill": {
     "duration": 74.648739,
     "end_time": "2024-12-19T15:51:45.895055",
     "exception": false,
     "start_time": "2024-12-19T15:50:31.246316",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 996/996 [01:14<00:00, 13.39it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 12.12it/s]\n"
     ]
    }
   ],
   "source": [
    "def process_file(filename, dirname):\n",
    "    df = pd.read_parquet(os.path.join(dirname, filename, 'part-0.parquet'))\n",
    "    df.drop('step', axis=1, inplace=True)\n",
    "    return df.describe().values.reshape(-1), filename.split('=')[1]\n",
    "\n",
    "def load_time_series(dirname) -> pd.DataFrame:\n",
    "    ids = os.listdir(dirname)\n",
    "    \n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        results = list(tqdm(executor.map(lambda fname: process_file(fname, dirname), ids), total=len(ids)))\n",
    "    \n",
    "    stats, indexes = zip(*results)\n",
    "    \n",
    "    df = pd.DataFrame(stats, columns=[f\"stat_{i}\" for i in range(len(stats[0]))])\n",
    "    df['id'] = indexes\n",
    "    return df\n",
    "\n",
    "train_ts = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_train.parquet\")\n",
    "test_ts = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "734980b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T15:51:45.979806Z",
     "iopub.status.busy": "2024-12-19T15:51:45.979353Z",
     "iopub.status.idle": "2024-12-19T15:52:40.747033Z",
     "shell.execute_reply": "2024-12-19T15:52:40.746146Z"
    },
    "papermill": {
     "duration": 54.81175,
     "end_time": "2024-12-19T15:52:40.748816",
     "exception": false,
     "start_time": "2024-12-19T15:51:45.937066",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Folds: 100%|██████████| 5/5 [00:54<00:00, 10.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Train QWK --> 0.7607\n",
      "Mean Validation QWK ---> 0.3855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----> || Optimized QWK SCORE :: \u001b[36m\u001b[1m 0.449\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sii</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00008ff9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000fd460</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00105258</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00115b9f</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0016bb22</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>001f3379</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0038ba98</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0068a485</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0069fbed</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0083e397</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0087dd65</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>00abe655</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>00ae59c9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>00af6387</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>00bd4359</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>00c0cd71</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>00d56d4b</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>00d9913d</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>00e6167c</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>00ebc35d</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  sii\n",
       "0   00008ff9    1\n",
       "1   000fd460    0\n",
       "2   00105258    0\n",
       "3   00115b9f    0\n",
       "4   0016bb22    1\n",
       "5   001f3379    1\n",
       "6   0038ba98    0\n",
       "7   0068a485    0\n",
       "8   0069fbed    1\n",
       "9   0083e397    0\n",
       "10  0087dd65    0\n",
       "11  00abe655    1\n",
       "12  00ae59c9    1\n",
       "13  00af6387    1\n",
       "14  00bd4359    1\n",
       "15  00c0cd71    1\n",
       "16  00d56d4b    0\n",
       "17  00d9913d    0\n",
       "18  00e6167c    0\n",
       "19  00ebc35d    1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/train.csv')\n",
    "test = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/test.csv')\n",
    "sample = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/sample_submission.csv')\n",
    "\n",
    "time_series_cols = train_ts.columns.tolist()\n",
    "time_series_cols.remove(\"id\")\n",
    "\n",
    "train = pd.merge(train, train_ts, how=\"left\", on='id')\n",
    "test = pd.merge(test, test_ts, how=\"left\", on='id')\n",
    "\n",
    "train = train.drop('id', axis=1)\n",
    "test = test.drop('id', axis=1)   \n",
    "\n",
    "featuresCols = ['Basic_Demos-Enroll_Season', 'Basic_Demos-Age', 'Basic_Demos-Sex',\n",
    "                'CGAS-Season', 'CGAS-CGAS_Score', 'Physical-Season', 'Physical-BMI',\n",
    "                'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference',\n",
    "                'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP',\n",
    "                'Fitness_Endurance-Season', 'Fitness_Endurance-Max_Stage',\n",
    "                'Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec',\n",
    "                'FGC-Season', 'FGC-FGC_CU', 'FGC-FGC_CU_Zone', 'FGC-FGC_GSND',\n",
    "                'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone', 'FGC-FGC_PU',\n",
    "                'FGC-FGC_PU_Zone', 'FGC-FGC_SRL', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR',\n",
    "                'FGC-FGC_SRR_Zone', 'FGC-FGC_TL', 'FGC-FGC_TL_Zone', 'BIA-Season',\n",
    "                'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC', 'BIA-BIA_BMI',\n",
    "                'BIA-BIA_BMR', 'BIA-BIA_DEE', 'BIA-BIA_ECW', 'BIA-BIA_FFM',\n",
    "                'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_Fat', 'BIA-BIA_Frame_num',\n",
    "                'BIA-BIA_ICW', 'BIA-BIA_LDM', 'BIA-BIA_LST', 'BIA-BIA_SMM',\n",
    "                'BIA-BIA_TBW', 'PAQ_A-Season', 'PAQ_A-PAQ_A_Total', 'PAQ_C-Season',\n",
    "                'PAQ_C-PAQ_C_Total', 'SDS-Season', 'SDS-SDS_Total_Raw',\n",
    "                'SDS-SDS_Total_T', 'PreInt_EduHx-Season',\n",
    "                'PreInt_EduHx-computerinternet_hoursday', 'sii']\n",
    "\n",
    "featuresCols += time_series_cols\n",
    "\n",
    "train = train[featuresCols]\n",
    "train = train.dropna(subset='sii')\n",
    "\n",
    "cat_c = ['Basic_Demos-Enroll_Season', 'CGAS-Season', 'Physical-Season', \n",
    "          'Fitness_Endurance-Season', 'FGC-Season', 'BIA-Season', \n",
    "          'PAQ_A-Season', 'PAQ_C-Season', 'SDS-Season', 'PreInt_EduHx-Season']\n",
    "\n",
    "def update(df):\n",
    "    global cat_c\n",
    "    for c in cat_c: \n",
    "        df[c] = df[c].fillna('Missing')\n",
    "        df[c] = df[c].astype('category')\n",
    "    return df\n",
    "        \n",
    "train = update(train)\n",
    "test = update(test)\n",
    "\n",
    "def create_mapping(column, dataset):\n",
    "    unique_values = dataset[column].unique()\n",
    "    return {value: idx for idx, value in enumerate(unique_values)}\n",
    "\n",
    "for col in cat_c:\n",
    "    mapping = create_mapping(col, train)\n",
    "    mappingTe = create_mapping(col, test)\n",
    "    \n",
    "    train[col] = train[col].replace(mapping).astype(int)\n",
    "    test[col] = test[col].replace(mappingTe).astype(int)\n",
    "\n",
    "def quadratic_weighted_kappa(y_true, y_pred):\n",
    "    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n",
    "\n",
    "def threshold_Rounder(oof_non_rounded, thresholds):\n",
    "    return np.where(oof_non_rounded < thresholds[0], 0,\n",
    "                    np.where(oof_non_rounded < thresholds[1], 1,\n",
    "                             np.where(oof_non_rounded < thresholds[2], 2, 3)))\n",
    "\n",
    "def evaluate_predictions(thresholds, y_true, oof_non_rounded):\n",
    "    rounded_p = threshold_Rounder(oof_non_rounded, thresholds)\n",
    "    return -quadratic_weighted_kappa(y_true, rounded_p)\n",
    "\n",
    "def TrainML(model_class, test_data):\n",
    "    X = train.drop(['sii'], axis=1)\n",
    "    y = train['sii']\n",
    "\n",
    "    SKF = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
    "    \n",
    "    train_S = []\n",
    "    test_S = []\n",
    "    \n",
    "    oof_non_rounded = np.zeros(len(y), dtype=float) \n",
    "    oof_rounded = np.zeros(len(y), dtype=int) \n",
    "    test_preds = np.zeros((len(test_data), n_splits))\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(tqdm(SKF.split(X, y), desc=\"Training Folds\", total=n_splits)):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "        model = clone(model_class)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_val_pred = model.predict(X_val)\n",
    "\n",
    "        oof_non_rounded[test_idx] = y_val_pred\n",
    "        y_val_pred_rounded = y_val_pred.round(0).astype(int)\n",
    "        oof_rounded[test_idx] = y_val_pred_rounded\n",
    "\n",
    "        train_kappa = quadratic_weighted_kappa(y_train, y_train_pred.round(0).astype(int))\n",
    "        val_kappa = quadratic_weighted_kappa(y_val, y_val_pred_rounded)\n",
    "\n",
    "        train_S.append(train_kappa)\n",
    "        test_S.append(val_kappa)\n",
    "        \n",
    "        test_preds[:, fold] = model.predict(test_data)\n",
    "        \n",
    "        print(f\"Fold {fold+1} - Train QWK: {train_kappa:.4f}, Validation QWK: {val_kappa:.4f}\")\n",
    "        clear_output(wait=True)\n",
    "\n",
    "    print(f\"Mean Train QWK --> {np.mean(train_S):.4f}\")\n",
    "    print(f\"Mean Validation QWK ---> {np.mean(test_S):.4f}\")\n",
    "\n",
    "    KappaOPtimizer = minimize(evaluate_predictions,\n",
    "                              x0=[0.5, 1.5, 2.5], args=(y, oof_non_rounded), \n",
    "                              method='Nelder-Mead')\n",
    "    assert KappaOPtimizer.success, \"Optimization did not converge.\"\n",
    "    \n",
    "    oof_tuned = threshold_Rounder(oof_non_rounded, KappaOPtimizer.x)\n",
    "    tKappa = quadratic_weighted_kappa(y, oof_tuned)\n",
    "\n",
    "    print(f\"----> || Optimized QWK SCORE :: {Fore.CYAN}{Style.BRIGHT} {tKappa:.3f}{Style.RESET_ALL}\")\n",
    "\n",
    "    tpm = test_preds.mean(axis=1)\n",
    "    tpTuned = threshold_Rounder(tpm, KappaOPtimizer.x)\n",
    "    \n",
    "    submission = pd.DataFrame({\n",
    "        'id': sample['id'],\n",
    "        'sii': tpTuned\n",
    "    })\n",
    "\n",
    "    return submission\n",
    "\n",
    "# Model parameters for LightGBM\n",
    "Params = {\n",
    "    'learning_rate': 0.046,\n",
    "    'max_depth': 12,\n",
    "    'num_leaves': 478,\n",
    "    'min_data_in_leaf': 13,\n",
    "    'feature_fraction': 0.893,\n",
    "    'bagging_fraction': 0.784,\n",
    "    'bagging_freq': 4,\n",
    "    'lambda_l1': 10,  # Increased from 6.59\n",
    "    'lambda_l2': 0.01  # Increased from 2.68e-06\n",
    "}\n",
    "\n",
    "\n",
    "# XGBoost parameters\n",
    "XGB_Params = {\n",
    "    'learning_rate': 0.05,\n",
    "    'max_depth': 6,\n",
    "    'n_estimators': 200,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'reg_alpha': 1,  # Increased from 0.1\n",
    "    'reg_lambda': 5,  # Increased from 1\n",
    "    'random_state': SEED\n",
    "}\n",
    "\n",
    "\n",
    "CatBoost_Params = {\n",
    "    'learning_rate': 0.05,\n",
    "    'depth': 6,\n",
    "    'iterations': 200,\n",
    "    'random_seed': SEED,\n",
    "    'cat_features': cat_c,\n",
    "    'verbose': 0,\n",
    "    'l2_leaf_reg': 10  # Increase this value\n",
    "}\n",
    "\n",
    "# Create model instances\n",
    "Light = LGBMRegressor(**Params, random_state=SEED, verbose=-1, n_estimators=300)\n",
    "XGB_Model = XGBRegressor(**XGB_Params)\n",
    "CatBoost_Model = CatBoostRegressor(**CatBoost_Params)\n",
    "\n",
    "# Combine models using Voting Regressor\n",
    "voting_model = VotingRegressor(estimators=[\n",
    "    ('lightgbm', Light),\n",
    "    ('xgboost', XGB_Model),\n",
    "    ('catboost', CatBoost_Model)\n",
    "])\n",
    "\n",
    "# Train the ensemble model\n",
    "sub2 = TrainML(voting_model, test)\n",
    "sub2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ed2d591",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T15:52:40.832874Z",
     "iopub.status.busy": "2024-12-19T15:52:40.832542Z",
     "iopub.status.idle": "2024-12-19T15:56:12.644743Z",
     "shell.execute_reply": "2024-12-19T15:56:12.643788Z"
    },
    "papermill": {
     "duration": 211.85638,
     "end_time": "2024-12-19T15:56:12.646508",
     "exception": false,
     "start_time": "2024-12-19T15:52:40.790128",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Folds: 100%|██████████| 5/5 [02:13<00:00, 26.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Train QWK --> 0.9179\n",
      "Mean Validation QWK ---> 0.3660\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----> || Optimized QWK SCORE :: \u001b[36m\u001b[1m 0.450\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sii</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00008ff9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000fd460</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00105258</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00115b9f</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0016bb22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>001f3379</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0038ba98</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0068a485</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0069fbed</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0083e397</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0087dd65</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>00abe655</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>00ae59c9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>00af6387</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>00bd4359</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>00c0cd71</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>00d56d4b</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>00d9913d</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>00e6167c</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>00ebc35d</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  sii\n",
       "0   00008ff9    2\n",
       "1   000fd460    0\n",
       "2   00105258    0\n",
       "3   00115b9f    0\n",
       "4   0016bb22    0\n",
       "5   001f3379    1\n",
       "6   0038ba98    0\n",
       "7   0068a485    0\n",
       "8   0069fbed    2\n",
       "9   0083e397    0\n",
       "10  0087dd65    1\n",
       "11  00abe655    1\n",
       "12  00ae59c9    1\n",
       "13  00af6387    1\n",
       "14  00bd4359    2\n",
       "15  00c0cd71    2\n",
       "16  00d56d4b    0\n",
       "17  00d9913d    0\n",
       "18  00e6167c    0\n",
       "19  00ebc35d    1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/train.csv')\n",
    "test = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/test.csv')\n",
    "sample = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/sample_submission.csv')\n",
    "\n",
    "featuresCols = ['Basic_Demos-Enroll_Season', 'Basic_Demos-Age', 'Basic_Demos-Sex',\n",
    "                'CGAS-Season', 'CGAS-CGAS_Score', 'Physical-Season', 'Physical-BMI',\n",
    "                'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference',\n",
    "                'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP',\n",
    "                'Fitness_Endurance-Season', 'Fitness_Endurance-Max_Stage',\n",
    "                'Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec',\n",
    "                'FGC-Season', 'FGC-FGC_CU', 'FGC-FGC_CU_Zone', 'FGC-FGC_GSND',\n",
    "                'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone', 'FGC-FGC_PU',\n",
    "                'FGC-FGC_PU_Zone', 'FGC-FGC_SRL', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR',\n",
    "                'FGC-FGC_SRR_Zone', 'FGC-FGC_TL', 'FGC-FGC_TL_Zone', 'BIA-Season',\n",
    "                'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC', 'BIA-BIA_BMI',\n",
    "                'BIA-BIA_BMR', 'BIA-BIA_DEE', 'BIA-BIA_ECW', 'BIA-BIA_FFM',\n",
    "                'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_Fat', 'BIA-BIA_Frame_num',\n",
    "                'BIA-BIA_ICW', 'BIA-BIA_LDM', 'BIA-BIA_LST', 'BIA-BIA_SMM',\n",
    "                'BIA-BIA_TBW', 'PAQ_A-Season', 'PAQ_A-PAQ_A_Total', 'PAQ_C-Season',\n",
    "                'PAQ_C-PAQ_C_Total', 'SDS-Season', 'SDS-SDS_Total_Raw',\n",
    "                'SDS-SDS_Total_T', 'PreInt_EduHx-Season',\n",
    "                'PreInt_EduHx-computerinternet_hoursday', 'sii']\n",
    "\n",
    "cat_c = ['Basic_Demos-Enroll_Season', 'CGAS-Season', 'Physical-Season', \n",
    "          'Fitness_Endurance-Season', 'FGC-Season', 'BIA-Season', \n",
    "          'PAQ_A-Season', 'PAQ_C-Season', 'SDS-Season', 'PreInt_EduHx-Season']\n",
    "\n",
    "train_ts = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_train.parquet\")\n",
    "test_ts = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_test.parquet\")\n",
    "\n",
    "time_series_cols = train_ts.columns.tolist()\n",
    "time_series_cols.remove(\"id\")\n",
    "\n",
    "train = pd.merge(train, train_ts, how=\"left\", on='id')\n",
    "test = pd.merge(test, test_ts, how=\"left\", on='id')\n",
    "\n",
    "train = train.drop('id', axis=1)\n",
    "test = test.drop('id', axis=1)\n",
    "\n",
    "featuresCols += time_series_cols\n",
    "\n",
    "train = train[featuresCols]\n",
    "train = train.dropna(subset='sii')\n",
    "\n",
    "def update(df):\n",
    "    global cat_c\n",
    "    for c in cat_c: \n",
    "        df[c] = df[c].fillna('Missing')\n",
    "        df[c] = df[c].astype('category')\n",
    "    return df\n",
    "\n",
    "train = update(train)\n",
    "test = update(test)\n",
    "\n",
    "def create_mapping(column, dataset):\n",
    "    unique_values = dataset[column].unique()\n",
    "    return {value: idx for idx, value in enumerate(unique_values)}\n",
    "\n",
    "for col in cat_c:\n",
    "    mapping = create_mapping(col, train)\n",
    "    mappingTe = create_mapping(col, test)\n",
    "    \n",
    "    train[col] = train[col].replace(mapping).astype(int)\n",
    "    test[col] = test[col].replace(mappingTe).astype(int)\n",
    "\n",
    "def quadratic_weighted_kappa(y_true, y_pred):\n",
    "    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n",
    "\n",
    "def threshold_Rounder(oof_non_rounded, thresholds):\n",
    "    return np.where(oof_non_rounded < thresholds[0], 0,\n",
    "                    np.where(oof_non_rounded < thresholds[1], 1,\n",
    "                             np.where(oof_non_rounded < thresholds[2], 2, 3)))\n",
    "\n",
    "def evaluate_predictions(thresholds, y_true, oof_non_rounded):\n",
    "    rounded_p = threshold_Rounder(oof_non_rounded, thresholds)\n",
    "    return -quadratic_weighted_kappa(y_true, rounded_p)\n",
    "\n",
    "def TrainML(model_class, test_data):\n",
    "    X = train.drop(['sii'], axis=1)\n",
    "    y = train['sii']\n",
    "\n",
    "    SKF = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
    "    \n",
    "    train_S = []\n",
    "    test_S = []\n",
    "    \n",
    "    oof_non_rounded = np.zeros(len(y), dtype=float) \n",
    "    oof_rounded = np.zeros(len(y), dtype=int) \n",
    "    test_preds = np.zeros((len(test_data), n_splits))\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(tqdm(SKF.split(X, y), desc=\"Training Folds\", total=n_splits)):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "        model = clone(model_class)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_val_pred = model.predict(X_val)\n",
    "\n",
    "        oof_non_rounded[test_idx] = y_val_pred\n",
    "        y_val_pred_rounded = y_val_pred.round(0).astype(int)\n",
    "        oof_rounded[test_idx] = y_val_pred_rounded\n",
    "\n",
    "        train_kappa = quadratic_weighted_kappa(y_train, y_train_pred.round(0).astype(int))\n",
    "        val_kappa = quadratic_weighted_kappa(y_val, y_val_pred_rounded)\n",
    "\n",
    "        train_S.append(train_kappa)\n",
    "        test_S.append(val_kappa)\n",
    "        \n",
    "        test_preds[:, fold] = model.predict(test_data)\n",
    "        \n",
    "        print(f\"Fold {fold+1} - Train QWK: {train_kappa:.4f}, Validation QWK: {val_kappa:.4f}\")\n",
    "        clear_output(wait=True)\n",
    "\n",
    "    print(f\"Mean Train QWK --> {np.mean(train_S):.4f}\")\n",
    "    print(f\"Mean Validation QWK ---> {np.mean(test_S):.4f}\")\n",
    "\n",
    "    KappaOPtimizer = minimize(evaluate_predictions,\n",
    "                              x0=[0.5, 1.5, 2.5], args=(y, oof_non_rounded), \n",
    "                              method='Nelder-Mead')\n",
    "    assert KappaOPtimizer.success, \"Optimization did not converge.\"\n",
    "    \n",
    "    oof_tuned = threshold_Rounder(oof_non_rounded, KappaOPtimizer.x)\n",
    "    tKappa = quadratic_weighted_kappa(y, oof_tuned)\n",
    "\n",
    "    print(f\"----> || Optimized QWK SCORE :: {Fore.CYAN}{Style.BRIGHT} {tKappa:.3f}{Style.RESET_ALL}\")\n",
    "\n",
    "    tpm = test_preds.mean(axis=1)\n",
    "    tp_rounded = threshold_Rounder(tpm, KappaOPtimizer.x)\n",
    "\n",
    "    return tp_rounded\n",
    "\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "ensemble = VotingRegressor(estimators=[\n",
    "    ('lgb', Pipeline(steps=[('imputer', imputer), ('regressor', LGBMRegressor(random_state=SEED))])),\n",
    "    ('xgb', Pipeline(steps=[('imputer', imputer), ('regressor', XGBRegressor(random_state=SEED))])),\n",
    "    ('cat', Pipeline(steps=[('imputer', imputer), ('regressor', CatBoostRegressor(random_state=SEED, silent=True))])),\n",
    "    ('rf', Pipeline(steps=[('imputer', imputer), ('regressor', RandomForestRegressor(random_state=SEED))])),\n",
    "    ('gb', Pipeline(steps=[('imputer', imputer), ('regressor', GradientBoostingRegressor(random_state=SEED))]))\n",
    "])\n",
    "\n",
    "sub3 = TrainML(ensemble, test)\n",
    "sub3 = pd.DataFrame({\n",
    "    'id': sample['id'],\n",
    "    'sii': sub3\n",
    "})\n",
    "\n",
    "sub3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ae772f06",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T15:56:12.735807Z",
     "iopub.status.busy": "2024-12-19T15:56:12.735041Z",
     "iopub.status.idle": "2024-12-19T15:56:12.754357Z",
     "shell.execute_reply": "2024-12-19T15:56:12.753160Z"
    },
    "papermill": {
     "duration": 0.065814,
     "end_time": "2024-12-19T15:56:12.756441",
     "exception": false,
     "start_time": "2024-12-19T15:56:12.690627",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority voting completed and saved to 'Final_Submission.csv'\n"
     ]
    }
   ],
   "source": [
    "sub1 = sub1.sort_values(by='id').reset_index(drop=True)\n",
    "sub2 = sub2.sort_values(by='id').reset_index(drop=True)\n",
    "sub3 = sub3.sort_values(by='id').reset_index(drop=True)\n",
    "\n",
    "combined = pd.DataFrame({\n",
    "    'id': sub1['id'],\n",
    "    'sii_1': sub1['sii'],\n",
    "    'sii_2': sub2['sii'],\n",
    "    'sii_3': sub3['sii'],\n",
    "})\n",
    "\n",
    "def majority_vote(row):\n",
    "    return row.mode()[0]\n",
    "\n",
    "combined['final_sii'] = combined[['sii_1', 'sii_2', 'sii_3']].apply(majority_vote, axis=1)\n",
    "\n",
    "final_submission = combined[['id', 'final_sii']].rename(columns={'final_sii': 'sii'})\n",
    "\n",
    "final_submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"Majority voting completed and saved to 'Final_Submission.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3b32ed80",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T15:56:12.844740Z",
     "iopub.status.busy": "2024-12-19T15:56:12.844385Z",
     "iopub.status.idle": "2024-12-19T15:56:12.854192Z",
     "shell.execute_reply": "2024-12-19T15:56:12.853312Z"
    },
    "papermill": {
     "duration": 0.055887,
     "end_time": "2024-12-19T15:56:12.856252",
     "exception": false,
     "start_time": "2024-12-19T15:56:12.800365",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sii</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00008ff9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000fd460</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00105258</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00115b9f</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0016bb22</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>001f3379</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0038ba98</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0068a485</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0069fbed</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0083e397</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0087dd65</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>00abe655</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>00ae59c9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>00af6387</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>00bd4359</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>00c0cd71</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>00d56d4b</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>00d9913d</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>00e6167c</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>00ebc35d</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  sii\n",
       "0   00008ff9    0\n",
       "1   000fd460    0\n",
       "2   00105258    0\n",
       "3   00115b9f    0\n",
       "4   0016bb22    1\n",
       "5   001f3379    1\n",
       "6   0038ba98    0\n",
       "7   0068a485    0\n",
       "8   0069fbed    1\n",
       "9   0083e397    0\n",
       "10  0087dd65    0\n",
       "11  00abe655    1\n",
       "12  00ae59c9    1\n",
       "13  00af6387    1\n",
       "14  00bd4359    1\n",
       "15  00c0cd71    0\n",
       "16  00d56d4b    0\n",
       "17  00d9913d    0\n",
       "18  00e6167c    0\n",
       "19  00ebc35d    1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_submission\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 9643020,
     "sourceId": 81933,
     "sourceType": "competition"
    },
    {
     "datasetId": 921302,
     "sourceId": 7453542,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 5860.185967,
   "end_time": "2024-12-19T15:56:15.847698",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-19T14:18:35.661731",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
