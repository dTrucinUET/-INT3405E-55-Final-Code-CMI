{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-12-22T18:33:54.138411Z","iopub.status.busy":"2024-12-22T18:33:54.137965Z","iopub.status.idle":"2024-12-22T18:34:01.884200Z","shell.execute_reply":"2024-12-22T18:34:01.883237Z","shell.execute_reply.started":"2024-12-22T18:33:54.138355Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import os\n","import re\n","from sklearn.base import clone\n","from sklearn.metrics import cohen_kappa_score\n","from sklearn.model_selection import StratifiedKFold\n","from scipy.optimize import minimize\n","from concurrent.futures import ThreadPoolExecutor\n","from tqdm import tqdm\n","import polars as pl\n","import polars.selectors as cs\n","import matplotlib.pyplot as plt\n","from matplotlib.ticker import MaxNLocator, FormatStrFormatter, PercentFormatter\n","import seaborn as sns\n","\n","from sklearn.preprocessing import StandardScaler\n","import matplotlib.pyplot as plt\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","from colorama import Fore, Style\n","from IPython.display import clear_output\n","import warnings\n","from lightgbm import LGBMRegressor\n","from xgboost import XGBRegressor\n","from catboost import CatBoostRegressor\n","from sklearn.ensemble import VotingRegressor, RandomForestRegressor, GradientBoostingRegressor\n","from sklearn.impute import SimpleImputer, KNNImputer\n","from sklearn.pipeline import Pipeline\n","warnings.filterwarnings('ignore')\n","pd.options.display.max_columns = None"]},{"cell_type":"markdown","metadata":{},"source":["# Fix seed for reproducibility"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-12-22T18:34:01.886306Z","iopub.status.busy":"2024-12-22T18:34:01.885700Z","iopub.status.idle":"2024-12-22T18:34:01.895758Z","shell.execute_reply":"2024-12-22T18:34:01.894794Z","shell.execute_reply.started":"2024-12-22T18:34:01.886275Z"},"trusted":true},"outputs":[],"source":["import random\n","def seed_everything(seed):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = True\n","\n","SEED = 314159\n","seed_everything(SEED)\n","\n","n_splits = 5"]},{"cell_type":"markdown","metadata":{},"source":["# Recipe 1: Masked Autoencoder"]},{"cell_type":"markdown","metadata":{},"source":["## Load timeseries"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-12-22T18:34:01.897410Z","iopub.status.busy":"2024-12-22T18:34:01.897064Z","iopub.status.idle":"2024-12-22T18:35:19.823691Z","shell.execute_reply":"2024-12-22T18:35:19.822853Z","shell.execute_reply.started":"2024-12-22T18:34:01.897372Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 996/996 [01:17<00:00, 12.83it/s]\n","100%|██████████| 2/2 [00:00<00:00, 10.40it/s]\n"]}],"source":["\n","\n","def process_file(filename, dirname):\n","    df = pd.read_parquet(os.path.join(dirname, filename, 'part-0.parquet'))\n","    df.drop('step', axis=1, inplace=True)\n","    return df.describe().values.reshape(-1), filename.split('=')[1]\n","\n","def load_time_series(dirname) -> pd.DataFrame:\n","    ids = os.listdir(dirname)\n","    \n","    with ThreadPoolExecutor() as executor:\n","        results = list(tqdm(executor.map(lambda fname: process_file(fname, dirname), ids), total=len(ids)))\n","    \n","    stats, indexes = zip(*results)\n","    \n","    df = pd.DataFrame(stats, columns=[f\"stat_{i}\" for i in range(len(stats[0]))])\n","    df['id'] = indexes\n","    return df\n","    \n","train_ts = load_time_series(\"../input/child-mind-institute-problematic-internet-use/series_train.parquet\")\n","test_ts = load_time_series(\"../input/child-mind-institute-problematic-internet-use/series_test.parquet\")"]},{"cell_type":"markdown","metadata":{},"source":["## Load tabular data\n","\n","Since MAE's self supervised learning (and self-supervised learning in general) often requires clean samples, we don't use any imputations on the values, but let the model learn the distribution of the features by itself."]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-12-22T18:39:21.019036Z","iopub.status.busy":"2024-12-22T18:39:21.018657Z","iopub.status.idle":"2024-12-22T18:39:21.173581Z","shell.execute_reply":"2024-12-22T18:39:21.172860Z","shell.execute_reply.started":"2024-12-22T18:39:21.019003Z"},"trusted":true},"outputs":[],"source":["train = pd.read_csv('../input/child-mind-institute-problematic-internet-use/train.csv')\n","test = pd.read_csv('../input/child-mind-institute-problematic-internet-use/test.csv')\n","sample = pd.read_csv('../input/child-mind-institute-problematic-internet-use/sample_submission.csv')\n","\n","\n","featuresCols = ['Basic_Demos-Enroll_Season', 'Basic_Demos-Age', 'Basic_Demos-Sex',\n","                'CGAS-Season', 'CGAS-CGAS_Score', 'Physical-Season', 'Physical-BMI',\n","                'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference',\n","                'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP',\n","                'Fitness_Endurance-Season', 'Fitness_Endurance-Max_Stage',\n","                'Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec',\n","                'FGC-Season', 'FGC-FGC_CU', 'FGC-FGC_CU_Zone', 'FGC-FGC_GSND',\n","                'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone', 'FGC-FGC_PU',\n","                'FGC-FGC_PU_Zone', 'FGC-FGC_SRL', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR',\n","                'FGC-FGC_SRR_Zone', 'FGC-FGC_TL', 'FGC-FGC_TL_Zone', 'BIA-Season',\n","                'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC', 'BIA-BIA_BMI',\n","                'BIA-BIA_BMR', 'BIA-BIA_DEE', 'BIA-BIA_ECW', 'BIA-BIA_FFM',\n","                'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_Fat', 'BIA-BIA_Frame_num',\n","                'BIA-BIA_ICW', 'BIA-BIA_LDM', 'BIA-BIA_LST', 'BIA-BIA_SMM',\n","                'BIA-BIA_TBW', 'PAQ_A-Season', 'PAQ_A-PAQ_A_Total', 'PAQ_C-Season',\n","                'PAQ_C-PAQ_C_Total', 'SDS-Season', 'SDS-SDS_Total_Raw',\n","                'SDS-SDS_Total_T', 'PreInt_EduHx-Season',\n","                'PreInt_EduHx-computerinternet_hoursday',\n","                \n","                # Main label\n","                'sii',\n","                \n","\n","                # We use full PCIAT labels for MAE's multilabel regression\n","                 'PCIAT-Season', 'PCIAT-PCIAT_01', 'PCIAT-PCIAT_02', 'PCIAT-PCIAT_03', 'PCIAT-PCIAT_04',\n","                'PCIAT-PCIAT_05', 'PCIAT-PCIAT_06', 'PCIAT-PCIAT_07', 'PCIAT-PCIAT_08',\n","                'PCIAT-PCIAT_09', 'PCIAT-PCIAT_10', 'PCIAT-PCIAT_11', 'PCIAT-PCIAT_12',\n","                'PCIAT-PCIAT_13', 'PCIAT-PCIAT_14', 'PCIAT-PCIAT_15', 'PCIAT-PCIAT_16',\n","                'PCIAT-PCIAT_17', 'PCIAT-PCIAT_18', 'PCIAT-PCIAT_19', 'PCIAT-PCIAT_20', 'PCIAT-PCIAT_Total',\n","]\n","\n","cat_c = ['Basic_Demos-Enroll_Season', 'CGAS-Season', 'Physical-Season', \n","          'Fitness_Endurance-Season', 'FGC-Season', 'BIA-Season', \n","          'PAQ_A-Season', 'PAQ_C-Season', 'SDS-Season', 'PreInt_EduHx-Season',\n","        'PCIAT-Season',\n","        ]\n","\n","\n","time_series_cols = train_ts.columns.tolist()\n","time_series_cols.remove(\"id\")\n","\n","train = pd.merge(train, train_ts, how=\"left\", on='id')\n","test = pd.merge(test, test_ts, how=\"left\", on='id')\n","\n","train = train.drop('id', axis=1)\n","test = test.drop('id', axis=1)\n","\n","featuresCols += time_series_cols\n","\n","train = train[featuresCols]\n","# train = train.dropna(subset='sii')\n","\n","def update(df):\n","    global cat_c\n","    for c in cat_c: \n","        if c not in df.columns: continue\n","        df[c] = df[c].fillna('Missing')\n","        df[c] = df[c].astype('category')\n","    return df\n","\n","train = update(train)\n","test = update(test)\n","\n","def create_mapping(column, dataset):\n","    unique_values = dataset[column].unique()\n","    return {value: idx for idx, value in enumerate(unique_values)}\n","\n","### Convert all the categorical \"Season\" features into numerical features\n","### and keep all the nan values.\n","for col in cat_c:\n","    if col in train.columns:\n","        if 'Season' in col:\n","            mapping = {\n","                'Missing': float('nan'),\n","                'Spring': 0.,\n","                'Summer': 1.,\n","                'Fall': 2.,\n","                'Winter': 3.,\n","            }\n","        else:\n","            mapping = create_mapping(col, train)\n","            print(f'{col}: {mapping}')\n","        train[col] = train[col].replace(mapping)\n","    if col in test.columns:\n","        if 'Season' in col:\n","            mappingTe = {\n","                'Missing': float('nan'),\n","                'Spring': 0.,\n","                'Summer': 1.,\n","                'Fall': 2.,\n","                'Winter': 3.,\n","            }\n","        else:\n","            mappingTe = create_mapping(col, test)\n","            print(f'{col}: {mappingTe}')\n","            \n","        test[col] = test[col].replace(mappingTe)\n","\n","def quadratic_weighted_kappa(y_true, y_pred):\n","    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n","\n","def threshold_Rounder(oof_non_rounded, thresholds):\n","    return np.where(oof_non_rounded < thresholds[0], 0,\n","                    np.where(oof_non_rounded < thresholds[1], 1,\n","                             np.where(oof_non_rounded < thresholds[2], 2, 3)))\n","\n","def evaluate_predictions(thresholds, y_true, oof_non_rounded):\n","    rounded_p = threshold_Rounder(oof_non_rounded, thresholds)\n","    return -quadratic_weighted_kappa(y_true, rounded_p)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["## MAE PyTorch Module\n","\n","**Note:** Adapted from [tydusky/remasker](https://github.com/tydusky/remasker)\n","\n","### Major Changes:\n","- Reimplement random_masking method for convenient batch inference.\n","- Adding regression head for semisl and sl.\n","- Adding cross validation, early stopping."]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-12-22T19:37:47.188445Z","iopub.status.busy":"2024-12-22T19:37:47.188107Z","iopub.status.idle":"2024-12-22T19:37:47.237121Z","shell.execute_reply":"2024-12-22T19:37:47.236301Z","shell.execute_reply.started":"2024-12-22T19:37:47.188413Z"},"trusted":true},"outputs":[],"source":["# current implementation: only support numerical values\n","\n","from functools import partial\n","from tkinter import E\n","\n","import torch\n","import numpy as np\n","import torch.nn as nn\n","import pandas as pd\n","from timm.models.vision_transformer import Block\n","import os\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","import pandas as pd\n","import math\n","import argparse\n","import random\n","\n","class MaskEmbed(nn.Module):\n","    \"\"\"\n","    Perform a linear projectiion for the features into a higher-dimensional space using.\n","    \n","    Parameters:\n","    rec_len (int): Length of the input feature sequence (default: 25).\n","    embed_dim (int): Dimension of the embedding space (default: 64).\n","    norm_layer (callable): Normalization layer to apply after projection (default: nn.Identity).\n","    \"\"\"\n","    def __init__(self, rec_len=25, embed_dim=64, norm_layer=None):\n","        \n","        super().__init__()\n","        self.rec_len = rec_len\n","        self.proj = nn.Conv1d(1, embed_dim, kernel_size=1, stride=1)\n","        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()\n","\n","    def forward(self, x):\n","        B, _, L = x.shape\n","        # assert(L == self.rec_len, f\"Input data width ({L}) doesn't match model ({self.rec_len}).\")\n","        x = self.proj(x)\n","        x = x.transpose(1, 2)\n","        x = self.norm(x)\n","        return x\n","\n","\n","class ActiveEmbed(nn.Module):\n","    \"\"\"\n","    Applying sinusoidal positional embeddings.\n","    \n","    Note: This module is not currently used, as no positional priors are observed in the data.\n","\n","    Input: (B, 1, L)\n","    Output: (B, L, D)\n","    \"\"\"\n","    def __init__(self, rec_len=25, embed_dim=64, norm_layer=None):\n","        \n","        super().__init__()\n","        self.rec_len = rec_len\n","        self.proj = nn.Conv1d(1, embed_dim, kernel_size=1, stride=1)\n","        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()\n","\n","    def forward(self, x):\n","        B, _, L = x.shape\n","        # assert(L == self.rec_len, f\"Input data width ({L}) doesn't match model ({self.rec_len}).\")\n","        x = self.proj(x)\n","        x = torch.sin(x)\n","        x = x.transpose(1, 2)\n","        #   x = torch.cat((torch.sin(x), torch.cos(x + math.pi/2)), -1)\n","        x = self.norm(x)\n","        return x\n","\n","\n","\n","def get_1d_sincos_pos_embed(embed_dim, pos, cls_token=False):\n","    \"\"\"\n","    Initialize sinuisoidal positional priors.\n","    Note: This module is not currently used, as no positional priors are observed in the data.\n","    \n","    Output: (M, D) \n","    \"\"\"\n","\n","    assert embed_dim % 2 == 0\n","    omega = np.arange(embed_dim // 2, dtype=np.float32)\n","    omega /= embed_dim / 2.\n","    omega = 1. / 10000**omega  # (D/2,)\n","\n","    pos = np.arange(pos)  # (M,)\n","    out = np.einsum('m,d->md', pos, omega)  # (M, D/2), outer product\n","\n","    emb_sin = np.sin(out) # (M, D/2)\n","    emb_cos = np.cos(out) # (M, D/2)\n","\n","    pos_embed = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)\n","\n","    if cls_token:\n","        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n","\n","    return pos_embed\n","\n","\n","def adjust_learning_rate(optimizer, epoch, lr, min_lr, max_epochs, warmup_epochs):\n","    \"\"\"Decay the learning rate with half-cycle cosine after warmup\"\"\"\n","    if epoch < warmup_epochs:\n","        tmp_lr = lr * epoch / warmup_epochs \n","    else:\n","        tmp_lr = min_lr + (lr - min_lr) * 0.5 * \\\n","            (1. + math.cos(math.pi * (epoch - warmup_epochs) / (max_epochs - warmup_epochs)))\n","    for param_group in optimizer.param_groups:\n","        if \"lr_scale\" in param_group:\n","            param_group[\"lr\"] = tmp_lr * param_group[\"lr_scale\"]\n","        else:\n","            param_group[\"lr\"] = tmp_lr\n","    return tmp_lr\n","\n","\n","def get_grad_norm_(parameters, norm_type: float = 2.0) -> torch.Tensor:\n","    \"\"\"Compute the gradient norm of a list of parameters.\"\"\"\n","    if isinstance(parameters, torch.Tensor):\n","        parameters = [parameters]\n","    parameters = [p for p in parameters if p.grad is not None]\n","    norm_type = float(norm_type)\n","    if len(parameters) == 0:\n","        return torch.tensor(0.)\n","    device = parameters[0].grad.device\n","    if norm_type == np.inf:\n","        total_norm = max(p.grad.detach().abs().max().to(device) for p in parameters)\n","    else:\n","        total_norm = torch.norm(torch.stack([torch.norm(p.grad.detach(), norm_type).to(device) for p in parameters]), norm_type)\n","    return total_norm\n","\n","\n","class NativeScaler:\n","    \"\"\"BF16 mixed precision version of optimizer.step()\"\"\"\n","    state_dict_key = \"amp_scaler\"\n","    def __init__(self):\n","        self._scaler = torch.cuda.amp.GradScaler()\n","\n","    def __call__(self, loss, optimizer, clip_grad=None, parameters=None, create_graph=False, update_grad=True):\n","        self._scaler.scale(loss).backward(create_graph=create_graph)\n","        if update_grad:\n","            if clip_grad is not None:\n","                assert parameters is not None\n","                self._scaler.unscale_(optimizer)  # unscale the gradients of optimizer's assigned params in-place\n","                norm = torch.nn.utils.clip_grad_norm_(parameters, clip_grad)\n","            else:\n","                self._scaler.unscale_(optimizer)\n","                norm = get_grad_norm_(parameters)\n","            self._scaler.step(optimizer)\n","            self._scaler.update()\n","        else:\n","            norm = None\n","        return norm\n","\n","    def state_dict(self):\n","        return self._scaler.state_dict()\n","\n","    def load_state_dict(self, state_dict):\n","        self._scaler.load_state_dict(state_dict)\n","\n","\n","\n","class MAEDataset(Dataset):\n","    \"\"\"Torch dataset for parallelization\"\"\"\n","    def __init__(self, X, M):        \n","         self.X = X\n","         self.M = M\n","\n","    def __len__(self):\n","        return len(self.X)\n","\n","    def __getitem__(self, idx: int):\n","        return self.X[idx], self.M[idx]\n","\n","\n","\n","\n","eps = 1e-6\n","import torch\n","import torch.nn as nn\n","import numpy as np\n","from torch.nn import TransformerEncoderLayer\n","from transformers.models.bert.modeling_bert import BertPooler\n","\n","class MaskedAutoencoder(nn.Module):\n","    \"\"\"\n","    Implements a masked autoencoder with a transformer-based architecture. \n","    Combining self-supervised learning, supervised learning, and semi-supervised learning tasks.\n","    \"\"\"\n","    def __init__(self, rec_len=25, embed_dim=64, depth=4, num_heads=4,\n","                 decoder_embed_dim=64, decoder_depth=2, decoder_num_heads=4,\n","                 mlp_ratio=4., cls_mlp_dim=64, norm_layer=nn.LayerNorm, norm_field_loss=False,\n","                 encode_func='linear', dropout=0.0):\n","        \"\"\"\n","            Initializes the MaskedAutoencoder with configurable parameters for encoder, decoder, and pooling layers.\n","        \"\"\"\n","        super().__init__()\n","        \n","        self.rec_len = rec_len\n","        self.embed_dim = embed_dim\n","        self.norm_field_loss = norm_field_loss\n","        \n","        # Encoder for infering features\n","        if encode_func == 'active': # Not used\n","            self.mask_embed = ActiveEmbed(rec_len, embed_dim, norm_layer)\n","        else:\n","            self.mask_embed = MaskEmbed(rec_len, embed_dim, norm_layer)\n","        \n","        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n","        self.pos_embed = nn.Parameter(torch.zeros(1, rec_len + 1, embed_dim), requires_grad=False)\n","        \n","        encoder_layer = TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=int(embed_dim * mlp_ratio),\n","                        dropout=dropout, batch_first=True)\n","        self.blocks = nn.TransformerEncoder(encoder_layer, depth)\n","        self.norm = norm_layer(embed_dim)\n","        \n","        # Pooler and prediction head for predicting labels\n","        self.enc_pooler = nn.ModuleList([\n","            nn.Sequential(\n","                nn.Linear(embed_dim, 1),\n","            ),\n","            nn.Sequential(\n","                nn.Linear((rec_len + 1) * 1, cls_mlp_dim), nn.ReLU(),\n","            ),\n","        ])\n","        self.enc_lbl_pred = nn.Sequential(\n","            nn.Linear(cls_mlp_dim, cls_mlp_dim), nn.ReLU(),\n","            # nn.Linear(cls_mlp_dim, cls_mlp_dim), nn.ReLU(),\n","            # nn.Dropout(dropout),\n","            nn.Linear(cls_mlp_dim, 100), nn.Sigmoid(), # Infer 100 predicted labels, but only use some of them\n","        )\n","        \n","        # Decoder for reconstruction\n","        self.decoder_embed = nn.Linear(embed_dim, decoder_embed_dim, bias=True)\n","        self.mask_token = nn.Parameter(torch.zeros(1, 1, decoder_embed_dim))\n","        self.decoder_pos_embed = nn.Parameter(torch.zeros(1, rec_len + 1, decoder_embed_dim), requires_grad=False)\n","        \n","        decoder_layer = TransformerEncoderLayer(d_model=decoder_embed_dim, nhead=decoder_num_heads, dim_feedforward=int(decoder_embed_dim * mlp_ratio),\n","                        dropout=dropout, batch_first=True)\n","        self.decoder_blocks = nn.TransformerEncoder(decoder_layer, decoder_depth)\n","        self.decoder_norm = norm_layer(decoder_embed_dim)\n","\n","        # Reconstruction head\n","        self.decoder_pred = nn.Linear(decoder_embed_dim, 1, bias=True)\n","        \n","        \n","        self.initialize_weights()\n","\n","    def initialize_weights(self):\n","        \"\"\"Initializes the weights of positional embeddings and other model parameters.\"\"\"\n","        # pos_embed = get_1d_sincos_pos_embed(self.pos_embed.shape[-1], self.rec_len, cls_token=True)\n","        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n","        \n","        # decoder_pos_embed = get_1d_sincos_pos_embed(self.decoder_pos_embed.shape[-1], self.rec_len, cls_token=True)\n","        self.decoder_pos_embed.data.copy_(torch.from_numpy(decoder_pos_embed).float().unsqueeze(0))\n","        \n","        torch.nn.init.xavier_uniform_(self.mask_embed.proj.weight.view([self.mask_embed.proj.weight.shape[0], -1]))\n","        torch.nn.init.normal_(self.cls_token, std=.02)\n","        torch.nn.init.normal_(self.mask_token, std=.02)\n","        \n","        self.apply(self._init_weights)\n","\n","    def _init_weights(self, m):\n","        \"\"\"Xavier uniform initialization for linear layers.\"\"\"\n","        if isinstance(m, nn.Linear):\n","            torch.nn.init.xavier_uniform_(m.weight)\n","            if m.bias is not None:\n","                nn.init.constant_(m.bias, 0)\n","        elif isinstance(m, nn.LayerNorm):\n","            nn.init.constant_(m.bias, 0)\n","            nn.init.constant_(m.weight, 1.0)\n","\n","    def random_masking(self, x, m, mask_ratio, training=None):\n","        \"\"\"\n","        Applies random masking to the input data during training.\n","        Training flag: \n","            True: Randomly mask some more in the available features, and try to reconstruct it.\n","            False: Don't mask anything anymore. Infer whole sample's embeddings using all available features.\n","        \"\"\"\n","        N, L, D = x.shape\n","        if training is None:\n","            training = self.training\n","        if training:\n","            len_keep = int(L * (1 - mask_ratio))\n","            noise = torch.rand(N, L, device=x.device)\n","            noise[m < 1e-6] = 1\n","            ids_shuffle = torch.argsort(noise, dim=1)\n","            ids_restore = torch.argsort(ids_shuffle, dim=1)\n","            ids_keep = ids_shuffle[:, :len_keep]\n","            mask = torch.ones([N, L], device=x.device)\n","            mask[:, :len_keep] = 0\n","            mask = torch.gather(mask, dim=1, index=ids_restore)\n","            mask = torch.logical_or(mask, ~m.bool())\n","            nask = ~mask\n","            return mask, nask\n","        else:\n","            mask = ~m.bool()\n","            nask = m.bool()\n","            return mask, nask\n","\n","    def forward_encoder(self, x, m, mask_ratio=0.5, training=None):\n","        \"\"\"\n","        Applies masking and infer the input data through the encoder.\n","        \n","        Training flag: \n","            True: Randomly mask some more in the available features, and try to reconstruct it.\n","            False: Don't mask anything anymore. Infer whole sample's embeddings using all available features.\n","        \"\"\"\n","\n","        \n","        x = self.mask_embed(x)\n","        x = x + self.pos_embed[:, 1:, :]\n","        mask, nask = self.random_masking(x, m, mask_ratio, training)\n","        x = x * (~mask.unsqueeze(-1)).float()\n","        cls_token = self.cls_token + self.pos_embed[:, :1, :]\n","        cls_tokens = cls_token.expand(x.shape[0], -1, -1)\n","        x = torch.cat((cls_tokens, x), dim=1)\n","        attn_mask = torch.cat((torch.zeros(x.shape[0], 1, device=x.device), mask), dim=1)\n","        x = self.blocks(src=x, src_key_padding_mask=attn_mask.bool())\n","        x = self.norm(x)\n","        return x, mask, nask\n","\n","    def forward_decoder(self, x, mask):\n","        \"\"\"\n","            Infer the encoded data through the decoder to reconstruct masked regions.\n","        \"\"\"\n","        x = self.decoder_embed(x)\n","        x = x + self.decoder_pos_embed\n","        mask_with_cls = torch.cat((torch.zeros(x.shape[0], 1, device=x.device), mask), dim=1)\n","        x = self.blocks(src=x, src_key_padding_mask=mask_with_cls.bool())\n","        \n","        x = self.decoder_norm(x)\n","        x = self.decoder_pred(x)\n","        x = x[:, 1:, :].sigmoid() # Outputs are [0, 1] constrained.\n","        return x\n","\n","    def forward_loss(self, data, pred, m, mask, nask):\n","        \"\"\"\n","        Forward reconstruction loss (MSE on sigmoid-ed logits)\n","        \"\"\"\n","        target = data.squeeze(dim=1)\n","        # if self.norm_field_loss:\n","        #     mean = target.mean(dim=-1, keepdim=True)\n","        #     var = target.var(dim=-1, keepdim=True)\n","        #     target = (target - mean) / (var + 1e-6) ** 0.5\n","        rec_mask = mask * m\n","        loss = (pred.squeeze(dim=2) - target) ** 2\n","        loss = (loss * rec_mask).sum() / (rec_mask.sum() + 1e-6) + (loss * nask).sum() / (nask.sum() + 1e-6)\n","        return loss\n","\n","    def forward(self, data, m):\n","        \"\"\"\n","        Infer labels\n","        \"\"\"\n","        x, _, _ = self.forward_encoder(data, m, 0.0, False)\n","        B = x.shape[0]\n","        h = self.enc_pooler[1](self.enc_pooler[0](x).reshape(B, -1))\n","        enc_pred = self.enc_lbl_pred(h)[:, 0]\n","        return enc_pred\n","\n","    def forward_selfsl(self, data, m, mask_ratio=0.5, training=None):\n","        \"\"\"\n","        Self supervised learning: Reconstruct masked out features.\n","        \"\"\"\n","        x, mask, nask = self.forward_encoder(data, m, mask_ratio, training)\n","        pred = self.forward_decoder(x, mask)\n","        loss = self.forward_loss(data, pred, m, mask, nask)\n","        return loss, (loss.item(), )\n","\n","    def forward_sl(self, data, m, lbl_cols):\n","        \"\"\"\n","        Ordinary supervised learning: Multilabel regression.\n","        \"\"\"\n","        num_lbls = len(lbl_cols)\n","        lbl_mask = m[:, lbl_cols]\n","        ft_mask = m.clone()\n","        ft_mask[:, lbl_cols] = 0\n","        ft = data.clone()\n","        ft[:, :, lbl_cols] = 0\n","        x, _, _ = self.forward_encoder(ft, ft_mask, 0.0, False)\n","        B = x.shape[0]\n","        h = self.enc_pooler[1](self.enc_pooler[0](x).reshape(B, -1))\n","        enc_pred = self.enc_lbl_pred(h)[:, :num_lbls]\n","\n","        tgt = data[:, 0, lbl_cols]\n","        enc_loss = (((enc_pred - tgt) ** 2) * lbl_mask).sum() / (lbl_mask.sum() + 1e-6)\n","        \n","        loss = enc_loss\n","        \n","        return loss, (enc_loss.item(), )\n","      \n","    def forward_semisl(self, data, m, lbl_cols, ema_model=None, hard=False):\n","        \"\"\"\n","        Semi supervised learning: Consistency regularization: Minimizing H(Y|Z).\n","\n","        Parameters:\n","            ema_model:  Mean teacher idea inherited from \"https://arxiv.org/abs/1703.01780\".\n","                        The teacher will have its weight updated as the ema of the student's weight.\n","            hard:       Whether to round the sii or not.\n","                        Addtionally, consider adding L2-normed perturbation into the input (for augmentation).\n","            \n","        \"\"\"\n","        num_lbls = len(lbl_cols)\n","        lbl_mask = m[:, lbl_cols]\n","        nlbl_mask = 1 - m[:, lbl_cols]\n","        ft_mask = m.clone()\n","        ft_mask[:, lbl_cols] = 0\n","        ft = data.clone()\n","        ft[:, :, lbl_cols] = 0\n","        \n","        noise = torch.randn_like(ft)\n","        noise_norm = torch.norm(noise, p=2, dim=-1, keepdim=True)\n","        noise = noise / (noise_norm + 1e-8)\n","        noise = noise * 0.0\n","        if not hard: noise *= 0\n","        \n","        x, _, _ = self.forward_encoder(torch.clamp(ft + noise, min=0.0, max=1.0), ft_mask, 0.0, False)\n","        B = x.shape[0]\n","        h = self.enc_pooler[1](self.enc_pooler[0](x).reshape(B, -1))\n","        enc_pred = self.enc_lbl_pred(h)[:, :num_lbls]\n","        \n","        if ema_model is None: raise NotImplementedError()\n","        \n","        with torch.no_grad():\n","            if hard:\n","                x_tgt, _, _ = ema_model.forward_encoder(ft, ft_mask, 0.0, False)\n","                \n","                B = x_tgt.shape[0]\n","                h_tgt = ema_model.enc_pooler[1](ema_model.enc_pooler[0](x_tgt).reshape(B, -1))\n","                semisl_tgt = ema_model.enc_lbl_pred(h_tgt)[:, :num_lbls].detach()\n","                semisl_tgt[:, 0] = (semisl_tgt[:, 0] * 3.).round() / 3.\n","                semisl_weight = 1.0\n","                semisl_loss = (\n","                    0.5 * (((enc_pred - semisl_tgt) ** 2) * nlbl_mask).sum() / (nlbl_mask.sum() + 1e-6) # all labels\n","                    + 0.5 * (((enc_pred[:, 0] - semisl_tgt[:, 0]) ** 2) * nlbl_mask[:, 0]).sum() / (nlbl_mask[:, 0].sum() + 1e-6) # sii only\n","                )\n","            else:\n","                semisl_weight = 0.0\n","                semisl_loss = torch.tensor([0.0]).to(x.device)\n","                \n","\n","        \n","\n","        sl_tgt = data[:, 0, lbl_cols]\n","        sl_loss = (((enc_pred - sl_tgt) ** 2) * lbl_mask).sum() / (lbl_mask.sum() + 1e-6)\n","        \n","        loss = 1.0 * sl_loss + semisl_weight * semisl_loss\n","        return loss, (sl_loss.item(), semisl_loss.item(),)\n","      \n","      "]},{"cell_type":"markdown","metadata":{},"source":["## MAE sklearn wrapper"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-12-19T11:14:34.521695Z","iopub.status.busy":"2024-12-19T11:14:34.521363Z","iopub.status.idle":"2024-12-19T11:14:34.553484Z","shell.execute_reply":"2024-12-19T11:14:34.552547Z","shell.execute_reply.started":"2024-12-19T11:14:34.521668Z"},"trusted":true},"outputs":[],"source":["# stdlib\n","from typing import Any, List, Tuple, Union\n","\n","# third party\n","import numpy as np\n","import math, sys, argparse\n","import pandas as pd\n","import torch\n","from torch import nn\n","from functools import partial\n","import time, os, json\n","from torch.utils.data import DataLoader, RandomSampler\n","import sys\n","import timm.optim.optim_factory as optim_factory\n","import torch.nn.functional as F\n","import copy\n","\n","\n","def quadratic_weighted_kappa(y_true, y_pred):\n","    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n","\n","\n","\n","eps = 1e-8\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","from argparse import Namespace\n","# Default args\n","remasker_args = Namespace(\n","    batch_size=64,\n","    max_epochs= 600,\n","    accum_iter=1,\n","    mask_ratio=0.5,\n","    embed_dim=32,\n","    depth=6,\n","    decoder_depth=4,\n","    num_heads=4,\n","    mlp_ratio=4.0,\n","    encode_func='linear',\n","    norm_field_loss=False,\n","    weight_decay=0.05,\n","    lr=None, blr=0.001,\n","    min_lr=1e-05,\n","    warmup_epochs=40,\n","    device='cuda', seed=SEED, overwrite=True, pin_mem=True\n",")\n","\n","\n","\n","def set_dropout_p(m, p):\n","    if isinstance(m, nn.Dropout):\n","        m.p = p\n","\n","def update_ema_variables(model, ema_model, alpha, global_step, max_global_step):\n","    \"\"\"Update ema-teacher's weights based on student's weight using exponential moving average.\"\"\"\n","    def f(alpha, t, T):\n","        \"\"\"Reduce alpha with rate 1/t\"\"\"\n","        A = 1\n","        B = alpha * T / (T - 1)\n","        return (B * (1 - A / (t + 1)))\n","    current_alpha = f(alpha, global_step, max_global_step)\n","    for ema_param, param in zip(ema_model.parameters(), model.parameters()):\n","        ema_param.data.mul_(current_alpha).add_(1 - current_alpha, param.data)\n","\n","\n","class ReMasker:\n","    \"\"\"\n","        MAE's sklearn wrapper.\n","        Include a .fit(), .predict() api.\n","    \"\"\"\n","    def __init__(self, args=remasker_args):\n","\n","        self.batch_size = args.batch_size\n","        self.accum_iter = args.accum_iter\n","        self.min_lr = args.min_lr\n","        self.norm_field_loss = args.norm_field_loss\n","        # self.weight_decay = args.weight_decay\n","        self.lr = args.lr\n","        self.blr = args.blr\n","        self.warmup_epochs = max(1, args.max_epochs // 10)\n","        self.ema_decay = args.ema_decay\n","        self.model = None\n","        self.norm_parameters = None\n","\n","        self.embed_dim = args.embed_dim\n","        self.depth = args.depth\n","        self.decoder_depth = args.decoder_depth\n","        self.num_heads = args.num_heads\n","        self.mlp_ratio = args.mlp_ratio\n","        self.cls_mlp_dim = args.cls_mlp_dim\n","        self.max_epochs = args.max_epochs\n","        self.mask_ratio = args.mask_ratio\n","        self.encode_func = args.encode_func\n","        self.dropout = args.dropout\n","\n","\n","    def fit(self, X_raw: pd.DataFrame, X_val=None, lbl_cols=None, model=None):\n","        global dbg_var\n","        X = X_raw.clone()\n","\n","        # Parameters\n","        no = len(X)\n","        dim = len(X[0, :])\n","\n","        X = X.cpu()\n","\n","        min_val = np.zeros(dim)\n","        max_val = np.zeros(dim)\n","\n","        for i in range(dim):\n","            min_val[i] = np.nanmin(X[:, i])\n","            max_val[i] = np.nanmax(X[:, i])\n","            X[:, i] = (X[:, i] - min_val[i]) / (max_val[i] - min_val[i] + eps)\n","\n","        self.norm_parameters = {\"min\": min_val, \"max\": max_val}\n","\n","        # Set missing\n","        M = 1 - (1 * (np.isnan(X)))\n","        M = M.float().to(device)\n","\n","        X = torch.nan_to_num(X)\n","        X = X.to(device)\n","\n","        if model is None:\n","            self.model = MaskedAutoencoder(\n","                rec_len=dim,\n","                embed_dim=self.embed_dim,\n","                depth=self.depth,\n","                num_heads=self.num_heads,\n","                decoder_embed_dim=self.embed_dim,\n","                decoder_depth=self.decoder_depth,\n","                decoder_num_heads=self.num_heads,\n","                mlp_ratio=self.mlp_ratio,\n","                cls_mlp_dim=self.cls_mlp_dim,\n","                norm_layer=partial(nn.LayerNorm, eps=eps),\n","                norm_field_loss=self.norm_field_loss,\n","                encode_func=self.encode_func,\n","                dropout=self.dropout,\n","            )\n","        else:\n","            self.model = copy.deepcopy(model)\n","            for param in self.model.blocks.parameters():\n","                param.detach_()\n","        self.ema_model = copy.deepcopy(self.model)\n","        self.ema_model.apply(lambda m: set_dropout_p(m, p=0.0))\n","        for param in self.ema_model.parameters():\n","            param.detach_()\n","        \n","\n","        self.model.to(device)\n","        self.ema_model.to(device).eval()\n","\n","        # set optimizers\n","        # param_groups = optim_factory.add_weight_decay(model, args.weight_decay)\n","        eff_batch_size = self.batch_size * self.accum_iter\n","        if self.lr is None:  # only base_lr is specified\n","            self.lr = self.blr * eff_batch_size / 64\n","        # param_groups = optim_factory.add_weight_decay(self.model, self.weight_decay)\n","        # optimizer = torch.optim.AdamW(param_groups, lr=self.lr, betas=(0.9, 0.95))\n","        optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.lr, betas=(0.9, 0.95))\n","        loss_scaler = NativeScaler()\n","\n","        dataset = MAEDataset(X, M)\n","        dataloader = DataLoader(\n","            dataset, sampler=RandomSampler(dataset),\n","            batch_size=self.batch_size,\n","        )\n","\n","\n","        best_loss = 1e9\n","        best_model = copy.deepcopy(self.model)\n","        for epoch in range(self.max_epochs):\n","            self.model.train()\n","\n","            optimizer.zero_grad()\n","            total_loss = 0\n","            lbl_loss = 0.\n","            \n","            \n","            import time\n","            dbgt1 = 0\n","            dbgt2 = 0\n","            dbgt3 = 0\n","\n","            iter = 0\n","            for iter, (samples, masks) in enumerate(dataloader):\n","\n","                # we use a per iteration (instead of per epoch) lr scheduler\n","                if iter % self.accum_iter == 0:\n","                    adjust_learning_rate(optimizer, iter / len(dataloader) + epoch, self.lr, self.min_lr,\n","                                         self.max_epochs, self.warmup_epochs)\n","\n","                samples = samples.unsqueeze(dim=1)\n","                samples = samples.to(device, non_blocking=True)\n","                masks = masks.to(device, non_blocking=True)\n","\n","                # print(samples, masks)\n","\n","                # with torch.cuda.amp.autocast():\n","\n","                if lbl_cols is not None:\n","                    input_samples = samples.clone()\n","                    input_masks = masks.clone()\n","                    \n","                    \n","                    # selfsl_loss, dbg_selfsl_loss = self.model.forward_selfsl(samples, masks, mask_ratio=self.mask_ratio)\n","                    \n","                    # sl_loss, dbg_sl_loss = self.model.forward_sl(input_samples, input_masks, lbl_cols)\n","                    \n","                    \n","                    hard = True if (epoch >= (self.max_epochs // 2)) else False\n","                    semisl_loss, dbg_semisl_loss = self.model.forward_semisl(input_samples, input_masks, lbl_cols, ema_model=self.ema_model, hard=hard)\n","                    loss = 1.0 * semisl_loss\n","                        \n","                else:\n","\n","                    selfsl_loss, dbg_selfsl_loss = self.model.forward_selfsl(samples, masks, mask_ratio=self.mask_ratio)\n","                    loss = selfsl_loss\n","                \n","                loss_value = loss.item()\n","                total_loss += loss_value\n","                if not math.isfinite(loss_value):\n","                    print(\"Loss is {}, stopping training\".format(loss_value))\n","                    dbg_var = (samples, masks)\n","                    sys.exit(1)\n","\n","                loss /= self.accum_iter\n","                loss_scaler(loss, optimizer, parameters=self.model.parameters(),\n","                            update_grad=(iter + 1) % self.accum_iter == 0)\n","\n","                if (iter + 1) % self.accum_iter == 0:\n","                    optimizer.zero_grad()\n","                    \n","                \n","            update_ema_variables(self.model, self.ema_model, self.ema_decay, epoch, self.max_epochs)\n","            # print(dbgt1)\n","            # print(dbgt2)\n","            # print(dbgt3)\n","            total_loss = (total_loss / (iter + 1))\n","            self.model.eval()\n","            if X_val is not None:\n","                val_loss = self.evaluate(X_val, lbl_cols)\n","            else:\n","                val_loss = total_loss\n","            if val_loss <= best_loss:\n","                best_loss = val_loss\n","                best_model = copy.deepcopy(self.model)\n","            if (epoch + 1) % max(1, self.max_epochs // 10) == 0 or epoch == 0:\n","                lbl_loss = lbl_loss / (iter + 1)\n","                \n","                if lbl_cols is not None:\n","                    print(\"Epoch: %d, train;val;best qwk: %.4f;%.4f;%.4f, loss: %.4f, val_loss: %.4f\" % \n","                        (epoch+1, -self.evaluate(X_raw, lbl_cols), -val_loss, -best_loss, total_loss, val_loss)\n","                    )\n","                else:\n","                    print(\"Epoch: %d, loss: %.4f\" % \n","                        (epoch+1, -best_loss)\n","                    )\n","                    \n","                \n","\n","        self.model = best_model\n","        print(f'Loaded best model with loss={best_loss:.4f}')\n","        # torch.save(self.model.state_dict(), self.path)\n","        return self\n","      \n","      \n","      \n","    def evaluate(self, X_raw: torch.Tensor, lbl_cols):\n","        keep_indices = torch.where(~X_raw[:, lbl_cols].isnan())[0]\n","        X_raw = X_raw[keep_indices]\n","        gt = X_raw[:, lbl_cols[0]].cpu().numpy().round(0).astype(int)\n","        X_raw[:, lbl_cols] = float('nan')\n","        yp = self.predict(X_raw, lbl_cols)\n","        yp = yp.cpu().numpy().round(0).astype(int)\n","        return -quadratic_weighted_kappa(gt, yp)\n","      \n","      \n","    def predict(self, X_raw: torch.Tensor, lbl_idx, bs=None):\n","        X_raw = torch.tensor(X_raw, dtype=torch.float32)\n","        \n","        # Normalize the input data\n","        min_val = self.norm_parameters[\"min\"]\n","        max_val = self.norm_parameters[\"max\"]\n","        X = X_raw.clone()\n","        for i in range(X.shape[1]):\n","            X[:, i] = (X[:, i] - min_val[i]) / (max_val[i] - min_val[i] + eps)\n","        \n","        M = (1 - (1 * torch.isnan(X))).float().to(device)\n","        \n","        X = torch.nan_to_num(X)\n","        X = X.to(device)\n","        \n","        if bs == None: bs = self.batch_size\n","        # Prepare DataLoader\n","        dataset = MAEDataset(X, M)\n","        dataloader = DataLoader(dataset, batch_size=bs, shuffle=False)\n","        \n","        # Ensure model is in evaluation mode\n","        self.model.eval()\n","        \n","        # Tensor to hold predictions\n","        predictions = torch.zeros(0).to(device)\n","        \n","        with torch.no_grad():\n","            for batch_samples, batch_masks in dataloader:\n","                # Prepare input for the model\n","                batch_samples = batch_samples.unsqueeze(dim=1).to(device)\n","                batch_masks = batch_masks.to(device)\n","                \n","                # Forward pass with training=False\n","                pred = self.model.forward(batch_samples, batch_masks)\n","                \n","                pred = pred.reshape(-1)\n","                \n","                predictions = torch.cat((predictions, pred), 0)\n","        \n","        return predictions * 3."]},{"cell_type":"markdown","metadata":{},"source":["## Prepare training tensors"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-12-19T11:14:43.547805Z","iopub.status.busy":"2024-12-19T11:14:43.547180Z","iopub.status.idle":"2024-12-19T11:14:43.562813Z","shell.execute_reply":"2024-12-19T11:14:43.561879Z","shell.execute_reply.started":"2024-12-19T11:14:43.547771Z"},"trusted":true},"outputs":[],"source":["# Self-supervise on train and test features.\n","full_df = pd.concat([train,test])\n","X_raw = torch.tensor(full_df.to_numpy()).float()\n","\n","def random_extend(arr, k): \n","    \"\"\"Randomly extend a tensor to simulate number of samples in private test.\"\"\"\n","    indices = np.concatenate([np.random.permutation(len(arr)) for _ in range(10)])[:k]\n","    return arr[indices]\n","\n","for c in full_df.columns:\n","    if c not in test.columns:\n","        test[c] = float('nan')\n","\n","test = test[full_df.columns]\n","\n","X_tensor_test = torch.tensor(test.to_numpy()).float()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-12-19T11:14:51.989735Z","iopub.status.busy":"2024-12-19T11:14:51.988918Z","iopub.status.idle":"2024-12-19T11:14:51.994088Z","shell.execute_reply":"2024-12-19T11:14:51.993242Z","shell.execute_reply.started":"2024-12-19T11:14:51.989701Z"},"trusted":true},"outputs":[],"source":["\n","def get_model_size(model):\n","    \"\"\"Get nn.Module's number of (million) parameters.\"\"\"\n","    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n","    model_size = sum([np.prod(p.size()) for p in model_parameters])\n","    return \"{}K\".format(round(model_size / 1e1) / 1e2)\n","  \n","# get_model_size(imputer.model.blocks.layers[:-1])"]},{"cell_type":"markdown","metadata":{},"source":["## Training api"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-12-19T11:14:54.188146Z","iopub.status.busy":"2024-12-19T11:14:54.187452Z","iopub.status.idle":"2024-12-19T11:14:54.206667Z","shell.execute_reply":"2024-12-19T11:14:54.205565Z","shell.execute_reply.started":"2024-12-19T11:14:54.188114Z"},"trusted":true},"outputs":[],"source":["\n","from sklearn.base import clone\n","\n","def quadratic_weighted_kappa(y_true, y_pred):\n","    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n","\n","def threshold_Rounder(oof_non_rounded, thresholds):\n","    return np.where(oof_non_rounded < thresholds[0], 0,\n","                    np.where(oof_non_rounded < thresholds[1], 1,\n","                             np.where(oof_non_rounded < thresholds[2], 2, 3)))\n","\n","def evaluate_predictions(thresholds, y_true, oof_non_rounded):\n","    rounded_p = threshold_Rounder(oof_non_rounded, thresholds)\n","    return -quadratic_weighted_kappa(y_true, rounded_p)\n","\n","from tqdm import tqdm\n","from sklearn.model_selection import KFold\n","\n","\n","def random_extend(arr, k):\n","    indices = np.concatenate([np.random.permutation(len(arr)) for _ in range(10)])[:k]\n","    return arr[indices]\n","\n","\n","num_folds = 5\n","\n","def PerformImpute(imputer_args):\n","    global X_raw, X_tensor_test, num_folds\n","    train_S = []\n","    test_S = []\n","    \n","    KF = KFold(n_splits=num_folds, shuffle=True, random_state=SEED)\n","\n","    oof_non_rounded = []\n","    oof_rounded = []\n","    oof_gt = []\n","    test_preds = np.zeros((len(X_tensor_test), num_folds))\n","    \n","    ### Keep indices of label columns.\n","    lbl_cols = [full_df.columns.get_loc(c) for c in full_df.columns if 'PCIAT' in c or 'sii' in c]\n","    lbl_idx = lbl_cols[0]\n","    \n","    lbled_indices = torch.where(~X_raw[:, lbl_idx].isnan())[0]\n","    \n","    \n","    \n","    X_raw_no_lbl = X_raw.clone()\n","    X_raw_no_lbl[:, lbl_cols] = float('nan')\n","    \n","    pretrain_args = copy.deepcopy(imputer_args)\n","    pretrain_args.max_epochs = pretrain_args.pretrain_epochs\n","    import time\n","    pretrain_tick = time.time()\n","    imputer_pretrain = ReMasker(pretrain_args)\n","    imputer_pretrain.fit(X_raw_no_lbl, None, None, None)\n","    ellapsed_time = time.time() - pretrain_tick\n","    print(f\"Pretrained in {ellapsed_time:.4f}s.\")\n","    \n","    pretrain_model = imputer_pretrain.model\n","\n","    \n","\n","    pbar = tqdm(KF.split(lbled_indices), desc=\"Training Folds\", total=n_splits)    \n","\n","    for fold, (train_idx_idx, test_idx_idx) in enumerate(pbar):\n","        train_idx = lbled_indices[train_idx_idx]\n","        test_idx = lbled_indices[test_idx_idx]\n","\n","        X_train = X_raw.clone()\n","        \n","        ### Remove labels of val samples in X_train \n","        ### MAE knows P(X) but not P(Y|X) on val samples.\n","        ### (do self-supervise but not ordinary supervise)\n","        X_train[test_idx.unsqueeze(1), lbl_cols] = float('nan')\n","        \n","        X_val = X_raw[test_idx].clone()\n","\n","        \n","        X_train = random_extend(X_train, 9000)\n","        # X_val = random_extend(X_val, 2000)\n","        \n","    \n","        train_nonna_indices = torch.where(~X_train[:, lbl_idx].isnan())[0]\n","        val_nonna_indices = torch.where(~X_val[:, lbl_idx].isnan())[0]\n","        if len(train_nonna_indices) == 0 or len(val_nonna_indices)==0: continue\n","    \n","        imputer = ReMasker(imputer_args)\n","        imputer.fit(X_train, X_val, lbl_cols, pretrain_model)\n","\n","        y_train_ = X_train[train_nonna_indices, lbl_idx].numpy().astype(int)\n","        y_val_ = X_val[val_nonna_indices, lbl_idx].numpy().astype(int)\n","\n","        ### Fill the labels as nan and let the MAE predict it.\n","        X_train[:, lbl_cols] = float('nan')\n","        X_val[:, lbl_cols] = float('nan')\n","        \n","        y_train_pred = imputer.predict(X_train[train_nonna_indices], lbl_cols).cpu().detach().numpy()\n","        y_val_pred = imputer.predict(X_val[val_nonna_indices], lbl_cols).cpu().detach().numpy()\n","        y_test_pred = imputer.predict(X_tensor_test, lbl_cols).cpu().detach().numpy()\n","\n","        # model = clone(model_init)\n","\n","        # model.fit(X_train_, y_train_)\n","\n","        oof_non_rounded += [y_val_pred]\n","        y_val_pred_rounded = y_val_pred.round(0).astype(int)\n","        oof_rounded += [y_val_pred_rounded]\n","        oof_gt += [y_val_]\n","        \n","        train_kappa = quadratic_weighted_kappa(y_train_, y_train_pred.round(0).astype(int))\n","        val_kappa = quadratic_weighted_kappa(y_val_, y_val_pred.round(0).astype(int))\n","\n","        train_S.append(train_kappa)\n","        test_S.append(val_kappa)\n","        \n","        \n","        test_preds[:, fold] = y_test_pred\n","\n","        pbar.set_description_str(\n","          \"Fold %d, Train MSE: %.4f, Val MSE: %.4f, Train QWK: %.4f, Val QWK: %.4f\" % (\n","              fold + 1,\n","              ((y_train_pred - y_train_) ** 2 / 9.).mean(),\n","              ((y_val_pred - y_val_) ** 2 / 9.).mean(),\n","              train_kappa,\n","              val_kappa\n","          )\n","        )\n","    \n","    \n","    print(f\"Mean Train QWK --> {np.mean(train_S):.4f}\")\n","    print(f\"Mean Validation QWK ---> {np.mean(test_S):.4f}\")\n","\n","    oof_non_rounded = np.concatenate(oof_non_rounded)\n","    oof_gt = np.concatenate(oof_gt)\n","    KappaOPtimizer = minimize(evaluate_predictions,\n","                              x0=[0.5, 1.5, 2.5], args=(oof_gt, oof_non_rounded), \n","                              method='Nelder-Mead')\n","    assert KappaOPtimizer.success, \"Optimization did not converge.\"\n","    \n","    oof_tuned = threshold_Rounder(oof_non_rounded, KappaOPtimizer.x)\n","    tKappa = quadratic_weighted_kappa(oof_gt, oof_tuned)\n","\n","    print(f\"----> || Optimized QWK SCORE :: {Fore.CYAN}{Style.BRIGHT} {tKappa:.3f}{Style.RESET_ALL}\")\n","\n","\n","    # return 0.5 * (np.mean(test_S) + tKappa)\n","  \n","  \n","    tpm = test_preds.mean(axis=1)\n","    tpTuned = threshold_Rounder(tpm, KappaOPtimizer.x)\n","    \n","    sample_sub_df = pd.read_csv('../input/child-mind-institute-problematic-internet-use/sample_submission.csv')\n","    submission = pd.DataFrame({\n","        'id': sample_sub_df['id'],\n","        'sii': tpTuned\n","    })\n","\n","    return submission\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["## Train the model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-12-19T11:15:03.288136Z","iopub.status.busy":"2024-12-19T11:15:03.287817Z","iopub.status.idle":"2024-12-19T11:17:00.003841Z","shell.execute_reply":"2024-12-19T11:17:00.002562Z","shell.execute_reply.started":"2024-12-19T11:15:03.288108Z"},"trusted":true},"outputs":[],"source":["imputer_args = Namespace(\n","    batch_size=64,\n","    max_epochs= 50,\n","    pretrain_epochs=400,\n","    accum_iter=1,\n","    mask_ratio=0.75,\n","    embed_dim=8,\n","    depth=8,\n","    decoder_depth=1,\n","    num_heads=4,\n","    mlp_ratio=64.,\n","    cls_mlp_dim=32,\n","    dropout=0.5,\n","    encode_func='linear',\n","    norm_field_loss=False,\n","    ema_decay=0.9,\n","    weight_decay=0.05,\n","    lr=None, blr=0.001,\n","    min_lr=1e-05,\n","    device='cuda', seed=SEED, overwrite=True, pin_mem=True\n",")\n","print(imputer_args)\n","\n","\n","\n","### Shuffle the data once.\n","SEED = random.randint(1, int(2e9))\n","np.random.seed(SEED)\n","indices = np.random.permutation(len(X_raw))\n","X_raw = X_raw[indices]\n","sub1 = PerformImpute(imputer_args)\n","sub1"]},{"cell_type":"markdown","metadata":{},"source":["# Recipe 2: No imputation"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def process_file(filename, dirname):\n","    df = pd.read_parquet(os.path.join(dirname, filename, 'part-0.parquet'))\n","    df.drop('step', axis=1, inplace=True)\n","    return df.describe().values.reshape(-1), filename.split('=')[1]\n","\n","def load_time_series(dirname) -> pd.DataFrame:\n","    ids = os.listdir(dirname)\n","    \n","    with ThreadPoolExecutor() as executor:\n","        results = list(tqdm(executor.map(lambda fname: process_file(fname, dirname), ids), total=len(ids)))\n","    \n","    stats, indexes = zip(*results)\n","    \n","    df = pd.DataFrame(stats, columns=[f\"stat_{i}\" for i in range(len(stats[0]))])\n","    df['id'] = indexes\n","    return df\n","\n","train_ts = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_train.parquet\")\n","test_ts = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_test.parquet\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/train.csv')\n","test = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/test.csv')\n","sample = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/sample_submission.csv')\n","\n","time_series_cols = train_ts.columns.tolist()\n","time_series_cols.remove(\"id\")\n","\n","train = pd.merge(train, train_ts, how=\"left\", on='id')\n","test = pd.merge(test, test_ts, how=\"left\", on='id')\n","\n","train = train.drop('id', axis=1)\n","test = test.drop('id', axis=1)   \n","\n","featuresCols = ['Basic_Demos-Enroll_Season', 'Basic_Demos-Age', 'Basic_Demos-Sex',\n","                'CGAS-Season', 'CGAS-CGAS_Score', 'Physical-Season', 'Physical-BMI',\n","                'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference',\n","                'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP',\n","                'Fitness_Endurance-Season', 'Fitness_Endurance-Max_Stage',\n","                'Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec',\n","                'FGC-Season', 'FGC-FGC_CU', 'FGC-FGC_CU_Zone', 'FGC-FGC_GSND',\n","                'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone', 'FGC-FGC_PU',\n","                'FGC-FGC_PU_Zone', 'FGC-FGC_SRL', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR',\n","                'FGC-FGC_SRR_Zone', 'FGC-FGC_TL', 'FGC-FGC_TL_Zone', 'BIA-Season',\n","                'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC', 'BIA-BIA_BMI',\n","                'BIA-BIA_BMR', 'BIA-BIA_DEE', 'BIA-BIA_ECW', 'BIA-BIA_FFM',\n","                'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_Fat', 'BIA-BIA_Frame_num',\n","                'BIA-BIA_ICW', 'BIA-BIA_LDM', 'BIA-BIA_LST', 'BIA-BIA_SMM',\n","                'BIA-BIA_TBW', 'PAQ_A-Season', 'PAQ_A-PAQ_A_Total', 'PAQ_C-Season',\n","                'PAQ_C-PAQ_C_Total', 'SDS-Season', 'SDS-SDS_Total_Raw',\n","                'SDS-SDS_Total_T', 'PreInt_EduHx-Season',\n","                'PreInt_EduHx-computerinternet_hoursday', 'sii']\n","\n","featuresCols += time_series_cols\n","\n","train = train[featuresCols]\n","train = train.dropna(subset='sii')\n","\n","cat_c = ['Basic_Demos-Enroll_Season', 'CGAS-Season', 'Physical-Season', \n","          'Fitness_Endurance-Season', 'FGC-Season', 'BIA-Season', \n","          'PAQ_A-Season', 'PAQ_C-Season', 'SDS-Season', 'PreInt_EduHx-Season']\n","\n","def update(df):\n","    global cat_c\n","    for c in cat_c: \n","        df[c] = df[c].fillna('Missing')\n","        df[c] = df[c].astype('category')\n","    return df\n","        \n","train = update(train)\n","test = update(test)\n","\n","def create_mapping(column, dataset):\n","    unique_values = dataset[column].unique()\n","    return {value: idx for idx, value in enumerate(unique_values)}\n","\n","for col in cat_c:\n","    mapping = create_mapping(col, train)\n","    mappingTe = create_mapping(col, test)\n","    \n","    train[col] = train[col].replace(mapping).astype(int)\n","    test[col] = test[col].replace(mappingTe).astype(int)\n","\n","def quadratic_weighted_kappa(y_true, y_pred):\n","    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n","\n","def threshold_Rounder(oof_non_rounded, thresholds):\n","    return np.where(oof_non_rounded < thresholds[0], 0,\n","                    np.where(oof_non_rounded < thresholds[1], 1,\n","                             np.where(oof_non_rounded < thresholds[2], 2, 3)))\n","\n","def evaluate_predictions(thresholds, y_true, oof_non_rounded):\n","    rounded_p = threshold_Rounder(oof_non_rounded, thresholds)\n","    return -quadratic_weighted_kappa(y_true, rounded_p)\n","\n","def TrainML(model_class, test_data):\n","    X = train.drop(['sii'], axis=1)\n","    y = train['sii']\n","\n","    SKF = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n","    \n","    train_S = []\n","    test_S = []\n","    \n","    oof_non_rounded = np.zeros(len(y), dtype=float) \n","    oof_rounded = np.zeros(len(y), dtype=int) \n","    test_preds = np.zeros((len(test_data), n_splits))\n","\n","    for fold, (train_idx, test_idx) in enumerate(tqdm(SKF.split(X, y), desc=\"Training Folds\", total=n_splits)):\n","        X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n","        y_train, y_val = y.iloc[train_idx], y.iloc[test_idx]\n","\n","        model = clone(model_class)\n","        model.fit(X_train, y_train)\n","\n","        y_train_pred = model.predict(X_train)\n","        y_val_pred = model.predict(X_val)\n","\n","        oof_non_rounded[test_idx] = y_val_pred\n","        y_val_pred_rounded = y_val_pred.round(0).astype(int)\n","        oof_rounded[test_idx] = y_val_pred_rounded\n","\n","        train_kappa = quadratic_weighted_kappa(y_train, y_train_pred.round(0).astype(int))\n","        val_kappa = quadratic_weighted_kappa(y_val, y_val_pred_rounded)\n","\n","        train_S.append(train_kappa)\n","        test_S.append(val_kappa)\n","        \n","        test_preds[:, fold] = model.predict(test_data)\n","        \n","        print(f\"Fold {fold+1} - Train QWK: {train_kappa:.4f}, Validation QWK: {val_kappa:.4f}\")\n","        clear_output(wait=True)\n","\n","    print(f\"Mean Train QWK --> {np.mean(train_S):.4f}\")\n","    print(f\"Mean Validation QWK ---> {np.mean(test_S):.4f}\")\n","\n","    KappaOPtimizer = minimize(evaluate_predictions,\n","                              x0=[0.5, 1.5, 2.5], args=(y, oof_non_rounded), \n","                              method='Nelder-Mead')\n","    assert KappaOPtimizer.success, \"Optimization did not converge.\"\n","    \n","    oof_tuned = threshold_Rounder(oof_non_rounded, KappaOPtimizer.x)\n","    tKappa = quadratic_weighted_kappa(y, oof_tuned)\n","\n","    print(f\"----> || Optimized QWK SCORE :: {Fore.CYAN}{Style.BRIGHT} {tKappa:.3f}{Style.RESET_ALL}\")\n","\n","    tpm = test_preds.mean(axis=1)\n","    tpTuned = threshold_Rounder(tpm, KappaOPtimizer.x)\n","    \n","    submission = pd.DataFrame({\n","        'id': sample['id'],\n","        'sii': tpTuned\n","    })\n","\n","    return submission\n","\n","# Model parameters for LightGBM\n","Params = {\n","    'learning_rate': 0.046,\n","    'max_depth': 12,\n","    'num_leaves': 478,\n","    'min_data_in_leaf': 13,\n","    'feature_fraction': 0.893,\n","    'bagging_fraction': 0.784,\n","    'bagging_freq': 4,\n","    'lambda_l1': 10,  # Increased from 6.59\n","    'lambda_l2': 0.01  # Increased from 2.68e-06\n","}\n","\n","\n","# XGBoost parameters\n","XGB_Params = {\n","    'learning_rate': 0.05,\n","    'max_depth': 6,\n","    'n_estimators': 200,\n","    'subsample': 0.8,\n","    'colsample_bytree': 0.8,\n","    'reg_alpha': 1,  # Increased from 0.1\n","    'reg_lambda': 5,  # Increased from 1\n","    'random_state': SEED\n","}\n","\n","\n","CatBoost_Params = {\n","    'learning_rate': 0.05,\n","    'depth': 6,\n","    'iterations': 200,\n","    'random_seed': SEED,\n","    'cat_features': cat_c,\n","    'verbose': 0,\n","    'l2_leaf_reg': 10  # Increase this value\n","}\n","\n","# Create model instances\n","Light = LGBMRegressor(**Params, random_state=SEED, verbose=-1, n_estimators=300)\n","XGB_Model = XGBRegressor(**XGB_Params)\n","CatBoost_Model = CatBoostRegressor(**CatBoost_Params)\n","\n","# Combine models using Voting Regressor\n","voting_model = VotingRegressor(estimators=[\n","    ('lightgbm', Light),\n","    ('xgboost', XGB_Model),\n","    ('catboost', CatBoost_Model)\n","])\n","\n","# Train the ensemble model\n","sub2 = TrainML(voting_model, test)\n","sub2"]},{"cell_type":"markdown","metadata":{},"source":["# Recipe 3: Mean imputation"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/train.csv')\n","test = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/test.csv')\n","sample = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/sample_submission.csv')\n","\n","featuresCols = ['Basic_Demos-Enroll_Season', 'Basic_Demos-Age', 'Basic_Demos-Sex',\n","                'CGAS-Season', 'CGAS-CGAS_Score', 'Physical-Season', 'Physical-BMI',\n","                'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference',\n","                'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP',\n","                'Fitness_Endurance-Season', 'Fitness_Endurance-Max_Stage',\n","                'Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec',\n","                'FGC-Season', 'FGC-FGC_CU', 'FGC-FGC_CU_Zone', 'FGC-FGC_GSND',\n","                'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone', 'FGC-FGC_PU',\n","                'FGC-FGC_PU_Zone', 'FGC-FGC_SRL', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR',\n","                'FGC-FGC_SRR_Zone', 'FGC-FGC_TL', 'FGC-FGC_TL_Zone', 'BIA-Season',\n","                'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC', 'BIA-BIA_BMI',\n","                'BIA-BIA_BMR', 'BIA-BIA_DEE', 'BIA-BIA_ECW', 'BIA-BIA_FFM',\n","                'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_Fat', 'BIA-BIA_Frame_num',\n","                'BIA-BIA_ICW', 'BIA-BIA_LDM', 'BIA-BIA_LST', 'BIA-BIA_SMM',\n","                'BIA-BIA_TBW', 'PAQ_A-Season', 'PAQ_A-PAQ_A_Total', 'PAQ_C-Season',\n","                'PAQ_C-PAQ_C_Total', 'SDS-Season', 'SDS-SDS_Total_Raw',\n","                'SDS-SDS_Total_T', 'PreInt_EduHx-Season',\n","                'PreInt_EduHx-computerinternet_hoursday', 'sii']\n","\n","cat_c = ['Basic_Demos-Enroll_Season', 'CGAS-Season', 'Physical-Season', \n","          'Fitness_Endurance-Season', 'FGC-Season', 'BIA-Season', \n","          'PAQ_A-Season', 'PAQ_C-Season', 'SDS-Season', 'PreInt_EduHx-Season']\n","\n","train_ts = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_train.parquet\")\n","test_ts = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_test.parquet\")\n","\n","time_series_cols = train_ts.columns.tolist()\n","time_series_cols.remove(\"id\")\n","\n","train = pd.merge(train, train_ts, how=\"left\", on='id')\n","test = pd.merge(test, test_ts, how=\"left\", on='id')\n","\n","train = train.drop('id', axis=1)\n","test = test.drop('id', axis=1)\n","\n","featuresCols += time_series_cols\n","\n","train = train[featuresCols]\n","train = train.dropna(subset='sii')\n","\n","def update(df):\n","    global cat_c\n","    for c in cat_c: \n","        df[c] = df[c].fillna('Missing')\n","        df[c] = df[c].astype('category')\n","    return df\n","\n","train = update(train)\n","test = update(test)\n","\n","def create_mapping(column, dataset):\n","    unique_values = dataset[column].unique()\n","    return {value: idx for idx, value in enumerate(unique_values)}\n","\n","for col in cat_c:\n","    mapping = create_mapping(col, train)\n","    mappingTe = create_mapping(col, test)\n","    \n","    train[col] = train[col].replace(mapping).astype(int)\n","    test[col] = test[col].replace(mappingTe).astype(int)\n","\n","def quadratic_weighted_kappa(y_true, y_pred):\n","    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n","\n","def threshold_Rounder(oof_non_rounded, thresholds):\n","    return np.where(oof_non_rounded < thresholds[0], 0,\n","                    np.where(oof_non_rounded < thresholds[1], 1,\n","                             np.where(oof_non_rounded < thresholds[2], 2, 3)))\n","\n","def evaluate_predictions(thresholds, y_true, oof_non_rounded):\n","    rounded_p = threshold_Rounder(oof_non_rounded, thresholds)\n","    return -quadratic_weighted_kappa(y_true, rounded_p)\n","\n","def TrainML(model_class, test_data):\n","    X = train.drop(['sii'], axis=1)\n","    y = train['sii']\n","\n","    SKF = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n","    \n","    train_S = []\n","    test_S = []\n","    \n","    oof_non_rounded = np.zeros(len(y), dtype=float) \n","    oof_rounded = np.zeros(len(y), dtype=int) \n","    test_preds = np.zeros((len(test_data), n_splits))\n","\n","    for fold, (train_idx, test_idx) in enumerate(tqdm(SKF.split(X, y), desc=\"Training Folds\", total=n_splits)):\n","        X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n","        y_train, y_val = y.iloc[train_idx], y.iloc[test_idx]\n","\n","        model = clone(model_class)\n","        model.fit(X_train, y_train)\n","\n","        y_train_pred = model.predict(X_train)\n","        y_val_pred = model.predict(X_val)\n","\n","        oof_non_rounded[test_idx] = y_val_pred\n","        y_val_pred_rounded = y_val_pred.round(0).astype(int)\n","        oof_rounded[test_idx] = y_val_pred_rounded\n","\n","        train_kappa = quadratic_weighted_kappa(y_train, y_train_pred.round(0).astype(int))\n","        val_kappa = quadratic_weighted_kappa(y_val, y_val_pred_rounded)\n","\n","        train_S.append(train_kappa)\n","        test_S.append(val_kappa)\n","        \n","        test_preds[:, fold] = model.predict(test_data)\n","        \n","        print(f\"Fold {fold+1} - Train QWK: {train_kappa:.4f}, Validation QWK: {val_kappa:.4f}\")\n","        clear_output(wait=True)\n","\n","    print(f\"Mean Train QWK --> {np.mean(train_S):.4f}\")\n","    print(f\"Mean Validation QWK ---> {np.mean(test_S):.4f}\")\n","\n","    KappaOPtimizer = minimize(evaluate_predictions,\n","                              x0=[0.5, 1.5, 2.5], args=(y, oof_non_rounded), \n","                              method='Nelder-Mead')\n","    assert KappaOPtimizer.success, \"Optimization did not converge.\"\n","    \n","    oof_tuned = threshold_Rounder(oof_non_rounded, KappaOPtimizer.x)\n","    tKappa = quadratic_weighted_kappa(y, oof_tuned)\n","\n","    print(f\"----> || Optimized QWK SCORE :: {Fore.CYAN}{Style.BRIGHT} {tKappa:.3f}{Style.RESET_ALL}\")\n","\n","    tpm = test_preds.mean(axis=1)\n","    tp_rounded = threshold_Rounder(tpm, KappaOPtimizer.x)\n","\n","    return tp_rounded\n","\n","imputer = SimpleImputer(strategy='median')\n","\n","ensemble = VotingRegressor(estimators=[\n","    ('lgb', Pipeline(steps=[('imputer', imputer), ('regressor', LGBMRegressor(random_state=SEED))])),\n","    ('xgb', Pipeline(steps=[('imputer', imputer), ('regressor', XGBRegressor(random_state=SEED))])),\n","    ('cat', Pipeline(steps=[('imputer', imputer), ('regressor', CatBoostRegressor(random_state=SEED, silent=True))])),\n","    ('rf', Pipeline(steps=[('imputer', imputer), ('regressor', RandomForestRegressor(random_state=SEED))])),\n","    ('gb', Pipeline(steps=[('imputer', imputer), ('regressor', GradientBoostingRegressor(random_state=SEED))]))\n","])\n","\n","sub3 = TrainML(ensemble, test)\n","sub3 = pd.DataFrame({\n","    'id': sample['id'],\n","    'sii': sub3\n","})\n","\n","sub3"]},{"cell_type":"markdown","metadata":{},"source":["# Majority voting 3 recipes"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["sub1 = sub1.sort_values(by='id').reset_index(drop=True)\n","sub2 = sub2.sort_values(by='id').reset_index(drop=True)\n","sub3 = sub3.sort_values(by='id').reset_index(drop=True)\n","\n","combined = pd.DataFrame({\n","    'id': sub1['id'],\n","    'sii_1': sub1['sii'],\n","    'sii_2': sub2['sii'],\n","    'sii_3': sub3['sii'],\n","})\n","\n","def majority_vote(row):\n","    return row.mode()[0]\n","\n","combined['final_sii'] = combined[['sii_1', 'sii_2', 'sii_3']].apply(majority_vote, axis=1)\n","\n","final_submission = combined[['id', 'final_sii']].rename(columns={'final_sii': 'sii'})\n","\n","final_submission.to_csv('submission.csv', index=False)\n","\n","print(\"Majority voting completed and saved to 'Final_Submission.csv'\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["final_submission\n"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":9643020,"sourceId":81933,"sourceType":"competition"},{"datasetId":921302,"sourceId":7453542,"sourceType":"datasetVersion"}],"dockerImageVersionId":30805,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
